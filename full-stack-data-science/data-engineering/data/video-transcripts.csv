video_id,datetime,title,transcript
eayzAZltV9U,2024-04-29T13:54:55.000000,4 Lessons from AI Consulting #freelancing,are four things I've learned from AI Consulting one trust is more important than anything else in my experience what differentiates clients from prospects is the belief that I could solve their problem and that I was on their side two don't skip the discovery phase when providing Technical Services it's easy to jump right into the coding the problem with this however is that you can spend a lot of time and money solving the wrong problem three find your number one sales Channel although there are several ways to get clients for example upwork referrals conference speaking content creation ads and more I and others that I've met in the space get most of their leads through one key sales Channel fourth and finally it's not real until the money's in the bank this is a lesson I've learned over and over again from Great Discovery calls that ended in nothing
03x2oYg9oME,2024-04-25T15:16:00.000000,How to Manage Data Science Projects,this video is part of a larger series on full stack data science in the previous video of this series I introduced this idea of a full stack data scientist and described the four hats that it involves in this video I'm going to dive into the first of these four hats which is that of a project manager I'll start by introducing a five-step project management framework specifically for data science and then I'll walk through a concrete example of the project manager role in implementing this framework and if you're new here welcome I'm Shaw I make videos about data science and Entrepreneurship and if you enjoy this content please consider subscribing that's a great no cost way you can support me in all the videos that I make before diving into the framework I think it's helpful to take a step back and ask ourselves why do we need project management while there surely cases where project management feels more like red tape and constraints as opposed to something that pushes a project forward the way I see it just just a little bit of planning and structure can go a very long way when it comes to implementing projects that involve a lot of moving pieces here are a few reasons for that first project management involves all of the planning and scoping for a project just like how an architect will draw out a blueprint for a building before it's actually built project managers will draw out the blueprint of a project before it is implemented next data science projects often involve bringing together multiple pieces this includes data sets compute Technologies and of course this involves people these could be individual contributors they could be your stakeholders or other people involved in making the project happen and then finally project management helps keep projects on time and on budget so while this video is specifically about data science projects everything that I said here is not something unique to data science projects in fact these three things are going to be relevant to project management in any domain however there are some unique considerations for data science projects which brings up a five-step project management framework that I like to use When approaching a model build or the development of some other datadriven solution the framework is shown here where it's broken down into five phases starting from phase zero which is what I call the problem definition and scoping phase phase one is the data acquisition exploration and preparation we can also think of this is all the data engineering work next is phase two which is the solution development this could be training a model or it could be doing some kind of analytics this will typically involve a data scientist or maybe even a data analyst next phase three is the solution deployment so this is actually taking the solution and putting it into the real world so that it can actually have an impact and then finally phase four is the evaluation and documentation step let's dive into each of these phases a bit more deeply starting with phase zero the problem definition and scoping phase this will involve formulating the business problem basically asking ourselves what problem are we actually trying to solve next is designing a solution to that problem any given problem will have countless ways it can actually be solved so it's important to make a decision on the best way one can solve the problem at hand finally defining the project requirements and road map so in other words what do we need to make this project happen and what will the implementation look like next we have phase one which is the data acquisition exploration and preparation so this might involve looking at internal datab bases looking to third-party data vendors or any other data source evaluating available data also means asking the question do we have sufficient information to solve the problem at hand another key step of this phase is acquiring and exploring the data so this includes things like exploratory data analysis where one is just trying to get a sense of the information that can be gleaned from the data set and then finally the development of data pipelines so this is your extract transform and load pipelines or your extract load and transform pipelines next we have phase two which is the solution development so this might be the part most people are excited about which is the model development so developing Ai and Data Solutions but it's not just about training a model or building some other analytic solution it's also about evaluating that solution in terms of its validity and the value that it generates and this evaluation process will often involve iterating with stakeholders next we have phase three which is solution deployment so the basic idea here is to integrate the solution into the real world business context whether that's automating a particular workflow or making information available to stakeholders via a dashboard maybe updating a widget on a website so this can take many different forms additionally for some sort of intervention you know you're updating a business process you also may need to implement a solution monitoring pipeline it's not just about making a change and walking away but making a change and consistently evaluating the efficacy of that change and then finally phase four is the evaluation and documentation step so this involves assessing project outcomes and comparing them against the expectations from the outset of the project this is also delivering technical documentation and user guides and then finally doing a perspective reflecting back on the project looking at things that went well looking for opportunities for improvement considering the Future Works and the limitations of the current project and starting to talk about the obvious next steps and of course we have these feedback loops in this five-step framework some key ones are shown here where you may come to phase one and you might be evaluating the available data and you may find that we thought we could solve this problem we could build this model with the data we have internally available but upon further investigation that's not the case and we'll have to figure something else out so that'll require going back to phase zero and asking ourselves okay how can we get this data do we look to a data vendor do we try to get it from publicly available sources another key feedback loop is between phases 2 in phase one so say during model development there seems to be some kind of bias in the model and then upon further investigation you find that a particular exception wasn't properly handled in the transform phase or the data pre processing step so that'll require going back to phase 1 updating the data preparation Pipeline and then returning back to phase 2 to train the model another feedback loop is from Phase 2 to phase zero so the data may be properly pre-processed and there's nothing wrong with it but still upon training the model the predictive performance may not be as good as the existing solution so this might require going back to phase zero and reassessing the solution design and asking is there a better way we can solve this problem the final feedback loop is if everything goes according to plan value is generated but of course no project is ever complete there are always opportunities for improvement and then You' return back to phase zero to start building a broader solution so while the project manager is ultimately responsible for the successful implementation of the project and ensuring that each of these phases happens the key contribution of the project manager is this phase zero because if this phase zero is is not done properly it's going to cause problems in every subsequent phase of the project so this brings us to the key role of the project manager which is this phase zero the problem definition and scoping and as we saw on the previous slide this consists of three key steps the first is the problem diagnosis it's often the role of the project manager to collaborate with stakeholders and project owners to get a clear understanding of the problem that they're trying to solve and it's critical to get this step right because if you think about it if you don't diagnose the problem properly you can spend a lot of time and effort solving the wrong problem which no one wants the stakeholder doesn't want that and the team doesn't want that so being able to facilitate these conversations with stakeholders is a key skill of any project manager while I won't go too deeply into what this might look like I do have an entire video dedicated to this topic which I'll link up on the screen here so once you have a clear idea of what problem you're trying to solve then comes the task of identifying the best way to solve that problem so this requires a project manager to bring together a lot of different information they have to bring together the business context the priorities of the stakeholder they have to consider the available Technologies the available resources the available data the available budget the available timeline and they're also might be competing priorities and beyond that any given problem will have a wide range of potential Solutions and varying degrees of complexity and scope so at the end of the day the project manager has to synthesize this information to help the stakeholder or the project owner make the BET of what's the best way to solve the problem and then finally is the implementation plan once the solution has been defined there's still the matter of the details of what this thing's actually going to look like and how it's going to be built and even still any given solution can have a wide range of potential implementations so to make this more concrete I'm going to walk through a concrete case study the point of this is to one perform the phase zero for this project that I'm building but the second is to walk through step by step what it looks like being a full stack data scientist and so here this is me putting on the project manager hat for this project starting with a bit of background I make content on YouTube and write articles on medium what this looks like is I'll make content about whatever is interesting to me whatever I'm curious about or things that I've personally experienced since I am just talking about whatever's interesting to me it might be difficult for people to navigate all the different pieces of content I make across these two platforms which brings up the problem potentially I talk about too many topics across too many platforms so to kind of give a flavor for this I'll talk about things from topological data analysis AI for business causal inference how much I made writing on medium how to get a data sign job more philosophical things like what it means to be antifragile more personal things like my struggles with anxiety and now to this series of becoming a full stack data scientist so someone who's seeing this content for the first time or is trying to navigate this very diverse landscape of content might have trouble finding the content that's most relevant to them which brings up the proposed solution which is a semantic search function for my YouTube videos what this might look like is a page where users can type in a natural language query or a question and then the web page will return search results of YouTube videos relevant to that query so this only addresses half of the problem I mentioned on the previous slide which is I make too many topics on too many channels this only solves that first problem of too many topics so the hypothesis is if these topics are more easily searchable and they're easier to navigate then more people will engage with the content and then more people will share the content and that will promote the growth of the YouTube channel and so you might be thinking Shaw why are you just solving half the problem why not solve the whole problem that brings up this broader question of should I build a POC or a proof of concept I know there are a lot of different opinions on poc's and some people love them some people hate them I personally believe that poc's are very valuable when it comes to building machine learning projects you can do all sorts of interesting things with machine learning but of course of all these interesting things you can do only a small subset of them actually generate value so the first reason why I believe in Po's is that they help you validate the idea I can put together this proof of concept version of the web page I can make it available to some people in my audience I can make it available to some people that I know and they can provide me feedback which will help in making this decision of is this something worth pursuing or should I just cut my losses and move on to something else the second reason is often with data science projects efforts tend to stack on top of each other so if I properly build this proof of concept for YouTube videos it should only be a marginal effort to include my medium articles into the same interface now that we have a clear picture of what the solution will look like how do we actually make that happen that brings us to the implementation plan the first part of which are the project requirements so this includes the roles the data the compute infrastructure and the Technologies needed to implement the project here I list off the roles of the full stack data scientist so these are the four hats mentioned in the previous video the first is the project manager then we have the data engineer data scientist and ml engineer and what's important to note even if you are a full stack data scientist maybe you still want to bring in a data engineer or a data scientist to help you build the project or maybe you want multiple data Engineers or maybe you want to bring in someone to do the data science and the ml engineering piece while you just do the project manager and data engineering piece so it's important to note that roles do not equal people multiple people can do multiple roles and a single role can be done by multiple people the next is the data requirements so here I put an evaluation data set which will consist of 50 query video pairs that will help evaluate the quality of the search so in other words I need a data set I can use to evaluate the performance of the search function that I built so what that'll look like is a list of 50 queries and an associate ated YouTube video ID with that query for infrastructure I'm going to use AWS light sale which makes it super easy to deploy Docker containers and then finally Technologies I included all that I could think of at least at this point so I'm going to use Python for basically everything I'm going to use the YouTube API to pull in some data there's a python library that downloads the automatically generated captions of a YouTube video going to use pandas or polers to handle the data structures the sentence Transformers library to generate text embeddings fast API to make an API Docker to containerize scripts and then gradio to spin up a front end and then finally I'm going to put everything in a GitHub repo and have the documentation be part of that repo next we have the project road map so this consists of a few different things these are the project Milestones which I map to phases 1 through four of the five-step project management framework discussed earlier then we have a task description so what are the tasks that make up this Milestone or this phase assigning a role to each task and assigning a due date to each task you can add more things here like a more detailed description or acceptance criteria but I would say that these four elements of Milestone task roll and due date are the bare minimum you need to make a proper project roadmap I'll just kind of briefly fly through these I don't want to spend too much time reading it but you can always pause the video and read through these if you like phase one is all the data engineering stuff so this is extracting the transcripts and saving them as a paret file phase two will consist of exploring multiple embedding models and testing the search function using the evaluation data set and then once that's done creating a video index that is searchable phase three will consist of building an API for this search function containerizing it testing it locally building a simple gradio UI and then deploying it on AWS SP war will consist of creating the documentation and doing a project retrospective so future videos of this series will walk through each of these steps and I haven't built this you're seeing this thing live you're seeing this thing raw so I'm sure there's going to be some changes here things are going to come up that I didn't expect and I'm going to have to come back to phase zero maybe I fall behind on the due date since this is an independent project I don't have a manager I don't have a broader team holding me accountable but committing to these due dates on this video and committing to posting a video every single week on YouTube is helping give me structure to this project and keeping me motivated so everyone's comments from the previous video saying I'm excited for the next video of this series were actually very helpful to me to sit down and be productive and work on this project the next video of the series is going to walk through phase one of this project so everything I described in the previous slide so at a high level this will consist of building a data pipeline more specifically an extract load and transform pipeline so what that'll look like is we'll start by extracting video captions from from YouTube using python then we'll load these video captions into a paret file and then we'll transform these captions into text embeddings so this is our data pipeline that I'll discuss in the next video since I am using an elt extract load and transform Paradigm here technically this will actually be in Phase 2 elt sounds better than e so that's why I put it all here okay so that brings us to the end I hope you got some value out of this video like I mentioned in the previous video this whole series is part of my own own personal learning process so if you have any questions or suggestions on this project please drop those in the comment section below those are very valuable to me and as always thank you so much for your time and thanks for watching
O5i_mMUM94c,2024-04-19T14:05:54.000000,How I’d learned #datascience (if I had to start over) ￼,here's how I'd learn data science if I had to start over I start by making a YouTube playlist with high level introductions to data science and putting together a list of Articles to read on the subject next I'd set up 10 interviews with data scientists to learn from people that are actually doing it I'd ask basic questions like what is data science how did you get into it and do you have any advice for me next I'd do a project learning from others can only get you so far eventually you need to get your hands on the keyboard if I was super green I'd start with a basic project using data from kaggle however if I'm starting with some adjacent experience I try to grab some data from The Real World and solve a real problem then finally I teach someone what I learned by making a YouTube video about the project or writing an article
xm9devSQEqU,2024-04-18T15:59:02.000000,4 Skills You Need to Be a Full-Stack Data Scientist,although it is common to delegate different parts of the machine learning workflow to specialized roles there are many situations which require individuals who can manage and Implement ml Solutions end to end I call these individuals full stack data scientists in this video I will introduce full stack data science and discuss its four hats and if you're new here welcome I'm Shaw I make videos about data science and Entrepreneurship and if you enjoy this content please consider subscribing that's a great no cost way you can support me in all the videos that I make starting with the basic question what is a fullstack data scientist so the way I'll Define it here is that a full stack data scientist is someone who can manage and Implement an ml solution from end to end in other words they have a sufficient understanding of the entire ml workflow which gives them a unique ability to bring ml solutions to reality so typical ml workflow might look something like this you'll start by diagnosing the business problem and designing an ml solution to that problem next with a design in mind you'll move on to sourcing and preparing the data for solution development then you'll develop the solution so in other words you'll train a machine learning model and then finally you will deploy Your solution so you'll integrate that machine learning model into existing workflows or into a product given the rise of specialized roles for each aspect of this machine learning workflow this idea of a full stag data scientist might seem a bit outdated and this was my thinking when I was working as a data scientist at a large Enterprise where we had a data engineering team and an ml engineering team and I was sitting on the data science team however over time the value of learning the entire Tex stack has become more and more obvious to me the spark for this realization and change in perspective for me happened around last year where I was interviewing top data science Freelancers on upwork one of the key takeaways from these interviews was that data science skills alone provide no value while this might sound like a provocative statement think of it like this if I'm a freelancer I'm probably going to be talking with small to medium-sized businesses and most of the times these businesses don't have a data science function that's the whole reason they're hiring a freelancer and so they often don't have the data infrastructure to provide the foundation for training machine learning models that means if I want to come in as a data scientist and train a machine learning model I need to be able to extract the data prepare the data and make it available for training but it doesn't stop there once the model is trained it needs to be integrated into their existing workflows and again they probably don't have a machine learning engineer on staff that can do this work so in order for the value to be realized that's something I would need to do as a freelancer the data science skills the model training piece of the ml workflow is sandwiched in between the data engineering piece and the ml engineering piece while it is an important part of the workflow it can't even happen if the data aren't available for model training and it can't provide any impact if it's not implemented into the real world however freelance isn't the only context where knowing the full Tech stack is valuable even if you're not a freelancer working with small to mediumsized businesses say if you're a full-time employee at one of these companies they are often in the early stages of their data maturity and AI maturity so you might be the only resource or part of a small team of resources that are responsible for implementing the AI strategy of the company another situation might be you work at a large Enterprise but you're embedded in a team where you are the lone AI contributor so in that situation you may not have a ton of support on the data engineering side or the ml engineering side to implement machine learning Solutions and then finally if you're a Founder that wants to build a machine learning product you're going to need skills from all aspects of the Tex deack because often you're the only person in your company and it's on you to build the product from end to end so that brings us to what I like to call the four hats of a full stack data scientist and each of these corresponds to key parts of the machine learning workflow so hat one is the project man manager so that diagnosing problems and designing Solutions piece of the workflow hat two is the data engineer so the sourcing and preparing of the data hat three is the data scientist so training the machine learning model and hat four is the ml engineer which consists of deploying the ml solution starting with hat one the project manager the way I see it the key role of a project manager is to answer three questions what why and how more specifically what are we building why are we building it and how are we going to build it while this might sound simple enough it's not uncommon for people to gloss over or skip this step entirely perhaps especially for technical folks who really want to dive into the implementation and building the model so it might be like this Meme here the technical folks are more excited about coding than necessarily doing this project management work but the reason it's important is that if you skip over this step you run the risk of spending a lot of time money solving the wrong problem and even if you are solving the right problem you may solve it in an unnecessarily complex and expensive way all that to say taking some time at the outset of any project to stop and think about the problem you're trying to solve and the solution that you want to build can save you a lot of time and wasted effort so the way I see it the key skills involved with the project manager hat are communication and managing relationships the reason for this first one is that as data scientist s or full stack data scientists you're probably not going to be solving your own problems more often than not you're solving other people's problems and what this typically looks like is you're talking with stakeholders to better understand their problem and talk through potential Solutions the next key skill is the ability to diagnose problems and design Solutions diagnose problems comes down to finding the root cause for why something is going wrong and then designing Solutions isn't just about automatically throwing AI at problem but thinking through the value and the costs of each potential solution in making your decision and then the final key skill is being able to estimate project timelines costs and defining requirements and again while this work may not seem as exciting as the technical stuff the coding the implementation Etc doing this step right can save you a lot of headaches down the line next we have hat two which is that of a data engineer so in the context of full stack data science what data engineering is all about is making data readily available for model development or inference data Engineering in this context has one key difference than what we might call traditional data engineering at a large Enterprise at a large Enterprise the bulk of the data engineering work is often optimizing data architectures to support a wide range of business use cases on the other hand in the context of full stack data science the work is typically more product Focus focused while having some understanding of how to design flexible databases is important the type of data engineering work you would do in this full stat context is more concerned with building data pipelines so this will be creating ETL processes which stands for extract transform and load as well as data monitoring so giving visibility to data flowing through your pipeline and all the data related to your machine learning product some of the key skills that come up here are are much more technical than what we saw on the previous slide so python has really become a standard among data Engineers python can be used for a wide range of tasks such as the extract process so scraping web pages or working with apis transforming the data so this could be things like d duplication exception handling feature engineering knowing SQL is a must especially if you're loading this data into a database which is going to be queried in some Downstream task also a basic understanding of command line interface tools while there are a lot of gooey based applications for data engineering being able to use command line tools allows you to automate and scale processes a bit more easily next we have building data pipelines so this could be things like ETL or elt again ETL is extract transform and load while elt is extract load and transform these will depend on the details of your use case that I'll talk about in a later video but common tools for building data pipelines are airflow which is an orchestration tool and Docker which is a containerization tool and then finally while you can definitely have your own servers and compute to acquire and store your data these days it's common to implement these data pipelines and data stores on some sort of cloud platform The Big Three are AWS gcp or Azure next we have Hat number three which is the data scientist hat my definition of a data scientist is someone who leverages regularities in data to drive impact and since computers are much better than us at finding regularities and patterns in data what this often boils down to is training a machine learning model what this typically looks like is you start with the real world which consists of things that you care about what we do is we'll collect data about those specific things that we care about and then we'll use that data to train a model and the model can be used to make a prediction such as the probability that someone will buy our product based on their demographics or their behavior or something else it could be the probability that they don't pay back their credit card bill based on their credit score and other things and so on and so forth there are countless applications of machine learning models but the role of the data scientist doesn't just stop with training the model what's just as important if not more important is how one evaluates the model so defining performance metrics that are meaningful and in the best case scenario the performance metrics you use to evaluate the model can be tied back to key business performance metrics so that there's a clear mapping between model performance and business impact and the nature of training models is very experimental and iterative so what often happens is that you'll go through this whole workflow you'll evaluate the model and you'll learn something and that'll create a feedback loop for the data scientist to update the algorithm used to train the model or the specific hyperparameters used or the specific features used in the model or the data scientist might realize there are inconsistencies in the data which requires to kind of go back to the data engineering step of the workflow and make some changes in how the data are transformed or they may realize that the data aren't sufficient so one has to go back and collect even more data and finally you may evaluate the model and realize that some of of your assumptions about how the real world processes work were flawed for example at the project outset you may have assumed a particular variable was a key driver in something that you cared about but then upon training your model you may realize that that specific variable doesn't have the predictive performance that you initially hoped so this requires you to kind of go back to the drawing board and rethink your assumptions about the specific use case this very iterative and experimental whole process makes data science in many ways more art than science which is one of the reasons I like doing it so much flexus one's creativity but it also introduces a fair amount of uncertainty into the model development process some of the key skills of the data scientist had is probably first and foremost python some common data science libraries include pandas and poers this provides data structures for working with data and ways to manipulate data there's sklearn which is a popular machine learn learning library it has several machine learning algorithms readily available and then finally there are deep learning libraries like tensor flow or pytorch which allow you to train neural networks another key skill is exploratory data analysis even before you train the model there's usually a step in between here where one looks at the data looking at distributions looking how different variables track with one another if there any missing values if there any duplicates double checking the quality of the Data before training a model on it and then finally model development and everything that goes into that one of the most important things is experiment tracking because there are so many knobs in dials that go into training a model such as the algorithm you choose the hyperparameters for the algorithm the train test split of the data the features used in your model the performance of the model and keeping track of all this information is very valuable when you're trying to discover the best performing model the fourth and final hat is that of the ml engineer and what this consists of is turning your machine learning model into a machine learning solution what I mean by that is a model in of itself provides very little value it's probably going to be implemented in Python so it's going to require you to run a python script and provide a particular data and then it'll spit something out and that output may not be inherently meaningful by itself so taking that model and embedding it into an existing workflow or a broader software solution is a critical part of this process so a very common way of deploying machine learning models is as follows you'll take your model and you'll containerize it which means you'll create a modular version of the model that can be deployed in many different contexts and this is typically done using Docker however the model just sitting in this container still doesn't provide a whole lot of value so a simple way to allow this container to talk to external applications or external workflows is by adding an API to it what this allows is that if you have some AI app let's say this is an internal website that allows employees to look up specific information users can write in a query into the user interface this will get sent to the machine learning model and then the API will spit back a response this is a very common and simple design you just have a container container version of the model and you slap an API on top of it two popular tools for doing this are Docker for the containerization bit and then fast API which is a python library that allows you to create apis for Python scripts however some use cases may not be so simple and require more sophisticated Solutions you know maybe something like this where you have your production model with the API SLA on top of it but you also want to do consistent model monitoring and you want to do some automated model retraining say every single month so if you want to do the retraining you're going to have to ingest data in an automated way do the ETL process to put it into a database maybe you also want to do some data monitoring so you slap that on top of the database as well this process data gets passed to a model retraining module which then gets pushed to the production model after some automated checks or something like that so for these more sophisticated Solutions you're probably going to want to use an orchestr ation tool like airflow which provides a abstract way to connect these different pieces of software together so the key skills for the ml engineering had is to containerize scripts using Docker and to build apis using perhaps fast API another key skill is orchestrating multiple data and machine learning processes together so connecting data and ml pipelines and a popular tool for that these days is airflow and then again while you could Implement all these Solutions on your local machine or some local hardware it's common practice these days to deploy these Solutions in the cloud while I've been doing data science for 5 years I would say I am just at the beginning of my journey toward becoming a full stack data scientist and while it might seem like this daunting and overwhelming task of learning the full Tex stack the way I think about it is that it's not about learning everything it's not about learning every single detail and skill involved in the machine learning workflow but rather it's about learning anything necessary to implement your particular solution and so the way I see it the best way to become a full stack data scientist is taking a more bottomup approach as opposed to a top- down approach as problems arise learn just enough to solve that problem and move on to the next thing on this journey of becoming a full stack data scientist here are three principles that I'm personally following the first is to have a reason to learn new skills there are many ways one can do this I'm personally building out my own projects and products both as a way to learn and as a way to solve specific problems that come up for me however there are other ways Beyond personal projects you know freelancing is a great opportunity instead of solving your own problems you're solving other people's problems which are going to require you to learn all aspects of the tech stack and indeed most of the Freelancers I know have skills across the entirety of the text de the second is to learn just enough to be dangerous this goes to this idea of not worrying about learning every single little detail but learning whatever is necessary to solve the problem in front of you then finally to keep things as simple as possible there are countless tools Technologies libraries Frameworks Solutions best practices for doing machine learning these days and it's easy to get so caught up in the best PR practices and what's scalable that you end up over complicating the project so in my view Simplicity is the best guide for building machine learning Solutions this video is part of a larger Series in upcoming videos I will Implement a machine learning project end to endend walking through each of the four hats discussed here so specifically I'm going to build a semantic search system that allows people to search across all of my YouTube videos I'll walk through each hat where I'll have a video for each one of these I'll do the project manager hat walking through AI project management estimating time and costs defining requirements I'll do the data engineering stuff which is walking through the data acquisition building the data Pipeline and creating the data store then hat three I'll walk through the solution development the experimentation phase and then evaluating the solution finally I'll do a video on the ml engineering so deploying the solution the container process and building an API so that brings us to the end I hope you got some value from this video this video and the others in this series are all part of my own personal learning process toward that end if you feel like anything's missing or you have suggestions for future content I invite you to drop those in the comment section below those are very valuable to me personally and as always thank you so much for your time and thanks for watching
Z6CmuVEi7QY,2024-04-11T10:00:27.000000,How I'd Learn Data Science (if I started over),when I was first learning data science it was easy to get overwhelmed by the mountain of buzzwords and technical details and hard to know exactly where to start while the number of buzzwords has only seemed to increase since I started I now have a much clearer view of the space and its essential elements in this video I'll answer one of the most common questions I receive which is how would you learn data science if you had to start over today I can honestly say this is the exact approach I would use because I'm currently foll following it to learn data engineering and ml engineering and if you're new here welcome I'm Shaw I make videos about data science and Entrepreneurship and if you enjoyed this content please consider subscribing that's a great no cost way you can support me in all the videos that I make I've been doing data science for the past 5 years in a few different contexts including research freelance and corporate one of the biggest challenges of learning data science is that it consists of a constantly changing ocean of Technologies the best way I found to cope with this constant change is to just get really good at learning and the way I see it there are four ways we can learn the first way is through consuming content this includes reading books taking courses watching YouTube videos second is through mentorship and learning from those who are ahead of you third is by doing so getting your hands on the keyboard and the fourth is by teaching when it comes to learning a comp completely new subject or field I combined these four methods into a four-step strategy here I'm going to walk through what that might look like for data science step by step the first step is to consume content this is especially important if I know absolutely nothing about the field I'm always amazed by the amount of clarity I can gain from Reading one good article or watching One Good YouTube video on a subject unfortunately what makes a good resource will vary from person to person for example I prefer to read revieww papers and other scientific articles when available however if you would have given me a paper to read when I was an undergrad I probably wouldn't have read it and even if I did I probably would not have gotten much out of it so it's important to find which modalities work best for you to get started I'd go to Google Scholar and search something like data science review here I'd be mainly looking for titles that sound interesting and relevant and looking at the number of citations each article has received to be clear this isn't meant to be a literature review so I'll typically pick out one to three articles that seem relevant next I'd go to YouTube and search something like what is data science similar to Google Scholar I'd evaluate videos based on their title and number of views so while doing these searches it's important to point out that I'm not actually reading these papers or watching these videos I'm just getting all these resources together to consume them later to actually consume the content I'll block time on my calendar specific spefically for learning this is important because it increases the probability that I'll actually do it I personally like to do my reading first thing in the morning while watching YouTube videos is something I can really do anytime either way I'll have my notes app open while consuming the content to take notes on key things that stand out to me and ensuring I site my sources in case I want to come back to them later or share it with anyone else although I recommend you do this kind of search for yourself I've linked a few resources in the description below in case it's a helpful jumping off point after getting a basic grounding of data science through consuming content I start reaching out to professional data scientists to see if they'd be willing to talk to me about their experience learning from those ahead of me is one of the greatest hacks I've learned for growth this is something that I did when first exploring entrepreneurship in the data space and something I'm doing currently in learning data engineering and ml engineering I found that interviewing 10 people is a good number because it gives a nice diversity of perspectives to help give a broad view of the field but of course getting 10 Busy professionals to talk to you is much easier said than done here's how I would do it I'd go to LinkedIn and search for data scientists that I'm directly connected with then I'd shoot them a DM saying something like hey hope things are going well I'm currently doing interviews with data scientists as part of an effort to expand my technical data skills would you be willing to do a call sometime this month if the person doesn't respond within a few days I'd follow up with something like hey I wanted to follow up did you get a chance to look at my previous message doing this recently for data and ml Engineers more than 50% of people responded to my messages of course this will vary from person to person but if we assume that 50% of the people you reach out to respond and of those 50% another 50% agree to talk to you that means you need to reach out to 40 data scientists in order to talk to 10 of them I'm personally connected to hundreds of data scientists on LinkedIn so this would be no problem for me however if I was only connected to say 20 data scientists I'd take the following approach I'd add more personalization to that initial reach out and be sure to follow up a third time for those who don't respond to the first two messages this should increase the number of people that actually respond to my request if this doesn't get the 10 interviews I'd expand my Outreach to data scientists I have at least one mutual connection with reaching out to strangers naturally has a lower success rate so personal iation and follow-ups are even more important for these contexts when actually getting on the calls with these professionals I'd ask basic questions like what is data science how did you get started what kind of projects are you working on these days what does your Tech stack look like and do you have any advice for me I'd be sure to take notes on these calls so I can compare different interviewees perspectives and do any deep dives into specific tools or resources that they mention in other words i' jump back to Step One to to fill in any gaps that may have come up during the interviews consuming content and learning from others can only get you so far eventually you need to get your hands on the keyboard and start doing this is where doing a data science project comes in if I was completely new to programming and analytics I would start with a very basic project like creating a simple data visualization what this might look like is installing python on my machine downloading an interesting data set from kaggle loading it in with pandas or polers processing the data in some way whether that's cleaning the data or doing some sort of analytics and then finally visualizing the data with matte plot lip if I was starting with some adjacent experience in say python or statistics I'd try to grab data from The Real World and solve a meaningful problem some examples of this might be creating a model to automatically create chapters for my YouTube videos predicting the click-through rate of a video based on the thumbnail and the title or building a web app that allows people to do semantic search over all my YouTube videos and blogs I find that doing projects to solve real world problems are not only more rewarding but much more instructive than purely academic ones as I make progress on the project I'd likely be jumping back to steps one and two as needed so to either read up on a specific approach or technology or to ask people with more experience for guidance in case I get stuck once I've completed my first project i' move on to the final step of teaching in my opinion teaching is the ultimate way to learn and the main reason why I make YouTube videos when explaining something to others I'm forced to distill my understanding into clear and concise language in this process I find that I'll stumble upon gaps in my understanding that I didn't realize were there this provides me the opportunity to bridge these gaps and learn more the three teaching modalities that I found most helpful are making YouTube videos writing medium articles and giving presentations I really like the first two methods cuz I can do them anywhere at any time the third method is also great because I get the added pressure of talking to a live audience and taking their questions but of course this requires a bit more work to make happen so to keep things simple I'd make a slide deck outlining my project and key learnings and then make a YouTube video of me walking through it although many will agree that teaching is a great way to learn most won't follow through on this they might be thinking what if I say the wrong thing what are people going to say about me what if I embarrass myself the best strategy I found for overcoming these fears is focusing on my own personal development and growth in other words it doesn't matter what other people think as long as I'm learning and for me these fears actually serve as F to help make sure that I really understand what I'm sharing and in the inevitable case that I do make a mistake that's beneficial too because those those public mistakes tend to stick much better than the private ones while this four-step strategy is simple putting it into practice may present some unexpected challenges to help with that here are three key habits I find helpful for learning first is to make room for it like I mentioned earlier I find it critical to block time for learning to ensure that it actually gets done for me this has helped develop this habit for continual learning the second is be willing to look dumb one of the biggest mistakes I made early in my educational career was an unwillingness to look dumb what this typically looked like was staying quiet and not asking questions when someone was telling me something I didn't really understand when I overcame this fear in grad school it unlocked a whole new level of learning and progress and third pursue curiosity again data science is an ocean of Technologies even if you spend every waking moment of everyday learning you still wouldn't come close to learning everything that's why I find it much more productive to pursue topics that peique my interest this is what makes data science fun even if the topics are technically challenging at first so that brings us to the end I hope this video was helpful to you in some way if you have any data science questions feel free to share them in the comments below or feel free to set up some office hours using the link in the description and as always thank you so much for your time and thanks for watching
INlCLmWlojY,2024-04-04T18:45:00.000000,I Was Wrong About AI Consulting (what I learned),"last year I quit my corporate data science job to pursue entrepreneurship full-time my plan was to sell data science Services as a way to fund the development of a product I could build a business around while this made a lot of sense on paper pursuing this path over the last N9 months has made me realize this plan was flawed in this video I'm going to share my experience and some key Lessons Learned in case it is helpful to anyone on a similar journey and if you're new here welcome I'm Shaw I make videos about data science and Entrepreneurship and if you enjoy this content please consider subscribing that's a great no cost way you can support me in all the videos that I make right out of grad school I went to work as a data scientist at Toyota this was in many ways my dream job and an incredible learning experience for me however after about 6 months in the role that initial excitement and learning curve began to flatten out and I slowly began to realize that the role was no longer aligned with my longer term goal of running my own business so after about a year in that role I decided to pass on a senior data scientist promotion and tank my income from over 10K a month down to basically zero since I had done some freelance work in grad school and had grown a small audience on YouTube my plan was to bring these things together and leverage my content to sell consulting services and to my surprise it worked over the next 8 months I took 36 Discovery calls of these 36 calls two of them turned into contracts and last month one of these contracts turned into an even bigger opport opportunity of over $25,000 where I was sitting in a project manager role and not doing any of the coding myself while it may sound like things were going great something was off this was similar to what I felt facing the promotion at Toyota it was a great opportunity on paper but something about it didn't feel aligned with my long-term goals so I made the tough decision to pass that opportunity off to another consultant looking back it's clear that my expectations of Consulting didn't match reality when I started this journey I saw Consulting as an easy way I could make cash while I explored other Ventures however after pursuing it as my main source of income as opposed to a side hustle like I did in grad school it became obvious that running a Consulting business wasn't as simple as I expected not just because of the technical challenges of building AI projects but also selling yourself nurturing leads working with subcontractors and the list goes on and on in fact most of the work were these non-technical aspects of the job with the biggest piece being the sales process as I've learned there are many unique challenges in selling AI Services three of which are as follows one for most businesses AI is a nice to have rather than a musthave so a lot of times it's not the client's number one priority two building AI projects requires a lot of experimentation and iteration which introduces a lot more uncertainty than the traditional software development process and reduces the perceived value of your offer and three since these are typically High ticket contracts they often require multiple touch points with the client before they close and I found this extra time commitment difficult to manage as a solo operator although I was learning a lot Consulting was taking up much more of my time and attention than I had anticipated so much so that my content output began to slow down and I virtually had no time to work on my own projects which was supposedly the main goal of all this this experience led me to take a step back and reminded me of some advice I had received from a successful product entrepreneur about a week after quitting my job I had asked him if Consulting was a good stepping stone to product development to which he immediately responded no the advice he gave was simple if you want to build a product then build a product looking back it's kind of funny that it took me 9 months to realize what he had told me 9 days into this journey but here's what I didn't fully appreciate building a product is hard building a consultancy is hard building a brand is hard entrepreneur ship is just hard the trick at least in my opinion is to pursue the hard thing that gets you fired up and that you find fulfilling and after trying it for 9 months I realized that selling AI projects to clients didn't get me as fired up as some of the other things I was working on that's why last quarter I removed the discovery call option from my website and passed that first major contract off to another consultant although building a consultancy wasn't for me I still believe it's a great business for those who enjoy it it also taught me a ton about sales marketing and working with customers which are universally applicable skills and Entrepreneurship if I had to boil it down here are my four key takeaways from this experience first is trust is more important than anything else for me what differentiated clients from prospects was the belief that I could solve their problem and that I was on their side through a lot of trial and error I eventually landed on the following approach be curious be transparent and be yourself more specifically be curious about the client's problem and where they're coming from be transparent about the limits of my skills and knowledge and to just be myself not trying to put up a front and pretend to be something that I'm not the second takeaway was not to skip the discovery when providing Technical Services like data science it's easy to dive head first into the coding the problem with this is that people end up spending a lot of time and energy solving the wrong problem that's why at the outset of every project it's critical to put on your project management hat so you can understand the business problem and fully scope a proposed solution the third takeaway is to find your one sales Channel although there are countless ways you can get clients upwork Fiverr cold Outreach LinkedIn content creation speaking at conferences referrals and the list goes on and on I and most of the people that I've interacted with in the space have just one main lead source for me my main source was my YouTube channel Channel and my funnel looks something like this someone would watch a YouTube video book a discovery call after the discovery call we would do a paid Discovery phase where the goal was to get a clear understanding of the client's problem and to scope out the project requirements and goals following the paid Discovery is building a proof of concept and then after the POC building an MVP the fourth and final takeaway is that it's not real until the money's in your bank account this is a lesson I had to learn over and over again and maybe I still haven't learned it there were many times I would have a great discovery call or multiple calls with prospects and it seemed like they were ready to move forward but then days and weeks would go by and I wouldn't hear from them and so while there's always excitement in sales I had to adopt this mindset to avoid going on these weekly emotional roller coaster rides at this point you might be thinking Shaw if you're not selling your data sign skills how are you going to make money while contract work has great short-term earning potential it is not my only Revenue source there are three other ways I've generated Revenue these past 8 months this includes revenue from my YouTube channel my medium blog and ad hoc paid Consulting calls which have generated a total of $766 38 although this isn't enough to pay the bills there's another thing here that's worth taking into consideration since quitting my job my YouTube channel has grown from 2,000 subscribers to 18,000 subscribers along with that my revenue from YouTube went from $100 in the first 3 months to 1,600 in these past 3 months which brings me to my new plan post one YouTube video a week while this might sound like an overly simplistic and also super risky plan here's my reasoning one YouTube is actually working for me two it allows me to focus on one thing three making one video a week gives me a clear quantifiable goal I can use to structure all of my efforts for instance here's a list of things that can go into making a YouTube video reading papers writing medium articles writing code examples talking to people conducting interviews building projects workshopping content ideas on other social media platforms and probably a lot more now here's a list of things that can result from making a YouTube video learning a new skill or topic getting more paid calls more speaking gigs more inbound leads more people joining the data entrepreneurs more content from my other channels and growing my audience nevertheless committing to one thing is scary especially something unpredictable like YouTube however the longer I spend on this journey the more I realize that commitment and focus are necessary ingredients for Success because this is the only way that every ounce of your effort can go in the same direction and to quote a fellow entrepreneur and friend Michael Lynn if you're doing less and less that means you're going in the right direction and indeed this feels like the right direction at least for now 9 months into this entrepreneurship journey I have three Reflections that are top of mind the first is I could have a very successful Consulting business and I could have a very successful YouTube channel but I can't have both I have to pick one and personally I just like making YouTube videos more the second is a subtle mindset shift which is instead of asking yourself will this thing work ask yourself how could I make this thing work it may seem like a subtle shift but this is the mindset that I'm adopting this quarter in making YouTube my main focus and the third and final mindset is to trust yourself trust that you'll figure it out trust that if you're backed into a corner your survival Instinct will kick in and you will solve the problem thanks for watching to the end I hope you got some value out of this if you have any specific questions about my journey feel free to drop them in the comment section below and as always thank you so much for your time and thanks for watching"
sNa_uiqSlJo,2024-03-29T15:57:34.000000,"Text Embeddings, Classification, and Semantic Search (w/ Python Code)","although there's a ton of excitement these days around llms and AI agents I'm more excited about the recent Innovations in text embeddings we saw these in the previous video of this series where we use text embeddings to implement a rag system to improve in llm in this video I'll discuss text embeddings in Greater detail and share two simple yet high value use cases namely text classification and semantic search and if you're new here welcome I'm Shaw I make videos about data science and Entrepreneurship and if you enjoyed this content please consider subscribing that's a great no cost way to support me in all the videos that I make there's a fundamental challenge in trying to analyze text put one way text isn't computable meaning we can't do math with text in the same way that we do with numbers for example say you go to a networking event and you're talking with other Professionals in the data space if you wanted to summarize the typical height of every person at the networking event this is something that would be pretty straightforward all you would need to do is measure the heights of everyone at the networking event and then you can summarize these Heights using something like an average and this is something that you can easily compute using a calculator Excel or your favorite programming language however if you have the job descriptions of everyone at this networking event summarizing everyone's roles wouldn't be as straightforward because there's no cell function or mathematical operator that allows you to put in text and generate a summary and so this is where text embeddings come in and put simply text embeddings translate words into numbers so if we have those same job descriptions from the networking event translating these descriptions into text embeddings would look something like this for every single description here we would map it to a set of number numbers but these aren't just any set of numbers these are numbers that capture the inherent meaning of the text in other words text embeddings translate words into meaningful numbers and one way we can see this is take all those numbers from the previous slide and use them to visualize all the job descriptions and so that would look something like this where the numbers that we generated in the text embedding Define the location of each person's job description we can see the data analyst in retail with 5 years of experience is somewhere here and they are located next to a freelance data visualization specialist with four years of experience however these people are relatively far away from this guy who is a data architect with 15 years of experience and so this is the way that text embeddings capture the meaning of the underlying text namely job descriptions that are similar will be located close together while job descrip descriptions that are very different will be located far away from each other from this view the bi analyst in Hospitality early career is very different than the data architect with 15 years of experience so if you've used Chad GPT in the past the thing that might come to mind is Shaw why should I care about these text embeddings and translating text into numbers can't I just pass all my text to chat GPT and have it figure out what I'm trying to do and the answer is yes there are many tasks that don't require these text embeddings and they're much better suited for chat GPT for example if we wanted to summarize the eight job descriptions from the networking event it would actually be super simple to pass all these to chat GPT and generate a summary however if you have a use case that's not just some oneoff low stakes task but it's something you're trying to integrate into your product or a website or some broader production level Software System then you'd probably want to at least consider using text embeddings before running off and building an AI agent and the reason is if we're talking about AI assistance it's really the early days for this type of technology and there are still a lot of things that we don't understand about using these large language models in this way another downside of using an AI assistant for your use case is that there's a major computational cost associated with running these Large Scale Models another downside is the inherent security risks that come with building llm based applications and so there's a nice list of top 10 llm security risks at reference number three which I recommend you check out for example these are things like prompt injection which are maliciously crafted prompts which get the llm to expose private or confidential data or disrupting some expected decision-making pipeline finally responses from these llm based systems might be unpredictable and prone to hallucinations the system generates fictitious or unhelpful information on the other hand text embeddings have been around for decades and there are several past example applications that we can look to to help guide future applications and use cases another upside of using text embeddings is that they have a much lower computational cost and thus Financial cost than using an entire large language model another is that there are far fewer security risks associated with using embedding model compared to a large language model that users can access through a chat interface finally the responses are more predictable because you know every word or piece of text Will generate a deterministic set of numbers so there's no Randomness in this process now that we have a basic understanding of what text embeddings are and why we should care about them let's see how we can use them for a practical use case the first use case I'm going to talk about is text classification which is the process of assigning a label to a piece of text for example if we were to take all the people from our networking event and their Associated job descriptions a classification task could be trying to determine which people are data analysts and which people are not data analysts based on their respective job descriptions so that might be pretty simple here because anything to the left of this blue line we could label as a data analyst and anything to the right we could label as not a data analyst however text classification is a broadly applicable use case some other situations might be classifying emails as fishing attacks versus not fishing attacks or classifying credit applications as fraudulent or not fraudulent and the list goes on and on and on with this high level understanding let's see what text classification with text embeddings looks like in code so in this example I'm going to take a data set of resumés and try to classify them as either data scientist or not data scientist based on the content of each resume the code in the data set for this example is freely available on the GitHub repository linked here you you to steal this code as a jumping off point for your own use case so the first thing we're going to do is import some helpful libraries here we're going to use open AI text embeddings to use open AI embedding models we'll need to use their their API which requires a secret key if you're unfamiliar with the open aai API or how to get a secret key I walk through that step by step in a previous video of the series next I'll import pandas numai and matte plot lip so we'll use pandas to structure the data we'll use numpy to do math and we'll use map plot lib to visualize things along the way and then finally we will import some things from s learn specifically their random Force classifier which is just a machine learning technique for doing classification and then the ROC Au score which is a way to evaluate a classification model's performance here we're going to import our data from a CSV file and the CSV file is available on the GitHub repo just one note here the data used in this example and the next example were actually synthetically generated using GPT 3.5 turbo although I don't like using synthetic data in these code examples I went with the fake data set here to avoid any privacy issues in using real people's resumés nevertheless this example is instructive in how you can apply text classification to text embeddings next we're going to generate our embeddings so here I'm going to define a function to do this for us basically what this function will do is take the text and my open AI secret key and it'll spit out the text embedding object the way we do that is we set up communication with the open a API and then we just make an API call by passing at the text and specifying which model we want to use this is one of open ai's newer embedding models and it has about 1,500 Dimensions so each piece of text is going to be translated into about 1,500 numbers and then they have another option which has about 3,000 numbers and that's called text embedding 3 large and then the function will return the API call response and then here for every resume in our data set we can generate the embeddings in one line of code then we can extract these text embeddings and store them in a list in this line of code here then we'll store all the text embeddings into a new pandas data frame called DF train so here I'm automatically generating column names so these column names will be embedding underscore 0 embedding underscore 1 embedding underscore 2 all the way up to embedding score like 15 35 or something however many embedding Dimensions there are then we can create the pandas data frame in one line of code so just by passing in this list of text embeddings and the column names and then I'm going to add one more variable to this data frame which is a true false a Boolean variable which indicates whether that row is associated with a data scientist's resume or not a data scientist's resume and then we can split our variables into a set of predictors so X will be all the text embeddings for every one of our resumés y will just be the true false is data sign scientist is not data scientist variable and then we'll pass this to the random Force classifier to train a model so just in one line of code we train our classifier we can print the accuracy and Au value of the model applied to the training data so the output here is one indicating perfect performance of the model on the training data which is super suspicious so let's evaluate the model on a different data set here we're going to load in a testing data set so this is also available on the GitHub repository here we're just repeating the same steps as we did for the training data we'll read it in as a pandas data frame we'll generate the embeddings in one line of code we'll extract the embeddings as a list then we'll create a new pandas data frame for the testing data and then we'll create a Target variable of is data scientist for the test data and then we'll split it into a set of predictors and the target variable with that we can simply evaluate the model on this new data set with the two lines of code and we see the model still has very good performance on the training data however before we kind of celebrate the model is still likely overfitting here and there are two reasons for that first is we have 1,537 predictors but only 100 records in these situations where you have a ton of predictors and very few examples it's not hard for the model to memorize the training data and overfit to it secondly the training data and testing data were generated in identical ways using GP PT 3.5 turbo and so this is one of the downsides of generating synthetic data is that they're going to be artifacts in the data generating process which may create similarities between our training data and testing data that won't be relevant to resumés from The Real World so while there are many ways we can solve the overfitting problem in this situation the best thing would be to one get many more examples of résumés so 100 RS isn't going to cut it probably want close to 1,000 or 10,000 résumés and then second if you want to use this model on real world resumés you should train it on real world resumés moving on to the second use case of semantic search the big idea here is that semantic search returns results based on the meaning of a user's query in contrast to keyword search which looks to match specific words or phrases in the query to a set of search documents or web pages and so the way this works in the context of text embeddings is that say we have a query like I need someone to build my data infrastructure if we were to take everyone from our networking event once again and embed their job descriptions in this concept space here the way we can do semantic search is we can embed the user query into this same space and if we were to do that the query might sit somewhere over here then if we wanted to return search results we could look at the three nearest job descriptions and return them in our search results and for this particular query keyword search may not work so well because there's no mention of the typical role that would be able to build a data infrastructure namely a data engineer however semantic search is able to connect this task of building a data infrastructure to things like data engineer or building data pipelines or ETL process or building a data model and other related skills and activities to building a data infrastructure which may not share the same exact wording here we're going to use text embeddings to implement semantic search in exactly the same way as we saw in the example on the previous slide so we'll use the same data set as in use case one and what we'll do is we'll take a user query such as I need someone to build my data infrastructure and return resumés in our data set which match that user's query again we're going to import some handy libraries we have numpy and pandas and next we import sentence Transformers which is actually an open-source library with many embedding models that you can use completely for free so no need to get an API key or pay to make API calls to open ai's API next again we're going to import things from Psychic learn namely this PCA function function which allows us to do dimensionality reduction and if you're unfamiliar with PCA I actually talked about this in an old video so I'll link that on the screen so you can check it out if you're interested and then we'll import this distance metric function which will allow us to measure the distance between the query and all the different resumés in our database finally we'll import matplot lib to visualize some things okay so here we'll read in the data in the same exact way as before so we'll read in this rumore train.csv file one thing we'll do quickly is that this CSV file has a column called rooll and then there are a handful of different roles but there's one of them that's a sentence it's not a specific role we'll just relabel all those elements as other next we're going to generate the embeddings so the sentence Transformer Library makes this super easy so all we do is import the model and then we can use this do encode method to pass in all the resumés and generate embeddings for all of them and so there are several embedding models to choose from in the sentence Transformers Library here I use this one all mini LM L6 V2 which was specifically designed to do this semantic search task if you're trying to do text classification or some other NLP task I would check out their pre-trained models in their library and try to tailor the model for your specific use case next we can visualize all the different resumés and roles in this embedding space in the following way what's Happening Here here is that I used PCA to reduce the dimensionality of the embedding space from 384 down to 2 so it can fit on this 2D plot here and since this is a lot of code to do the data visualization I don't want to show it here but it's available on the GitHub for anyone who's interested here we can see that even though we're flattening this embedding space from 384 Dimensions down to two we can see that the different roles such as data scientist data engineer machine learn learning engineer AI consultant data entrepreneur and other they tend to be localized pretty well in this visualization so this is a good sign because if we have a query that aligns with a data scientist it will likely be close to a lot of data scientists that can fulfill that role next we can Define our query so we'll use that same I need someone to build out my data infrastructure and then we can incode it in the same exact ways we encoded the resumés using this do encode method that'll generate a one dimension numpy array called query embedding then we can compute the nearest neighbors and so the way we do that is first I'll Define this dist object which is a distance metric and then we can compute the pairwise distances between the query embedding and all the resume embeddings in this embedding array that we defined earlier all these hairwise distances will be stored in discore Array which is a numpy array of the distance between the query and every single resume in the data set and then we can use this ARG sort function from numpy to sort all of the distances in ascending order note that this isn't sorting the disc array it's actually doing two things it's sorting the disc array in ascending order and then returning the index of each of those elements so the first element is not going to be the smallest distance value itself it's going to be the index of the smallest distance value that's all we need to do the semantic search so what that'll look like is if we want to print the roles of the top 10 results we can go back to our original DF resume data frame look at the roll column and then we can just return the first 10 elements in this disc array sorted array that'll look like this so the query I need someone to build out my data infrastructure returns a ton of data engineers which is a good sign because those are the people that will be able to do that another thing we can do is to look at the resume of the top result and so we can do that in this way we go back to our DF res data frame look at the resumé column and then return the resumé that corresponds to the first element in this Idis array sorted array and so the resume looks like this just reading through this highly skilled and experienced data engineer with a strong background in designing implementing and maintaining data pipelines proficient in data modeling ETL processes and data warehousing Adept at working with large data sents and optimizing data workflows to improve efficiency and so notice that nowhere here does it say built data infrastructure but it does say all these things that go into building data infrastructure designing implementing maintaining data pipelines data modeling ETL processes data warehousing scalable data pipelines optimiz ETL processes data architectur there's so many buzzwords and so much jargon in this space and just matching keywords together may not be super helpful but when you use text embeddings which captures the underlying meaning of the text it tends to give good results visualizing these queries in the embedding space it'll look something like this so that same one I need someone to build out my data infrastructure we see that the query is pretty close to all the data Engineers which is a good sign another one is project manager for AI feature development so this one tends to be closer to the machine learning engineers and the AI Consultants and it's kind of like on the border between the two which makes sense because you're going to need an ml engineer for the feature development but you probably want an AI consultant for the project management side of the project and so probably a mix of those two skills will be best and then finally a data engineer with Apache airflow experience pretty close to the data Engineers but it also seems close to this other category which is just randomly generated resumés okay so before celebrating I want to kind of zoom into this particular query here data engineer with Apache airflow experience for this simple semantic search example it seems to struggle with specific search requirements such as I want a data engineer but I specifically want them to know Apachi airflow and so when I pass this into the semantic search system only one of the top five results had airf flow listed on their resume and of the top five it was the third one that had it even though there were three other resumés in the data set that had Apache airflow experience and so this example illustrates that semantic search isn't better than keyword search in all situations each of them has their pros and cons and so if you want to build a robust search system you'll likely want to employ both keyword-based search and semantic search to get the best of both worlds which brings us to a few additional strategies for improving a search system first is hybrid search and so this is exactly what I just mentioned which is bringing together keyword-based search and semantic search while there are many ways we can bring these two approaches together a simple method would be given a user's query apply a keyword based search to it so you'll filter down the search results based on specific words in the query such as data engineer or Apache airflow and then from those results you'd apply the semantic Sur search another option is to use a ranker and so a ranker is a special type of model which takes in a query in a document for example and it spits out a similarity score notice that this is an alternative way to compute the similarity or the difference between two pieces of text common way to use a ranker is let's say you use your semantic search system and you return the top 25 search results from the semantic search you can use the ranker to take those 25 search results and compare them with the query to do an additional ranking and empirically this seems to have pretty good results especially when integrated into a rag system and then finally you can fine-tune an embedding model for a specific domain or use case one of the downsides to these embedding models is that they tend to be trained on a large Corpus of text so while this makes them very very good at general purpose tasks they may fall short in specific domains where there may be heavy use of Jaron for example if I wanted to further improve this semantic search system navigating all the jargon of data engineering ml engineering and data science I could Additionally fine-tune the embeddings on this type of text so if you enjoyed this video and you want to learn more check out the blog published in towards data science and although this is a member only story you can access it completely for free using the friend Link in the description below while this does bring us to the end of the llm series at least for now if there's anything that wasn't included in the series please feel free to drop that in the comment section below and perhaps the series will be resuscitated once again in a few months and as always thank you so much for your time and thanks for watching"
Ylz779Op9Pw,2024-03-18T17:32:21.000000,How to Improve LLMs with RAG (Overview + Python Code),this video is part of a larger series on using large language models in practice in the previous video of this series we saw how we can efficiently fine-tune a large language model to respond to YouTube comments in my likeness while the fine-tune model did a really good job at capturing my style when responding to most YouTube comments it didn't do so well when responding to technical questions which required more Niche and specialized knowledge in this video I'll discuss how we can improve llm based systems using retrieval augmented generation or rag for short I'll start with a highle overview of rag before diving into a concrete example with code and if you're new here welcome I'm Shaw I make videos about data science and Entrepreneurship and if you enjoy this video please consider subscribing that's a great no cost way you can support me in all the content that I make a fundamental feature of large language models is the ability to compress World Knowledge the way this works is you take a huge slice of the world's knowledge through more books and documents than anyone could ever read in their lifetime and you use it to train a large language model and what happens in this training process is that all the knowledge and Concepts and theories and events that have happened in the world that are represented in the text of the training data they get represented and stored in the model's weights so essentially what has happened is we've compressed all that information into a single language model well this has led to some of the biggest AI Innovations the world has ever seen there are two key limitations for compressing knowledge in this way so the first limitation is that the knowledge that is compressed in a large language model is static which means that it doesn't get updated as new events happen and new information becomes available and for anyone that's used chat GPT and tried to ask it a question about current events probably have seen a message like this as my last update in January 2022 I don't have access to real-time information so I can't provide specific events from February 2024 the second limitation is that these large language models are trained on a massive Corpus of text the result of that is that they're really good at general knowledge but they tend to fall short when it comes to more Niche and specialized information mainly because the specialized information wasn't very prominent in their training data and so when I asked chbt how old I was it said that there was no widely available information about shahim tab's age he might be a private individual or not widely known in public domains one way we can mitigate both of these limitations is using retrieval augmented generation or rag for short starting with the basic question what is rag this is where we augment an existing large language model using a specialized and mutable knowledge base base so basically we can have a knowledge base that contains domain specific information that is updatable where we can add and remove information as needed the typical way we'll use a large language model is we'll pass it a prompt and it will spit out a response this basic usage will rely on the internal knowledge of the model in generating the response based on the prompt if we want to add rag into the mix it would look something like this so instead of starting with a prompt we'll start with say a user query which gets passed into a rag module and what the rag module does is that it connects to a specialized knowledge base and it will grab pieces of information which are relevant to the user's query and create a prompt that we can pass into the large language model and so notice that we're not fundamentally changing how using the large language model it's still prompt in and response out the only thing we're doing is augmenting this whole workflow using this rag module which instead of passing in a user query or prompt directly to the model we just have this pre-processing step to ensure that the proper context and information is included in the prompt one question you might have is why do we have to build out this rag module can't we just fine-tune the large language model using specialized knowledge so that we can just use it in the standard way and the answer to that question is yes so you can definitely fine-tune a large language model with specialized knowledge to teach it that information so to speak however empirically fine-tuning a model seems to be a less effective way of giving it specialized knowledge and if you want to read more about that you can check out Source number one Linked In the description below with this basic understanding of what rag is let's take a deeper look into this rag module to see how it actually works the rag module consists of two key elements first is the Retriever and second is the knowledge base so the way these two things work together is that a user query will come in it'll get pass to the retriever which takes the query and searches the knowledge base for Relevant pieces of information it then extracts that relevant information and uses it to Output a prompt the way this retrieval step typically works is using so-called text embeddings before we talk about how we can use text embeddings to do search let's talk about what they are exactly put simply text embeddings are numbers that represent the meaning of some given text so let's say we have a collection of words like tree lotus flower daisy the sun Saturn Jupiter basketball baseball satellite spaceship text embeddings are a set of numbers assoc associated with each word and concept that we're seeing here but they're not just any set of numbers they actually capture the meaning of the underlying text such that if we are to plot them on an XY AIS similar concepts are going to be close together while Concepts that are very different from each other are going to be spaced far away here we see plants tend to be located close together celestial bodies tend to be close together these Sports balls tend to be close together and things that you typically see in space tend to be close together and notice that the balls are closer to celestial bodies than they are to say plants because perhaps balls look more like celestial bodies than they do plants and trees so the way we can use this for search is say each of these items is a piece of information in our knowledge base you know we have some description of this tree a description of this lotus flower the description of Jupiter and so on and so forth what we can do is represent each item in our knowledge base as a point in this embedding space and then we can represent the user's query as another point in this embedding space and then to do search we simply just look at the items in the knowledge base that are closest to the query and return them as search results that's all I'll say about text embeddings and text embedding base search for now this is actually a a pretty rich and deep topic which I don't want to get into in this video but I'll talk about in the next video of this series next let's talk about the knowledge base say you have a stack of documents that you want to provide to the large language model so you can do some kind of question answering or search over those documents the process of taking those raw files and turning them into a knowledge base can be broken down into four steps the first step is we'll load the documents what this consists of is getting together the collection of documents you want to include in the knowledge base and getting them into a ready to parse format the key thing here is that you want to ensure the critical information in your documents is in a text format because at the end of the day large language models only understand text so any information you want to pass to it needs to be in that format the next thing you want to do is chunk the documents the reason that this this is an important step is that large language models have a fixed context window which means you can't just dump all your documents into the prompt and pass it to the large language model it needs to be split into smaller pieces and even if you have a model with a gigantic context window chunking the documents also leads to better system performance because it often doesn't need the whole document it might just need one or two sentences out of that document so by chunking it you can and ensure that only relevant information is getting passed to The Prompt the third step is to take each of these chunks and translate them into the text embeddings we saw on the previous slide so what this does is it'll take a chunk of text and translate it into a vector or a set of numbers that represents the meaning of that text finally we'll take all of these numbers all these vectors and load them into a vector database over which we can do the text embedding based search we saw on the previous slide so now that we have a basic understanding of rag and some key Concepts surrounding it let's see what this looks like in code here we're going to improve the YouTube comment responder from the previous video with rag we're going to provide the fine-tuned model from the previous video articles from my medium blog so that it can better respond to technical data science questions and so this example is available on on Google colab as well as in the GitHub repository the articles that we use for the rag system are also available on the GitHub and the fine-tuned model is available on the hugging face Hub so we start by importing all the proper libraries this is code imported from the Google collab so there are a few libraries that are not standard including llama index the PFT Library which is the parameter efficient fine-tuning library from hugging face there's Auto gptq Q which we need to import the fine tune model as well as Optimum and bits and bytes and if you're not running on collab also make sure that you install the Transformers library from hugging face with all the libraries installed we can just import a bunch of things from llama index next we're going to set up the knowledge base there are a few settings we need to configure in order to do this first of which is the embedding model the default embedding model on llama index is actually open AI but for this example I wanted to keep everything within the hugging face ecosystem so I used this hugging face embedding object which allows us to use any embedding model available on the hugging face Hub so I went with this one from ba AI it's called BGE small version 1.5 but there are hundreds if not thousands of embedding models available on the hugging face Hub the next thing I do is set this llm setting to none and the reason I do this is that it gives me a bit more flexibility in configuring The Prompt that I pass into the fine-tuned model and then two things I set here are the chunk size which I go with 256 characters and the chunk overlap this wasn't something I talked about but we can also have some overlap in between the chunks and this just helps avoid abruptly chopping a chunk in the middle of a key idea or piece of information that you want to pass into the model with all these settings configured we can can create this list of documents using this simple directory reader object and the load data set method here I have a folder called articles which contains three articles in a PDF format from my medium blog and what happens is this line of code will just automatically go through read the PDFs chunk it and store them in this list called documents so there's actually a lot of magic happening under the hood here the next thing I do is just a little bit of ad hoc pre-processing of the text there are chunks that don't include any relevant information to the meat of the article itself and the reason is these PDFs were printed directly from the medium website so there's a lot of text that is before and after the article itself that's not really relevant to the use case here so here are just three ad hoc rules I created for filtering chunks the first thing I remove is any chunk that includes the text member only Story the reason is this will typically be the text before the article and it'll look something like this it'll say member only story then it'll have the title of the article and then it'll have my name the author's name and then it'll say like where it was published it was 11 minute read when it was published and it'll have the image caption and some just irrelevant text to the article itself another rule I use here is that I remove any chunk that includes the data entrepreneurs this is text I include in the footer of each of my articles which links the reader to the data entrepreneurs community so you can see what that might look like is this is the last sentence of the article although each approach has its limitations they provide practitioners with quantitative ways of comparing the fat tailed inness empirical data probably not helpful to any questions that you're going to ask about this article and most of it is just text from the footer of the article and then finally I remove any chunk that has Min read which typically comes up in the recommendations after the article so we can kind of see that in this chunk of text here and of course this isn't a super robust way of filtering the chunks but a lot of times your pre-processing doesn't have to be perfect it just really has to be good enough for the particular use case and then finally we can store the remaining chunks these remaining 61 chunks into a vector data store using this line of code so now we have our knowledge base set up so index is our Vector database that we're going to be using for retrieval with a knowledge base set up the next thing we're going to set up is the retriever first we're going to define the number of docs to retrieve from the knowledge base and then we're going to pass that into this Vector index retriever object the two things we need to pass here are the index or the vector database and the number of chunks to return from the search next we assemble the query engine so the query engine brings everything together it takes in the user query and spits out the relevant context and so we can use the query engine in the following way so let's say the query is what is fat tailedness which is the same technical question we passed to the fine-tuned model in the previous video of the series and the query engine spits out this response object which includes the top three most relevant chunks but it also includes a lot of other information such as the file name that the chunk was retrieved from the page number the date accessed and some other metadata so in order to take this response and turn it into something we can actually pass to a large language model we'll need to do a little reformatting which I do in this chunk of code here and then we can print what that looks like and so this text is probably small on your screen but you can see that there are three chunks of text and this is ready to go and be passed into a promp pred and subsequently fed into a large language model so at this point we now have all the ingredients of our rag module we have our knowledge base which was created using three PDFs and then we set up the retrieval step which takes in a user query and Returns the relevant context from the knowledge base so the next thing we need to do is to import the fine-tuned model so that we can generate a response to the user query and so here we're importing a few things from the PFT and transform forers Library this is the base model that we fine-tuned in the previous video and then here we transform the base model into the fine-tuned model based on the config file available on the hugging face Hub and then we load the tokenizer this is all stuff I reviewed in the previous video of the series so I would check that out if you're curious to learn more so now that we have the fine-tuned model imported let's use it to respond to a technical question without using the rag system so the way that'll look is we'll create a prompt this is the same prompt from the previous video and I'm creating a prompt template using a Lambda function which will dynamically take the instruction string here and the user comment which I Define here to create the prompt so when we print it it looks something like this where we have the instruction start and end special tokens here we have the instruction string here and this goes on for a long time and then here we have the comment so with that prompt defined we can pass the prompt into our tokenizer to translate it from words into tokens we can then pass those tokens into our model to generate response and then we can print the output this is what the model says without any context from the rag system great question fat tailedness is a statistical property of a distribution in simple terms it refers to the presence of extreme outliers or heavy tails in the distribution for instance consider the distribution of heights in a population a normal distribution would have most people cluster around an average height with a few people deviating slightly from the mean however in a fat tailed distribution you would observe a larger number of people being significantly taller or shorter than the average in finance fat tailedness is often used to describe the presence of extreme Market moves or outliers in financial returns it's an important Concept in risk management and financial modeling I hope this helps let me know if you have any questions Shaw GPT overall the tone in the format is really good here it explains things in simple terms however the description here doesn't really match the description in my video on fat tailedness or the article on fat tailedness so let's see what happens when we inject the proper context into the prompt using the rag System created earlier we'll again create a Lambda function so here we're not just passing in the comments but we're also going to pass in the context of the comment and so this is something we'll get from our rag system but given that we have both we'll have the same instruction string as before but now we're going to inject the context into the prompt and then we'll put the comment in the same place as it was before so with this new prompt template we can pass in the context and comment to get a new prompt we'll pass in the prompt to the token tokenizer in exactly the same way as before to generate tokens and we'll pass those tokens into the model to generate an output and then we'll print the results here's the model's response when we include context using rag great question fat tailedness is a property of a distribution that describes how much the presence of extreme values rare events affects the overall statistics of the distribution in the context of the video the author discusses how mediocris and extremist distributions differ in terms of fat tailedness mediocris distributions have a relatively small impact from extreme values while extremist distributions are heavily influenced by extreme values the author also mentions that fat tailedness is a more General concept than specific distributions like Paro or power laws instead it lives on a spectrum ranging from thin-tailed gausian to very fat tailed Paro 8020 I hope that clarifies things a bit let me know if you have any questions shot GPT so this does a much better job at capturing the way I describe fat tailedness in my video it defines fat tailedness as how much rare events Drive the overall statistics of a distribution and it even talks about mediocris and extremist which is something I talked about in the video to frame the difference between thin-tailed and fat tailed distributions I also like that it mentioned that fat tailedness is not like a binary thing which is something I talked about in the video but rather it lives on a spectrum from not fat tail to very fat tail looking ahead to the next video of the series I'm going to dive more deeply into text embeddings which was an essential part of the rag system so I'll talk in Greater detail about text embeddings and discuss two major use cases namely semantic search and text classification if you enjoyed this video and you want to learn more check out the blog in towards data science and even though this is a member only story you can access it completely for free using the friend Link in the description below and as always thank you so much for your time and thanks for watching
XpoKB3usmKc,2024-02-27T22:52:38.000000,QLoRA—How to Fine-tune an LLM on a Single GPU (w/ Python Code),"fine-tuning is when we take an existing model and tweak it for a particular use case although this is a simple idea applying it to large language models isn't always straightforward the key challenge is that large language models are very computationally expensive which means fine-tuning them in a standard way is not something you can do on a typical computer or laptop in this video I'm going to talk about Cur which is a technique that makes fine-tuning large L language models much more accessible and if you're new here welcome I'm Shaw I make content about data science and Entrepreneurship and if you enjoy this video please consider subscribing that's a great no cost way you can support me in all the content that I make since I talk in depth about fine-tuning in a previous video of this series here I'll just give a highlevel recap of the basic idea so like I said before fine tuning is tweaking an existing model for a particular use case so an analogy for this fine tuning is is like taking a raw diamond and refining it and Distilling it into something more practical and usable like a diamond you might put on a diamond ring in this analogy the raw diamond is your base model so this would be something like gpt3 while the final Diamond You Come Away with is your fine-tuned model which is something like chat GPT and so again the core problem with fine-tuning large language models is that they are computationally expensive to get a sense of this let's say you have a pretty powerful laptop and it comes with a CPU and a GPU where the CPU has 16 GB of RAM and your GPU has 16 GB of RAM let's say we want to finetune a 10 billion parameter model each of these parameters corresponds to a number which we need to represent on our machine standard way of doing this is using the fp16 number format which requires about two bytes of memory per parameter so just doing some simple math here 10 billion parameters time 2 bytes per parameter comes to 20 GB of memory just to store the model parameters so one problem here is that this 20 GB model won't fit on the CPU or GPU but maybe we can get clever in how we distribute the memory so the load of the model is split between the CPU and GPU and that allows us to do things like inference and make predictions with the model however when we talk about fine-tuning we're talking about retraining the model par parameters which is going to require more than just storing the parameters of the model another thing we need are the gradients these are numbers that we use to update the model parameters in the training process we'll have a gradient which is just going to be a number for every parameter in the model so this adds another 20 GB of memory so we've went from 20 to 40 and now even if we get super clever with how we distribute it across our CPU and GPU GPU it's still not going to fit so we'd actually need to add another GPU to even make that work but of course this isn't the whole story you also need room for the optimizer States so if you're using an Optimizer like atom which is very widely used this is going to take the bulk of the memory footprint for model training where this is coming from is an Optimizer like atom is going to store a momentum value and variance value for each parameter in your model so we'll have two numbers per parameter additional these values need to be encoded with higher Precision so instead of the fp16 format these are going to be encoded in the fp32 format and so when it's all said and done there's about a 12x multiplier for the memory footprint of these Optimizer states which means we're going to need a lot more gpus to actually fine-tune this model these calculations are based on reference number two which is a paper about zero which is a method for efficiently fine-tuning these deep neural networks works so we come to a grand total of 160 GB of memory required to train a 10 billion parameter model of course these enormous memory requirements aren't going to fit on your laptop and it's going to require some heavyduty Hardware to run so 160 GB if you get like a 80 gb GPU like the a100 you'll need two of those at least and those are about $20,000 a pop so you're probably talking about like $50,000 just for the hardware to fine-tune a 10 billion parameter model in the standard way this is where Cur comes in so Cura is a technique that makes this whole fine-tuning process much more efficient so much so that you can just run it on your laptop here without the need for all these extra gpus before diving into Cur a key concept that we need to understand is quantization and even though quantization might sound like this scary and sophisticated word it's actually a very simple idea whenever you hear quantization just think splitting a range of numbers into buckets so as an example let's consider any number between 0 and 100 obviously there are infinite numbers that can fit in this range you know there's like 27 55.3 83.7 823 and so on and so forth what quantization consists of is taking this infinite range of numbers and splitting it into discrete bins one way of doing this is quantizing this infinite range using whole numbers so what that would look like for our three numbers here is that 27 would go into this 27th bucket 55.3 would go into this 55 bucket and then 83.78% would go to 20 55 would go to 50 and 83 would go to 80 so that's the basic idea and the reason this is important is that quantization is required whenever you want to represent numbers in a computer and the reason is that if you wanted to encode a single number that lives in an infinite range of possibilities this will require infinite btes of memory it just can't be done at some point when you're talking about a physically constrained system like a computer you have to make some approximations and so if we go from this infinite range to this range quantized by whole numbers this would require about 0.875 bytes per number and then if we go one step further and just split it into these 10 different buckets it would require about half a bite per number one thing to point out here is that there's a natural tradeoff you know we could have a lot of buckets which would give us a lot of precision but it's going to increase the memory footprint of our model however you could have very few buckets for quantization which would minimize the memory footprint but this would be a pretty crude approximation of the model you're working with so balancing this tradeoff is a key contribution of Q Laura there are actually Four ingredients that come together to make up Q Laura the first is 4bit normal float the second is double quantization the third are paged optimizers and then finally is loraa I'm going to talk through each of these ingredients one by one starting with ingredient one one 4bit normal float all this is is a better way to bucket numbers it's a better way to do quantization so let's break it down when we say something is 4 bit what we mean is we're using four binary digits to represent that piece of information and since each digit can be either zero or one this gives us 16 unique combinations which means with a 4bit representation we have 16 buckets at our disposal for quantization compressing a range of numbers into just 16 buckets is great for memory saving you know we only have four bits which translates to half a bite per parameter so if we have 10 billion parameters that's going to translate to 5 GB of memory but of course this brings up the same problem I mentioned earlier which is we have this tradeoff it's like yeah we get huge memory savings but now we have a very crude approximation of the number we're trying to represent the way ingredient one this 4bit normal float works is it buckets the numbers in a particular and clever way suppose we have all the parameters in our model and we plot their distribution when it comes to these deep neural networks it turns out that most of the parameter values are going to be around zero and very few values are going to be much smaller and much larger than zero what that means is we have something that resembles a normal distribution when it comes to our model parameters so if we follow a quantization strategy that I talked about a couple slides ago where we just split the numbers into these equally spaced buckets we're going to get a pretty crude approximation of these model parameters because most of our numbers are just going to be sitting in these two buckets here with very few numbers sitting in these end buckets here an alternative way we can do quantization is instead of using equally spaced buckets we can consider using equally sized buckets so instead of mapping each parameter into these eight buckets we map these parameter values into these eight buckets so now you can see that we have a much more even distribution of model parameters across these buckets and this is exactly the idea that 4-bit normal float uses to balance that tradeoff between low memory and accurately representing model parameters so the next ingredient is double quantization which consists of quantizing the quantization constants I know the word quantize is appearing way more than anyone would ever like on this slide but let's break it down step by step to see what this all means so consider this simple quantization strategy so let's say we have this array of numbers X that's represented using 32 bits and we want to translate it into an 8bit representation on the left hand side here and then we want this 8bit representation to have values in between minus 127 and 127 essentially what we're doing is we're quantizing by whole numbers forcing it to live in this range ofus 127 to 127 so that's what we're trying to do so a simple way of doing that is we rescale all the values in this array by the absolute maximum value in the array and then we'll multiply it by the new maximum value which is 127 in our quantized range and then we'll round it just so that there are no decimal points so this is a very simple way we can quantize this arbitrary array encoded in 32bit into a 8bit integer representation and just to make this more simple we can translate this prefactor here into a constant encoded in 32bit so while this simple quantization strategy isn't how we do it in practice cuz again if we're doing the equally sized buckets it's not just going to be this linear transformation that we're seeing here but this does illustrate the point that anytime you do Quant ization there's going to be some memory overhead involved in that computation so in other words these constants are going to take up precious memory in your system so as an initial strategy you might think well if we have this input tensor or input array and we rescale all the parameters we're only going to have one new constant a 32bit number for all the parameters in our model what's the big deal about that what's another number compared to 10 billion parameters for example so while this does have trivial memory implications it may not be the best way to quantize our model parameters because this is going to be very sensitive to extreme values in our input tensor and the reason is if we're talking about these model parameters where most of them are close to zero but then you have this one parameter way far off in the tails that is your absolute Max it's going to introduce a lot of bias in your quantization process so this standard quantization approach does minimize memory but it comes with maximum potential for bias an alternative strategy could be as follows where we take the input tensor we reshape it to look like this and then we split this tensor into buckets and then within each bucket we do the rescaling process so this significantly reduces the odds of one extreme value skewing all the model parameters in the quantization process this is called blockwise quantization and although it comes with with a greater memory footprint it has a lot less bias so to mitigate the memory cost of this blockwise quantization approach we can employ double quantization which will do this quantization process here but then we'll do the quantization process once again on all these constants that pop up from this blockwise approach so if we just kind of repeat this very simple strategy here now we have an array of constants we have multiple constants popping out they're encoded in 32bit and then we can quantize them into a lower bit format using this simple approach that's double quantization so we are indeed quantizing the quantization constants while it might be an unfortunate name it is a pretty straightforward process so ingredient three is a paged Optimizer all we're doing here is looping in your CPU into the training process so let's say we have a small model like 51 which has 1.3 billion parameters which which based on those same calculations we saw earlier would require about 21 GB of memory for full fine-tuning the dilemma here is that although we have enough memory across the GPU and CPU for all 21 GB needed to fully fine-tune 51 this isn't something that necessarily just works out of the box these are independent modules on your machine and typically the training process will just be restricted to your GPU and so this paged Optimizer what that means is instead of just restricting training to only fitting on your GPU you can move memory as needed from the GPU to the CPU and then bring it back onto the GPU as needed what that might look like is you'll start model training and you'll have one page of memory and a page of memory is like a fundamental unit or block of memory on the GPU or CPU the pages will start accumulating during the training process until your memory gets full and then at which point if you have this paged Optimizer approach you can start moving pages of memory over to the CPU to make room for new memory for training and then if you need a page of memory that was moved to the CPU back onto the GPU you can make room for it there and then you can just move it back over using this paged Optimizer this is the basic idea honestly I don't know exactly how this all works I'm not like a hardware guy I don't know how computer architecture fully works but this is like my highlevel understanding as a data scientist so if you want to learn more check out the cura paper where they talk a little bit more about it and provide some additional references the final ingredient of cura is loraa which stands for low rank adaptation and so I actually talked about Lura in depth in a previous video on fine-tuning so here I'm just going to give a brief highlevel description of how it works if you want more details you can check out that previous video or check out the low R paper Linked In the description below what Laura does is that it fine-tunes a model by adding a small number of trainable parameters so we can see how this works by contrasting it with the standard full fine-tuning approach so let's say this is our model here this is our neural network and we have this input layer we have some hidden layer and then we have the output layer here full fine tuning consists of retraining every single parameter in this model we're just considering one layer at a time we'll have this weight Matrix corresponding to all these lines in this picture here we'll have this Matrix W KN consisting of all the parameters for that particular layer and all of these are trainable while that's probably not going to be a big deal about these six parameters in this shallow Network here if you have a large language model these matrices will get pretty big and you'll have a lot of them because you'll have a lot of layers Lura on the other hand instead of fine-tuning every every single parameter in your model it'll actually freeze every parameter in the model and it works by adding a small set of trainable parameters which you'll then fine-tune the way this works is you'll have your same hidden layer and then you'll add a small set of trainable parameters through this Delta W Matrix so if you're looking at this you might think well how does this help us because Delta W is going to be the same size as W KN so how is this adding a smaller set of trainable parameters and so the trick with Laura is that this Delta W will actually be the product of two smaller matrices b and a which have the appropriate Dimensions to make all the math work out so visually what that looks like is you have your W KN here but then you have BNA a which have far fewer parameters than W KN but when you multiply it together their product it'll have the proper shape to make all the Matrix operations work here so you'll actually freeze W KN so you won't train these parameters and then these parameters housed in BNA will be the trainable ones the result of training the model this way is that you can get 100 to even 1,000x savings and model parameters so instead of having to train 10 billion parameters you're only having to train like 100 million parameters or 50 million parameters so let's bring these four ingredients together let's first look at the standard fine tuning approach as a baseline so here let's say we have our base model represented in fp16 so we'll have this memory footprint from the base model and then we'll have this larger memory footprint from the optimizer States and then we won't have any adapters because adapters only come in when doing lowra or another parameter efficient fine-tuning method and so we'll do like the forward pass on the model it'll go to the optimizer and then the optimizer will do the backward pass and will update the model parameters this is the same standard fine-tuning approach we talked about earlier so a 10 billion parameter model will require about 160 gbes of memory another thing we could do is use lowra so we can get that 100 to 1,000x Savings in the number of trainable parameters we still have our model represented in 16bit but now instead of fine-tuning every single parameter in the model we only have a small number of trainable parameters and then each of those parameters will have an Associated op Optimizer state which significantly reduces the memory footprint so that a 10 billion parameter model would only require about 40 GB of memory while this is a tremendous savings like a 4X Savings in memory 40 GB is still a lot to ask for from consumer Hardware so let's see how Cur helps the situation even further the key thing here is that instead of using the 16bit representation we can use ingredient one and encode the base model as 4bit normal float and then we'll have the same number of trainable parameters from Lura so that'll be exactly the same and then we can use ingredient 3 with the paged optimizers to avoid any out of memory errors that might come up during the training process with that and including the double quantization here we can use Cur to fine-tune a 10 billion parameter model with just about 12 gigabyt of memory which is something that can easily fit in consumer Hardware and can even run using the free resources available on a Google collab so let's see a concrete example of that here we're going to do fine-tuning using mistol 7B instruct to respond to YouTube comments this example is available on the Google collab associated with this video the model and data set are freely available on hugging face and then additionally there is a GitHub repo that has all the resources put together as well as the code to generate the training data set here first thing we need to do is import some libraries everything here is coming from hugging face their Transformers Library their PFT Library which is parameter efficient fine-tuning this is what's going to allow us to do Q lur and then we're using hugging fa's data sets library because I uploaded the training data set onto hugging faes Hub and then finally we just import the Transformers Library these are kind of like sub dependencies to ensure some of these modules work I think it's mainly this one prepare mod for kbit training you don't need to import these but you need to make sure they're installed in your environment and this was actually a paino because bits and bytes only works on Linux and windows and on Nvidia hardware and then gptq this format for encoding models it doesn't run on Mac so as a Mac User this was kind of frustrating lots of trial and error to try to get it to work on my machine locally but I wasn't able to get it to work so if anyone was able to get it to run on a M1 or a M2 or or even M3 send me your example code or send me any resources you found helpful I would love to get a version working on my personal machine but since collab they have a Linux environment using Nvidia Hardware the code here works fine next we can load the quantized model and so here we're going to grab a version of mistol 7B instruct from the bloke and so if you're not familiar with the bloke he's actually quantized and shared thousands of these large language models completely for free on the hugging face Hub and then we can just import this model using this from pre-trained method so we just need to specify the model name on The Hub device map set to Auto just has the Transformers Library kind of figure out the optimal way to spread the load between the GPU and CPU to load in the model trust remote code basically it's not going to allow like a custom model file to run on your local machine so this is just a way to protect your machine when downloading code from The Hub and then revision main is just saying we want the main version of the model available at this repo here then again gptq which is the format used here does not run on Mac there are some other options with Mac but I wasn't able to get it working on my machine once we have the quantized model loaded we can load the tokenizer so we can do this actually pretty easily using this from pre-train method so we just specify the model name and then specify this use fast argument as true with just those two simple blocks of code we can use the base model one thing we do here is we put the model into evaluation mode which apparently deactivates the Dropout modules next we can craft our prompt so let's say we have a command from YouTube that says great content thank you and then we put it into the proper prompt format so mistal 7B instruct is an instruction tuned model so it's actually Expecting The Prompt in a very particular format and namely it's just expecting this instruction start and instruction end special tokens in the prompt so we set that up very easily what this is doing is it's just going to dynamically take this comment variable and stick it into this prompt here and then once we have that we can pass the prompt to the tokenizer so basically we're taking this prompt and We're translating it from a string into an array of numbers and then we can take that array of numbers and we can pass it into our model to generate more text once we do that we can get the outputs and then pass them back into our tokenizer and have the tokenizer decode the vector back into English the output of this great content thank you comment is I'm glad you found the content helpful if you have any specific questions or topics you'd like me to cover in the future feel free to ask I'm here to help in the meantime I'd be happy to answer any questions you might have about the content I've already provided just just let me know which article or blog post you're referring to and I'll do my best to provide you with accurate and up-to-date information thanks for reading and I look forward to helping you with any questions you may have so while this is a fine response there are a few issues with it one it's very long I would never respond to a YouTube comment like this second is it kind of just like repeats itself it's like glad you found it helpful feel free to ask and then it says happy to answer questions that you have happy to provide you with acurate update information and like look forward to helping you with questions so saying the same thing in different words like a few different times and then finally it says thanks for reading and if this is for YouTube comments people aren't reading this stuff they're watching videos so one thing we can do to improve model performance is by doing so-called prompt engineering I actually have a in-depth guide on prompt engineering where I talk about seven tricks to kind of improve your prompts in a previous video of the series so feel free to check that out if you're interested The Prompt that I ended up using here is something that I generated through trial and error and the way I did that is using a website called together. which I can link in the description below essentially together. a they have a chat interface kind of like chat GPT but for a lot of open- source models including mistl 7B instruct version 0.2 so I was able to test a lot of prompt ideas and get feedback and just kind of eyeball which gave the best performance and I ended up using that one so I have this set of instructions here Shaw gbt functioning as a virtual data science consultant on YouTube communicates in clear accessible language escalating to technical depth upon request it reacts to feedback aply and ends responses with its signatur sha GPT sha gbt will tailor the length of its responses to match the viewers comment providing concise acknowledgements to brief expressions of gratitude or feedback thus keeping the interaction natural and engaging then I have this instruction please respond to the following comment and then I have this Lambda function where given a comment I'll piece together this instruction string and comment together within the instruction special tokens that the model is expecting with that I can just pass the comment into the prompt template and generate a new prompt what that looks like is this so you see we have the instruction special tokens you see it's well formatted this is the instructions please respond to the following comment and says great comment thank you Now using this new prompt instead of just passing the comment directly to the model we have this set of instructions with the comment this is the response thank you for your kind words I'm glad you found the content helpful sha GPT so this is really good this is actually already pretty close to how I typically respond to YouTube comments and a lot of them tend to be something like this and it appropriately signed off as Sha GPT so people know that it came from an AI and not from me personally well maybe we could just call it here it's like okay this is good enough let's just start using this as the comment responder let's see how we can use Q Laura to improve this model even further using fine tuning so the way to do that is we need to prepare the model for training so we'll put it from eval mode into training mode we're going to enable gradient checkpointing which isn't something I talked about and it's not necessarily part of the qora technique because it's actually pretty standard it's just a memory saving technique that clears specific activations and then recomputes them during the backward path of the model and then we need to enable quantized training the base model is going to be in 4 bit and we're going to freeze them but we still want to do training in higher Precision with Lowa we need to make sure we enable this quantize training option next we want to set up lowra so we can use that using this low ra config file I talk more about low RA in the fine-tuning video so just briefly going through this we're going to set the rank as 8 set the alpha s32 we're going to Target the query modules in the model we're going to set drop out to 0.5 we're not going to have any bias values and then we're going to set the task as causal language modeling with the config file we can pass the model and the config into this method get PFT model so this will just create a lowr trainable version of the model and then we can print the number of trainable parameters so doing that we see that we actually have a significant saving so less than 1% of the original number of trainable parameters just one point of confusion for me personally is it's showing that mistol 7B instruct has 264 million parameters here based on the quick research I did seemed like when you do quantization there could be some terms that you can drop but honestly I don't fully understand why we went from 7 billion parameters to just 264 million parameters so if anyone knows that please drop it in the comments I'm very curious but the main point here is that we're only using 0.8% of the original number of train parameters so huge memory savings using lowon next we're going to load the data set which is freely available on the hugging face Hub it's called shot GPT YouTube comments also the code to generate this data set is available at the GitHub repo if you're curious on how to do the formatting and stuff and then here's an example from this data set you'll see we have the special token the start string and the end string we have the start instruction and end instruction and then we have the same set of instructions as before and then we have the comment here which is a real comment from the YouTube channel then after the instruction string we have the actual response I left to this comment and then I just appended this Shaw GPT sign off so the model learns the appropriate format in style that it should respond to we've got a data set of 59 of these examples so not a huge data set at all and then next we need to pre-process the text so this is very similar to how I did it in the previous find tuning video basically we Define this tokenized function which if the example is too long so if it's longer than 52 tokens it's going to truncate it so it's not more than this max length and then we'll return it as numpy values and then we can apply this tokenized function to every single example in the data set using this method here the map method where we have our data set and then we just pass in the tokenized function and set batched equal to true so it doesn't batches I guess instead of doing it one by one the other thing we need to do is this handles if the examples are too long but when you're training the model each example in a batch they actually need to be the same size so you can actually do matrix multiplication so for that we can create a data cator what that does is if you have multiple examples of different lengths so let's say you have like four examples in a batch and they're all of different lengths the data cator will dynamically pad each example example so they have the same length for that we need to define a pad token which I set as the end of string token and then I create the data collator using this method here and then I think this is masked language modeling set equal to false and that's because we're doing so-called causal language modeling not masked language modeling now we're ready to start setting up the training process so here we're setting hyperparameters we have the learning rate batch size number of epoch we're setting the output directory of the model the learning rate the batch size goes here number epochs goes here weight Decay we set it as 0.01 for logging evaluation and save strategy we set it to every Epoch that means every Epoch will print the training loss we'll evaluate at every Epoch we'll also print the validation data set loss and then save strategy so we'll save the model every Epoch in case something goes wrong we're going to load the best model at the end because maybe the best model was actually at the eighth Epoch and it got worse on the ninth Epoch or something like that gradient accumulation is equal to four warm-up steps equal to two so I actually talk a lot about gradient accumulation and weight decay in the previous video on training a large language model from scratch so if you're curious about what's going on there you can check out those videos next we'll set fp16 equal to true so here we're going to use 16bit values for training and then we'll enable the paged Optimizer by setting this optim equal to paged atom W 8bit so this is ingredient three from before lots of hyper parameters and of course you can spend your whole life tuning and tweaking this but once we have that we can run the training job so we initialize our trainer we give it the model give it our training data set our validation data set training arguments we defined on the previous slide and then the data collator we're going to silence warnings this is what I saw on an example from hugging face when they were introducing bits and bites so I just did it again here and then we can run the training process this took about 10 minutes to run on Google collab so it's actually pretty quick and this is this is what will get printed the training loss and validation loss so we can see a smooth monotonic decrease of both implying stable training which is good and then once it's all said and done we have our model and we can use it so if we pass in that same test comment great content thank you we get the response glad you enjoyed it shot GPT and then it even adds this disclaimer that note I am an AI language model I don't have the ability to feel emotions or watch videos I'm here to answer questions and provide explanations so this is good I feel like this is exactly how I would respond to this comment if I wanted to remove the disclaimer I could easily do that with some like string manipulation just keeping all the text before the sign off or something like that but the point is that the fine-tuning process at least from this one example seemed to work pretty nicely let's try a different comment something more technical like what is fat tailedness the response of the model is actually similar to what we saw in the previous video when we fine-tuned open AI model and then we asked it the same question where where it gives a good concise explanation of fat tailedness the only issue is it doesn't explain fat tailedness the same way that I explained it in my video series on the topic so this brings up one of the limitations of fine-tuning which is that it's great for capturing style but it's not always an optimal way to incorporate specialized knowledge into model responses which brings us to what's next instead of trying to give the model even more examples trying to include this specialized knowledge a simpler approach is that we can improve the model's responses to these types of technical questions by providing it specialized domain knowledge the way we can do that is using a so-called rag system which stands for retrieval augmented generation right now we just get the comment and we pass it into the model with the appropriate prompt and it spits out a response the difference with a rag system is that we take the comment we use the comment to extract ra relevant information from a knowledge base and then we incorporate that into the prompt that we pass into the model so that it can generate a response so that's going to be the focus of the next video in this series we're going to see how we can improve shot GPT using specialized knowledge coming from my medium blog articles and speaking of medium blog articles if you enjoy this video but you want to learn more check out the article published in towards data science on Cur there I cover details that I might have missed in this video here and even though this is a member only story you can access it completely for free using the friend Link in the description below other things I'll point out is that the code example is available for free on collab there's more code available on the GitHub and then again the model and the data set are available on hugging face and as always thank you so much for your time and thanks for watching"
qPrVqTIkobg,2024-02-27T00:59:53.000000,Difference Between #AI Chatbots and Assistants,here's the difference between an AI chatbot and an AI assistant although these terms are often used interchangeably they can mean slightly different things put simply a chatbot is an AI you can have a conversation with while an AI assistant is a chatbot that can use tools a tool can be things like web browsing a calculator or anything else that expands the capabilities of a chatbot for example if you use the free version of chat GPT that's a chatbot because it comes with a basic chat functionality however if you have the premium version of chat GPT that's an assistant because it comes built in with functionalities like web browsing knowledge retrieval and image generation assistants transform AIS from chat buddies to agents that can solve and automate complex problems
LqOJCPonUQU,2024-02-19T14:51:09.000000,The Best Way to Think About Goals #goalsetting,goals are essentially an excuse for us to grow this is actually something that you know I first heard from Ray Doo who has a book called like principles I think that's what it's called oh he said that I don't think it's a new idea I think people have known for a while that the point of a goal isn't necessarily the goal if you've watched the David Beckham documentary and there was something that he said in there I don't know what episode but he was like the game of football is just an excuse for them to keep playing yeah and I was like whoa okay something clicked it has nothing to do with the outcome it has nothing to do with the output it's about the process right levels I think should be more as like a bonus than the the focus and that's so different than how I feel like how we learned about goals yeah you need to get to the goal and that's the point and the person that you have to become yeah in the process that's the real reward take the focus off the goal put it on the person that you want to be the dream life that you want to have
r5qk3uIdkks,2024-02-05T14:59:54.000000,What is #ai? — Simply Explained,what is AI exactly I work in the industry and I can still find myself scratching my head about it but a simple way to think about it is as follows AI stands for artificial intelligence the word that's a little tricky here is intelligence what does that mean exactly the way I like to think about it is that intelligence is the ability to solve problems and make decisions so if we take that definition for intelligence it then follows that artificial intelligence is simply a computer's ability to solve problems and make decisions
4RAvJt3fWoI,2024-02-05T00:44:32.000000,"3 Ways to Make a Custom AI Assistant | RAG, Tools, & Fine-tuning","one of the most common asks I get from clients is how do I build a custom AI chatbot well a few months ago this was something you needed to hire a consultant for today that's not necessarily the case in this video I'll walk through three ways to make a custom AI assistant using a few new releases from open AI I'll review a no code solution via their new gpts feature a python solution via the new assistance API and then finally talk about how to fine-tune an assistant with your data and if you're new here welcome I'm Shaw I make videos about data science and Entrepreneurship and if you enjoy this content please consider subscribing that's a great no cost way you can support me in all the videos that I make before jumping into the solutions I wanted to quickly differentiate a AI chatbot from a AI assistant while these terms might be used interchangeably the way I like to think about it is that a chatbot is an AI you can have a conversation with while an AI assistant is essentially a chatbot that can use tools a tool could be web browsing it could be a calculator it could be a python interpreter or anything else that helps augment the abilities of a chatbot for example if you have the free version of chat GPT that's a chatbot because it comes with a basic chat functionality without any additional tools however if you have the premium version of chat GPT that's an assistant because it has built-in tools like web browsing and document retrieval turning chatbots into assistance via tools is a powerful way to to expand its capabilities and Effectiveness each of the approaches I talk about in this video can be used to develop an AI assistant and I'll use each of these approaches to build an AI that can respond to my YouTube comments in my likeness which I'll call Shaw GPT the first way I'm going to talk about is the easiest and fastest of the three and it consists of using open ai's new gpts feature so the way to get to that is we go to chat GPT and if you have the premium version version you can go to this Explorer GPT thing here and then you can go look at my gpts and so you can make these custom gpts and they'll appear here so I've already made a version of this Shaw GPT but we'll go ahead and create a new GPT so we can see what this looks like from scratch the thing about this gpts feature is it actually has a chat interface to help you build the AI assistance so not only is this a no code solution it has a step bystep chatbot that will help you build this Ai and so here the GPT Builder says hi I'll help you build a new GPT you can say something like make a creative who helps generate visuals for my new product or make software engineer who helps format my code what would you like to make make a YouTube comment responder let's see what that spits out so you can see that GPT Builder is thinking what's happening is you say this to GPT Builder and then behind the scenes it's going to go ahead and update the instructions for the AI assistant so if you go to the configure tab it'll start filling in these things it'll film the description for the assistant and it'll give it its instructions but let's go back how about the name shot GPT for this GPT does that work that's perfect okay so now it's going to generate a profile picture so it'll use Dolly 2 which is if you're not familiar that's a different AI that instead of generating text like chat GPT it generates images like this one here so that's a cool thing about this gbt Builder it'll generate a profile picture for you you can also just upload your own so I've got a folder here called Shaw GPT and I've got some different images that we can use so I'll just upload this one and then I'll tell GPT Builder to use this image instead so it's going to use the picture that I uploaded okay so now it's asking for a bit more guidance on how to answer questions outside the expertise in data science and Entrepreneurship so a lot of questions that I get on YouTube aren't actually about data science or entrepreneur preneurship it's usually people just saying great content or something like that just sharing feedback about the video so I'll provide those instructions here okay so here I wrote out instructions for both positive and negative feedback so for positive feedback unrelated to data science Express gratitude for their viewership suggestions or positive feedback given negative feedback thank the viewer rephrase their criticism to convey not convert convey understanding and offer an idea for improvement so these are just some general rules of thumb of how to respond to comments how do you envision sha GPT addressing comments that ask for personal advice on Career changes to Day signs or starting a new business okay so that's a good question I didn't see this last time I made sha GPT through this interface basically when people ask me these questions I tell them that there's no way to just give general advice to anyone without knowing more details and ultimately I only know what's worked for me and I don't necessarily know what's the best choice for other people and that's ultimately up to them but I'm always happy to kind of share my personal experience and what has worked for me so basically going to try to put that into words so that GPT Builder can instruct the model accordingly so the way I tried to convey that is acknowledge that advice for their unique situation is hard to give without a full understanding of their situation however sha GPT can share Shaw's personal experience in case it is helpful for the viewer okay here we go sha GPT will now take the approach that when addressing personal advice inquiries okay so for the Final Touch what tone of L language do you prefer shbg use more formal and professional or casual and friendly casual and friendly and concise I don't like reading a lot of text and so I try not to put that burden on other people so I like to keep text very concise okay sweet feel free to try out shot yout in the preview and see how it responds if there's anything else you'd like to address what would it be okay so we can now give it a whirl how do I start in data science so it gives a very long response doesn't feel very concise starting data science is exciting here's a quick guide to getting you going learn the basics explore online courses Hands-On practice understanding data manipulation diving into machine learning build a portfolio Network and learn continuously so it's casual it's friendly but it's not very concise and doesn't really sound like me the response sounds a lot like what chat GPT would say so if someone reads this comment or response from me they'll be like did sha just copy paste my comment into chat GPT and paste the response so one thing we can do to remove that ambiguity is we can have sh GPT sign off with its name just so people know that it was generated by an AI assistant and not by me personally we can add this to the GPT Builder Okay so it updated the instructions so now it should sign up as Shaw GPT so here's another one can you explain machine learning in simple terms so it gave a pretty good response I like this example just learning by example and experiences it signed off with sha GPT which is good again however the response is way too long I would never respond like this but it's good that it identifies itself I want to move over to this configure tab so I guess some weird things even though it said it was going to use the name sha GPT that never made it over it's still I guess in beta they're still working out some Kinks if anything isn't what you want from this chat interface you can always move over to the configure Tab and just set these things manually so you can set the name of the assistant you can set the description this was autogenerated friendly and informative responder for a data science and Entrepreneurship YouTube channel and then these are the instructions that it generated through the conversation we were having sha GPT is the Casual friendly and concise voice behind a YouTube channel on data science and Entrepreneurship it acknowledges the complexity of giving personalized advice offering insights from Shaw's experiences instead positive feedback is met with gratitude and criticism is handled constructively sha gbt always signs off its messages with its name sha gbt so we can see like a lot of the aspects of the conversation were incorporated into to the instructions another cool thing here is you can set these conversation starters which is nice for the UI you can just put like frequently asked questions for your AI assistant here another really cool thing about gpts is that it has builtin retrieval also called retrieval augmented generation so basically what this means is you can equip your assistant with specialized knowledge that lives in like a PDF or like a Word document or whatever so you can upload those files and the assistant will be able to read those files and incorporate the appropriate context as needed so for example I have a bunch of my articles here so I could upload four ways to quantify fat taals with python I can upload that PDF and then I can ask it a question what is a fat tail it didn't use the PDF in this case it just gave a standard definition of fat Tails as something more extreme than a normal distribution which is a common way of defining bat Tales however it's not the definition of fat tails that I used in that article so another thing we could try is like how can we quantify fat tails so even though it didn't use the definition of fat tailedness from that article it did seamlessly grab the four ways to quantify fat taals from the article the way this is working is that GPT 4 or whatever GPT is underlying these custom gpts here it has been trained to know when to seek outside knowledge if it doesn't know something and so so here it does it pretty seamlessly it feels like it read the article and is like rephrasing it in its own words which is pretty interesting okay so that's knowledge you can upload I think up to like 10 files or something actually probably more the next thing I want to point out are these capabilities so this is what makes these gpts assistant and not just chat Bots it's because it has these extended capabilities such as web browsing image generation and code interpreter so web browsing is exactly what you expect if the user says hey look up something or search the web for some resources or whatever the assistant knows when to call upon this web browsing capability to do that if the user asks the assistant to generate an image it knows to use Dolly and the code interpreter allows the assistant to not only write python code because that's what a chatbot can do but actually run that python code and then return the values to the user or use the values from that computation to inform the response to the user so since for this YouTube responder use case we don't need image generation because you can't have images on YouTube probably don't need a code interpreter either web browsing could be cool to provide additional resources and of course I could upload all these other articles as well so we can just do that to give it more knowledge so we can't bulk upload PDFs at this point but I can go through one by one and upload all of these articles to have it have a bit more specialized knowledge about the actual content that is on the channel the last thing are these actions which are pretty interesting so essentially this allows gpts to make API calls so we can learn more here how to create a GPT okay so this answers one question you can have 20 files uploaded that answers that okay here custom actions you can make third party API calls available to your GPT so that's pretty cool like the canonical case here is like if you want to grab weather data so if you want to get weather from a particular City you can endow the assistant with that capability to look up the weather in a particular City not necessarily for this YouTube comment responder I think here just knowledge and the web browsing is sufficient and so what's cool about these is you can save these gpts and make it available to everyone so you don't have to worry about like deploying your assistant into production because they have this built-in way to do it so you can make it published only to yourself anyone with the link or everyone in the world and then you can select a category so that's pretty cool it got the category right we can uh confirm but I won't because I've already published a version of sha GPT and I spent a bit more time making that one so I don't want to put this example version out there but if you want to interact with sha GPT I'll put a link in the description and you can kind of see for yourself the performance I guess another thing is people say like you can become a millionaire making a GPT or something and I don't know maybe based on my interaction with this interface I don't know how that would work however looking at the other gpts this doesn't feel so much as like a direct way to generate Revenue however it feels like a very big opportunity for these companies or really any company to do promotion and lead generation for their business so like canva being number one on here it's like that's amazing advertising cuz whatever it is they get 100 million daily users of chat GPT that's a lot of eyeballs seeing canva basically for free feels more like an opportunity for these companies to promote their business so like Ali Academy I don't know who that is but I can just like go to her website look at that geni Innovation for Creative so look at that of course this might be a way to generate revenue or whatever but to me it feels more an opportunity for advertising a business or something like that as opposed to direct monetization so just my thoughts on becoming a millionaire with custom gpts okay so when it's all done and published so this is publicly available data scientist GPT for YouTube comments and so we can have these things ready to go so people can ask their data science questions even though it has the Articles available to it that I wrote it doesn't explain it in the way that I explained it in the Articles it kind of gives a more traditional explanation of these things so that's kind of one downside for these custom gpts it still feels a lot like chat GPT but just with slightly different wrapping paper and of course if I spent a lot more time with it I could build something that doesn't sound so much like Chad GPT but the second obvious downside of this is it requires a pre premium version of chat GPT and some people just like a hobbyist or something or you're like a student it may not make sense to pay $2 a month for this and of course this isn't something you can easily integrate into like an app that you make or a website for people to use this they have to come to the chat GPT website so if you don't have premium or you want to incorporate this assistant into some application or some website we can turn to the assistance API which is way number two I'm going to talk about so if you don't have a premium account there's still a no code solution for building a AI assistant and that's through the assistance playground this is a relatively new feature so the playground's been around for a while this is the original version the complete playgrounds you can pick whatever model you like here and it'll just do the auto regressive text generation it'll just keep predicting the next word recursively chat is similar where you can set the model and whatever parameters you like and set the system message and just do like a chat however assistance takes things to the next level where it's a lot like the gpts configure tab that we saw earlier where you can have a name instructions models and then you can add tools to the chat bot so we can really just copy paste all this over so we have sha GPT here are your instructions we can select the model so we can have GPT for Turbo we can add functions so we can write custom functions here it has to be in a particular format shown here so you need to give it a name for the function you need to describe it in words and then you need to provide the inputs for the function so what information does the assistant need to pass to the function in order to generate an output also has the code interpreter so it can not only write python code it can run the python code receive a response and then it also has retrieval like we saw with the custom GPT the one thing missing here is web browsing natively I think that's something that they're going to add later but as of right now it's January 30th 2024 web browsing is not integrated into the assistance playground or assistance API but we do have functions code interpreter retrieval and we can also add files like we did before let's see if we can do bulk file upload here aha we can there we go bulk file upload available in the playground okay so we can just see how this works we can do like a side by-side comparison so we'll ask sha GPT what is fat tailedness and we'll ask sha GPT playground what is fat tailedness okay so again it's kind of giving the more traditional definition of something fat tailed has heavier Tails compared to a normal distribution so the response is very similar to what we see over here in the gpt's case so while using the playground to make an assistant in this way seems like a free hack a free version of the custom gpts it has one serious limitation which is that if you only use the playground your assistant is trapped in the playground it's not like the GPT T's interface we saw earlier where you can with one click deploy the assistant onto the chat GPT website so if you want to release the assistant from this playground you want to put it into a website an app or whatever you're going to need to do some coding so let's do that okay so here we got a jupit notebook this is available in the GitHub repository Linked In the description below so if you want this code feel free to grab it there kind of walking through this just importing some modules of course we're going to use the open AI assistance API which is still in b so that's why I put that there you'll need to import your secret key and if you don't have a secret key or don't know how to get your secret key for the open AI API I have a video all about that I'll link it in the description below maybe pop it up on the screen here so you can check that out but you can get a key in a very straightforward way you don't need to be premium user if you just have an open AI account you can get an API key and if you're new you get some free credits to start what I have is a text file called sk. and I'm in importing the secret key from that text file that's the way I like to do it and also importing time for a helper function we'll see in a second here the first thing we need to do is to set up the client so set up communication with the opening eye API so you just do that oneline in code I make a quick helper function because when we're using these assistants they have to think so it takes time for us to get a response from the API so what this function does is weights and periodically grabs the status of the API call and then when it's done it'll move on to the next step of the code and it'll tell us how long it took to do that so that's just like a helper function it's not super important this is kind of what we were doing before in the playground and in the gpt's chat interface but now we're going to do it in Python the first step is defining our instructions so this is essentially the system message so I'm using the same thing we had in the playground here sha GPT functioning as a virtual data science consultant on YouTube blah blah blah blah blah just copy paste that here and then we can create the assistant using this chunk of code here the way this works is we created this client object earlier and then we're going to be accessing the assistance API and we're just going to use this create method what that allows us to do is create an assistant set its name its description its instructions and the model that we want to use notice that this is everything we said here the name description isn't here but you can set the description via the assistance API the instructions and the model so it's all the same and then I just print the assistant not that it gives us a whole lot of useful information but it's just this object that looks like this okay in order to talk to the assistant you can set up this thread object which is new in the assistance API compared to the chat completion API which is what I talked about in a previous video we can create this thread which is just like a back and forth conversation it helps avoid doing this boiler plate code of keeping track of what the system message is and what the user says and what the assistant says and what the user says and assistant says so on and so forth It's all handled by the thread so that's super convenient you can generate the user message so here I just put something like great content thank you so that's a comment and then we can add this comment to the thread using this so we have to specify which thread we're going to add the message to which role is saying the message and then the content of the message we get the thread idea from here role is user and then content is here then finally we can send the message to the assistant to generate a response the way that works is you use this method here so we're in the threads module the runs subm module and we're going to use this create method to create a run which is just an API call essentially so we'll need to provide the thread ID which we just added this user message to and we need to provide the assistant ID and so where this comes from is the assistant object we made ear earlier if we do that we can run that and then it'll take a few seconds to run so that's why I run this helper function I made and then once the run is complete we can view that returned object so it took about 5 Seconds to do that first one so actually we can just like run this whole thing I think that's fine yeah we'll just run this whole thing all right so it took less time this time so it took about 3 seconds to run so automatically when we do this run when the assistant generates the response it gets automatically added to the thread so we don't have to do anything extra so all we can do is grab the messages from this thread that was just updated by the assistant and we can print the most recent message in the thread so the response from the assistant is you're welcome I'm glad you found it helpful if you have any more questions or topics you're curious about feel free to ask shot GPZ it signed off correctly which is nice and it is a pretty positive response it captures the sentiment that I convey it's just super long I never talk like this again I don't like reading and I don't want to burden people people with just putting way too much text for them to read I try to keep my responses pretty concise then here I just delete the assistant because what happens is every time you make an assistant it'll pop up here and if you're running this notebook as just like a tutorial or you're running it multiple times debugging or something you're going to have a very long list of assistants here if you don't delete them so that's what this line of code's doing okay so we have this problem that it's giving nice responses but these are just way too long they don't really sound like me one thing we can do to get sha GPT to sound more like me is to give it some examples to learn from so that's what I do here through so-called fuse shot prompting it's essentially where you put a set of examples in the instructions for the assistant so these are real past comments and my responses to these comments and all I did was append this sha GPT to them so it has that format we're looking for and then we just do the same thing we create another system in the same way we name it shot GPT we give it a description we give it the new set of instructions which is is what we defined here and then we Define the model then we can create a new thread so we can talk to the assistant we'll use the same user message great content thank you we'll add the message to the thread so again specifying the thread ID the role of who's saying it and the message then we'll run the thread so we'll send it to the assistant to generate a response by passing in the thread ID and assistant ID and then we'll wait a few seconds for the assistant to generate a response so it took about 3 seconds and then here we're just printing the assistance response so this is what sha GPT says now with the updated instructions to the same exact comment which is you're welcome happy to hear you found it useful shot GPT so that's a lot better I would probably have said just something like glad it was helpful happy to hear you found it useful maybe just one of these to make it a bit more concise cuz you're just saying the same thing in more words but this is a lot better than the original and a lot better than what we were seeing in the playground with the no code Solutions but of course we could have have added the few shot examples in the no code solution and we would have gotten the same results so another thing is technical questions which Chad GPT might respond in a certain way but it may not be the same way that I would respond to that technical question so before we tried adding PDF versions of my article so we could get the context of how I talk about fat tailedness for example but it for whatever reason it wasn't capturing that but now we've done this few shot prompting so let's see how its response changes we're just doing the same exact thing as before we're creating a new thread we're generating a user message so a new thread is as if we opened a new chat GPT chat so it's not going to remember this previous message that's why we're making the new thread add user message to thread so exactly what we did before but now the user message is what is fat tailedness and then we're going to send the message to the assistant we actually don't need this here I haven't gone through and cleaned up this code okay and then we wait for the assistant to process the prompt it takes a bit longer now presumably because it's a longer response it's not just like glad you liked it it's explaining what fat tailedness is and in fact when we print the results this is what we get fat tailedness refers to property probability distribution which tails are fatter than those of a normal distribution okay so it's just giving the same thing as we saw before but notice that we haven't added any documents like we did in the no code Solutions so let's do that I quickly deleted the assistant so we don't have them piling up here's how we can add documents to the assistant so the way that works is like this so we go to the files module and just do this create method so we just use this syntax open which is just opening a file so it's creating a python readable version of the file so we're going to open the file we specify the path which is in this articles folder this is just setting that we'll be opening this file for reading purposes as opposed to writing to it we also need to specify the purpose of this file so we'll say it's for assistant the other option is for fine-tuning which we'll see here in a little bit once we create this file it'll actually populate here but I think it actually got deleted at the end of this notebook so if you were to just run this chunk of Code by itself it's going to create a file in your files tab on your open AI account and actually any assistant that you make can use these files for retrieval all you have to do is provide the file ID as we'll see in a second here but notice where all these came from is when we were in the playground and we uploaded all 10 of these articles with the file uploaded we can create a new assistant that can access that particular file so the way that works is we created this file object and now we can just pass it to the assistant so this is the same exact method we were using before to generate the assistant we said the name the description the instructions but now we're going to define the tools so here we're going to use retrieval as a tool and when we do that we also need to specify the file IDs of the documents that the assistant can use so in this case we're just going to use this one that we created up here but of course if you have a ton of files uploaded here you can use all of them and you just need to specify all of the file IDs in this list here and then finally you specify the model so now that we added retrieval let's try the technical question once again so this one took a lot longer to run took about 45 seconds but we're running it in the same way this is interesting it generated kind of like a mixed response something close to what we were seeing before where it's kind of giving a traditional definition of fat tailedness it's just more fat tailed than a normal distribution but then it's bringing in Concepts from the articles that I provided which is you know fat tailedness ranges from thin tailed to very fat tailed which is almost verbatim from the article and then it adds these four different ways to quantify F tailedness and then it kind of gives some context these methods offer different insights into distributions shape allowing for more detailed understanding of the data's Behavior especially regarding the occurrences and impact of rare and extreme events sha GPT yeah that's pretty interesting that it added in those four heris and gave this dichotomy of thin tailed versus fat tailedness which is something I talk about in the article and then finally I delete the assistant and delete the file if you're running this multiple times you don't accumulate a bunch of files and assistants on your account and then some more resources I'll put all these in the description you can read more about the assistance AP API just from like a high level the documentation which is the API reference breaks down the python side of things and then finally more On Tools so here we just used retrieval but there are two other tools that you can use which are the code interpreter which we saw in both the playground and the gpts and the functions tool which is something we saw in the assistants playground so just going back to that we only used retrieval here but we could have also used the code interpreter and functions so if you want to read more about that I'll provide this link in the description kind of talks about the tools talks about the code interpreter which as it says here allows assistance API to write and run python code so that's on the code interpreter reading images and files okay that's cool and also had the input output logs that's cool knowledge retrieval which is what we did in all the different examples I talked about here and then function calling so this is the cool thing and you can almost have any function with this capability so the way this works is you need to give the name of the function along with a natural language description and then you need to also provide a so-called Json schema which essentially outlines what the inputs of the function are so the assistant knows what information to give to this function in order to get a response at this point we've seen both no code and python ways of generating an assistant and we saw that we can get pretty far by using things like retrieval and fuse shot prompting to improve the assistant performance however there's still still something missing from the assistant responses it just doesn't really feel like something I would say it's very verbose and doesn't quite explain things how I would explain it so to kind of better capture this feel aspect of the assistant responses let's turn to fine-tuning the model unlike the assistance API there's no no code solution for the fine-tuning bit you have to write some python code to fine-tune the model and it requires a bit more work because in order to do fine tuning you need to curate a training data set so here I'm going to kind of walk through the process of how I did that for this YouTube comment responder use case here's another Jupiter notebook example code is on the GitHub repository so check it out if you like here we're importing some modules again importing the open AI python Library importing the secret key in the same way that we did in the previous example and then here importing a few other libraries that we're going to use for the data preparation so again we're going to set up our client and then we're going to need to prepare the training data so this is what makes fine tuning a lot of work it's just acquiring the right data and a important rule in machine learning if there are any rules in machine learning is that data quality is the most important thing not how fancy your model is not how efficient your code is but how good your data is at capturing the thing that you're trying to model and so in this case what I did was I went to my YouTube channel and I took real comments that people left and then real comments that I responded with to create this input output pair so what that looks like is this so I got about 60 comments and responses just copy pasted into this numbers sheet I use numbers because I'm on Mac but you could use Excel to just copy paste it in there and then I exported it as a CSV file just to see some examples of this comment was this was a very thorough introduction to llms and answer answered many questions I had thank you to which I said great to hear glad it was helpful smiley face and of course we can have emojis in here this one says Mr Moneybags over here which was funny so these are just real world comments which we incorporate into F shop prompting but you just can't fit 60 comments and responses into your assistant instructions and if you do it's just going to create this like bulky overhead because the instructions are going to be passed to the assistant every time a thread is initiated so you can imagine that the API calls are just going to become more costly over the long run if you just have a really large instruction set and so that's where fine tuning is helpful in fact open AI has a nice guide on fine tuning here we go common use cases so I'll also put this in the description some common use cases when fine-tuning can improve results setting the style tone format and other qualitative aspects so that's the main motivation for using it in this use case another one's improving reliability at producing the desired output next is correcting failures to follow complex prompts handling many edge cases in specific ways so I guess that's good if there's like a specific prompts or specific situations where the assistant is failing you can just include those in your training data set and it'll learn how to not do that and then performing a new skill or task that's hard to articulate in a prompt and so I like how they put it here the high Lev way to think about it is you want to find tune when it's easier to show not tell kind of like how I was experiencing in the initial no code GPT solution it was asking me these questions and I wasn't necessarily sure how to give the best instructions that's another downside of these no code Solutions is that even though it doesn't require us to write python code it may not be obvious how to give good instructions using natural language showing not telling is just a better way to convey the desired behavior of the assistant going back to the training data prep we have all these comments and responses in the CSV file how however the fine-tuning API doesn't take CSV files as input the data need to be in a very specific format which they talk about here data preparation and Analysis for chat model fine tuning and basically it wants the examples the inputs and outputs in a Json L format so basically it wants it in a text file here's an example it wants it in this format where each row is an example and it consists of a system message a user message and then an assistant message and each line is in the Json format which you can essentially think of as a python dictionary what we need to do is take the CSV file from here and translate it into the proper format in order to use it for fine tuning the way that works here is I initialize these lists to hold all the comments and all the responses then I open the CSV file this is a CSV file YouTube comments. CSV in read mode this like a common syntax tax for reading text files in Python not entirely sure how it actually works but it's something I've used a lot so maybe I should figure out how it works but anyway my understanding is that it handles the opening and closing of the text file but if someone knows better than me drop it in the comments below I'm curious so the first thing we do is use this CSV do reader method to read the file and then what that allows us to do is go through the CSV file line by line and just grab each chunk of text so we're doing that in this for Loop and then we're actually going to skip the first line cuz as you can see here the first line says comment and response so what I'm doing is if the first element of the line is equal to comment to just skip that line that's how I'm skipping the first line here but this will probably be different if your first line isn't comment and response with that first line out of the way the for Loop will go to the next line which is this comment and this response and then what I'm going to do is append the comment to the comment list and then append the response to the response list but also appending this Shaw GPT sign off to the response so we get that desired behavior and of course you can do whatever string manipulation you like here to ensure that the output format of the assistant is whatever you like so here I just have like a simple thing that I'm adding to it but if you want it to be in like a Json format or you want it to have like a particular format you can do whatever string manipulation you like to ensure that it has that format so this will just go all the way through the CSV file just grabbing each comment and each response so we do that it'll save all the comments to the comment list so that's what this is and then it'll have all my responses here or should I say all of sha gpt's responses here and so that's what that looks like you can see emojis and all okay so all we did right now is just put all the comments and responses from the CSV file in a list we haven't actually made it into this Json L format so in order to do that we go to this next cell where we Define our instruction string and we generate each example line by line so the way that works is again going back to this example here each example is a dictionary so it's this key value pair where the key is always messages for whatever reason and then the value is a list which is this list here however the list is a list of dictionaries so it contains three dictionaries the first one corresponding to the system message the second dictionary corresponding to the user's message and the third dictionary corresponding to the assistance response that's the goal that's what we're trying to do here and so the way I do it is Define the system message just once then I just go like index by index through the comment list we go 0 to 58 through the comment list because there are 59 elements and we just generate each of these three dictionaries we generate the system message we generate the user message and then we generate the assistant response and so that's what we're doing here so roll system content is always going to be this instructions string then we have roll user the content is going to be the I element of the comment list and then we have the assistant role and its content is the I element of the response list okay and then we just take these three dictionaries put them into a list and then we create a dictionary I kind of do two things in one step here so sorry if it's unclear but we take this list and create a dictionary with the key is messages and the value is this messages list and then we append this dictionary to the example list so it's matching this format here where each line of this text file is a Json format so it's essentially a dictionary and in the same way each element of this example list is a dictionary okay so there was a issue with with this line of code which I just fixed but what we're doing here is we're going to create our train test split we're going to designate a set of examples for training and then we're going to designate another set of examples for the validation data set and so the way I do that is I randomly generate nine integers between zero and the number of examples minus one and then I use those indices to create a new list for the validation data set so this is just creating a list here where it's going to go through each index in this index list here and it's going to copy that example from example list and put it into Data validation list and since these examples will still be an example list we can go through and iteratively remove them in this for Loop so now at this stage we've got two lists one is called example list which is a list of dictionaries that have the data in the proper format this should have 50 elements it does and then we have have validation data list which is just all the examples in our validation data set and so this is also a list of dictionaries and then we write these examples to two different files so the first one is for the training data and the second one is for the validation data and so kind of in a similar way as before we're opening this text file specifically a Json L file with the right flag on and then we'll just go through each element of example list and dump it into this Json file and then we'll do a new line character so it creates a new line for each example and then we'll do the same exact thing for the validation data set and so once we do that it should create these files here training data. Json L validation data. Json L and then we can upload these files to the open aai API for fine-tuning the way you do that is very similar to what we saw in the previous notebook when we're uploading the article for rag purposes for retrieval purposes so we just use this files. create method meod we open the file which we just saved but now we set the purpose as fine-tune as opposed to assistance do that for both the training and validation data I just put that there so it didn't run the job prematurely and then we can just run the fine-tuning job so the way it works here is we specify the training file the validation file ID which is just a property of these two objects we can set a suffix which is basically some unique identifier or some identifier we can put into the name of the fine-tuning job or name of the model so we can identify it and then we can specify what model we want to use so gp4 is not available for fine tuning the most state-of-the-art advanced model for fine tuning available is gbt 3.5 turbo so that's what we're going to use here and then we can just run this and it'll create this fine-tuning job and then if we go back to our open AI account click on this fine-tuning tab we can see this fine tuning job is running and so this actually kind of takes 15 or 20 minutes it's not something that runs like immediately fast but I did this yesterday so we can just use this model it's kind of like the cooking shows like we already cooked the pasta last night and we're going to eat it in front of you and so yeah it's already set up here so fine tuning jobs running once it finishes running after like 20 minutes or so we'll be able to use it another thing about fine-tuning is that your fine-tune models don't integrate into the assistance API they only work with the chat completions API which means those tools that we could include like retrieval like the code interpreter like the functions which we could just easily add to our system by specifying some keyword argument that's not available natively through the chat completions API so if you want to add rag you want to add tools you got to build out that functionality yourself using python code and of course you can use like Lang chain or llama index or some python library to do that but it's not integrated into chat completions I'm sure at some point the fine-tuning models will be incorporated to the assistance API but at this point it's not available okay so we're going to throw into test comment so we specify the model and then we have to define the messages like this so we Define the system message using the instruction string from before then we pass the user comment just like that then we can run that and then Sean GPT just says thanks it's more concise I'll say that for sure and I don't know if I've ever just said thanks to a comment but this is a pretty brief response let's see what another response looks like and so this is a comment that came in recently it wasn't in the training data set so let's see what the response to this looks like so the comment was I'm typing this after watching half of the video as I'm already amazed with the clarity of explanation exceptional and then sha GPT says wow thanks for the compliment that's pretty good that that is something that I would write I was playing around with this yesterday and it gave like a pretty cheeky answer I don't know if it'll generate that again but I found that funny it was something like thanks for commenting I hope the second half of the video is good and it had some emojis in there and I thought it was funny but now it's like generating responses that I would say oh yeah here we go this is something similar to what I saw yesterday so appreciate the compliment hope the second half doesn't disappoint so now let's see how it handles a technical question again the fine tune model doesn't have retrieval built into it if I wanted to add retrieval to this model I need to use like llama index or Lang chain or something like that to add it in there so let's see how it handles this of course it's not going to describe fat tilted how I did in the articles but it is giving a much more concise and closer response to what I would say compared to what we were seeing before like this monstrosity without the fine tuning there you go that's how you do fine tuning it takes a little bit more effort up front to get together your training data set but honestly it took me about maybe 20 to 30 minutes to manually copy paste those comments into the CSV file maybe another like 20 minutes to write this script to prepare the training data but you don't have to write the script you can just bring in your CSV file and it should be ready to go and then again I'll delete the training in valid ation files hopefully that doesn't ruin the fine-tuning job but it doesn't really matter and then more resources here I'll drop these in the description below so you can take a closer look at it so open AI guide for fine-tuning that was pretty helpful all of open ai's documentation is super clean and easy to understand let's see they have their API reference on fine tuning and then how to prepare your data for fine tuning is also available here so overall impressions of fine tuning I'm honestly pretty impressed I didn't expect it to work as well as it did especially because it only used 50 training examples in grad school if I wanted to train a good model I needed 100,000 examples to train a good predictive model but because of the power of GPT 3.5 turbo it's already a powerful model even fine tuning with just 50 examples results in really good results that's why personally I feel like fine-tuning is probably the biggest Innovation that we've seen in machine learning because if you can find the right base model for your use case and you can just tweak it with just a handful of examples you can get really good results in a fraction of the time that it would have taken you to develop that model from scratch okay so that's basically it we talked about three different ways to build a custom AI assistant using open AI we first talked about the no code solution which allowed you to build an assistant pretty quickly without any python coding next we looked at the assistance API which gave us a pretty straightforward way of creating assistance using python code which we can take and Port over to an application or a website and then finally we saw how we can fine tune an assistant to dramatically change its style and really Come Away with something that has good performance if you want to play around with the no Cod shot GPT I'll provide a link in the description below but if you want to interact with the fine tuned version of sha GPT drop a comment in the comment section below and I'll be sure to respond with shot GPT unless you explicitly don't want me to respond with shbt just let me know and I'll respond as myself so I hope this tutorial was helpful and gave you an idea of which approach might be best for your particular use case of building an AI assistant while open AI does currently have the state of the art large language models for building these things a natural question is how can we build these types of AI assistants using open-source Solutions so that's exactly what I'm going to cover in future videos of this series where we're going to explore both retrieval augmented generation or rag and model fine-tuning using open-source models and if you enjoyed this content this is just one video in a much larger series on using large language models in practice so if you want to learn more check out the series playlist Linked In the description below and in the comment section and as always thank you so much for your time and thanks for watching"
ytmK_ErTWss,2024-01-29T14:53:40.000000,LLMs EXPLAINED in 60 seconds #ai,I'm going to explain large language models in 60 seconds if you've heard of chat GPT then you've heard of large language models or llms for short while this might make you think that llms are just chat Bots that's not necessarily the case an llm is essentially a word predictor meaning given a sequence of words it produces the most likely next word this is just like the autocomplete function on your smartphone the way it gets good at this is by reading trillions of words from the internet and learning which word should come next what makes llms different than pre-existing Technologies like autocomplete is that they can take this ability to Simply predict the next word and generate humanlike and helpful responses to prompts one can then take an llm tweak it a bit and turn it into a powerful chat bot like chat GPT
mtu_v335bQo,2024-01-22T15:06:15.000000,3 Lessons from AI & Data Consultants #freelancing,recently sat down with 10 Ai and data science consultants and asked them how they get clients here are three key takeaways from those conversations first don't skip the discovery phase this avoids rushing into building something and inadvertently solving the wrong problem second is to always ask why why is this important to your business why do this now why me this simultaneously uncovers context for the project and the core problem the client is trying to solve third and finally is to find your lead Source while there are many ways you can get clients like referrals outbound marketing freelancing platforms content creation everyone that I talk to runs their business through one single lead source
8z-WPpP1_-8,2024-01-20T00:48:38.000000,AI & Machine Learning for Business | A (non-technical) introduction,these days it seems like everyone is talking about AI with new Innovations seemingly coming out every single week however if you're a professional an entrepreneur or business operator you might be thinking to yourself what is this AI thing and how can I use it to drive value in my business so in this video I'm going to share a non-technical introduction to Ai and machine learning and share how it can fit into how we do business and if you're new here welcome I'm Shaw I'm a data scientist turned entrepreneur and if you enjoy this content please consider subscribing as a great noost way you can support me in all the videos that I make so in this video I'm going to be talking about two main things in part one I'm going to be answering the question what is AI and along the way defining some key terms such as artificial intelligence which is what AI stands for we'll be talking about models and why those are important for AI and then finally we'll be talking about machine learning so once we have a good understanding of what all these terms mean we can turn to part two and figure figure out how we can actually use these Technologies I'll give a concrete example of what AI might look like in practice then I'll share five rules of thumb that I like to use when thinking about how and when to use AI in practice starting from the top what is AI so when you hear the term AI you might think chat GPT or AI generated art or you might think of the Terminator or something similar but if we just take a step back AI stands for artificial intelligence so we've got two words here and one of them is a bit more problematic than the other the first word artificial is not the issue artificial simply means something that is made by humans however the second term intelligence isn't so well defined and even today there's not really a consensus of what this word actually means however a definition that I like to use and one that I think is relevant in a business context is intelligence is the ability to solve problems and make decisions based on this operational definition AI or artificial intelligence is simply a computer's ability to solve problems and make decisions and so to get a better sense of what we mean by intelligence let's see it in action so let's say we wake up Saturday morning and we're trying to figure out what we want to do today and we look out the window and see this so if we see this and we're trying to decide between a pool day or a Netflix day I think most people would pick the Netflix day because the dark clouds in the sky is probably a good indication that the weather's not going to be so great another example is if we see this sales data with this peak in November and someone asks us what caused the peak we might reasonably say that oh was probably because of Black Friday which is one of the biggest retail days of the entire year and then finally if we see this text exchange of someone saying fine do what you want the other person responding are you okay and then that original person saying yeah whatever and if we ask are they really fine most people probably say no know even though the person is saying that they're fine their choice of words like fine do what you want and their use of whatever is probably indicating that they are actually not fine each of these scenarios is a situation where we used our intelligence to make a decision or to solve a problem so even though each of these examples is very different there's a Common Thread that runs through each of them which is intelligence requires knowing how the world works but of course the world is a mass place and it's very complicated so the way we make sense of this huge and complicated world is through models and a model is simply a approximation of a real world thing that can fit into our heads and more specifically models allow us to make predictions for example when we saw the dark cloudy sky that information went into our mental model of the world and allowed us to make the prediction that it's probably going to rain later however models aren't only restricted to the ones that we have in our heads we can also have computer models and in fact essentially all weather forecasts are done by computer models instead of your weatherman standing outside for 5 minutes and making a forast for the day so models be they mental models or computer models are an essential part of intelligence but a natural question here is where do these models come from so there are two types of models that I'm going to talk about the first I'll call principle driven models which are based on a set of rules these are things you might read in a book or learn from your grandma the other kind of model is based on past examples a principle driven model would say that if we see dark clouds in the sky then it's probably going to rain later while a datadriven model would say the sky is similar to other times when it rained and so each of these models comes to the same conclusion that it's going to rain but they are built on top of a different different Foundation but of course each of these models isn't restricted to something we hold up in our heads but these are things we can program into computers so principle driven models we can explicitly program computers to execute using standard programming techniques but more recently we've seen the rise of datadriven techniques to derive models the most popular of which is called machine learning machine learning is potentially another one of those buzzwords you may have heard around but it's a really simple concept machine learning is just a computer's ability to learn by example so the way this works is we have a set of training data which consists of predictors and targets where targets are the things that we're trying to predict like if it's going to rain or not and predictors are all the information that we're going to use in order to estimate the target the key Point here is instead of explicitly telling the computer how to take predictor to estimate the target machine learning allows the computer to figure out the relationship between predictors and targets simply by seeing many different examples of the two so the way that works is we pass this training data into a machine learning algorithm and out poops our machine learning model with this machine learning model in hand what we can do is get new data pass it into the model and obtain a prediction which is exactly what we did when we saw the dark cloudy Sky we looked out the window we received some information and we were able to make a prediction that it's going to rain later and a machine learning model Works in exactly the same way so up until this point we've talked about three things we talked about artificial intelligence which we Define simply as a computer's ability to solve problems and make decisions we also talked about models which were a essential part of intelligence because they allow us to understand how the world works works and then finally here we talked about machine learning which is a way a computer can generate a model based on past examples with these three terms defined we can move on to the second part of the talk which is how do we use these things how do we use these Technologies like Ai and machine learning to drive value in our businesses so I'll start with a concrete example and talk about credit decisioning which is something I have some real world experience with so I can talk about it some somewhat intelligibly so when we're talking about credit decisioning what we're talking about is people applying for a loan and financial service providers evaluating that application and making a decision of whether to approve the loan or deny the loan so the way that works is someone submits an application for a loan and the financial services company makes a decision of whether to approve deny in the terms of the agreement so the traditional way of doing this is that the application goes to an underwriter which is a person who makes the decision and defines the terms of the contract however now that we've learned about Ai and machine learning we might think oh we can just replace the human underwriter with an AI underwriter right and the answer to this question is yes and no while it might be easy to imagine replacing a human job with an AI the reality is a bit more complicated so what this looks like in practice is something like this with all these steps within the blackbox being our AI underwriter and really what it is is not just a machine learning model but rather a large number of business rules data and it processes all working together to take the application and finally make a credit granting decision so although machine learning is a critical part of this AI underwriter here it is only a component in a much broader solution and so this is often the reality of what AI looks like in practice although from an outside view it might look as simple as we have an AI underwriter in reality what's going on under the hood is a bit more complicated which is an important point to keep in mind when trying to implement AI Solutions in your business so while this might be an illustrative example it may not give us a good idea of how and when to use AI to solve business problems so for that I'm going to talk about five rules of thumb that I like to use when thinking about how and when to use AI in practice so the first one is to focus on problems not Technologies next is to apply AI to problems that you solve repeatedly next we have look for problems in which a 70% solution is good enough to generate value the fourth is pick situations in which one failure of your AI solution doesn't erase nine successes and then finally start simple fast and easy and build sophistication as needed so I'm going to talk through each of these rules of THB one by one and share concrete examples of each first focus on problems not Technologies so this brings up what in data science we might call the hammer problem which is when you have a really nice Hammer everything looks like a nail so let's say you have some problem in your business something is broken if you take a technology first approach you might grab your hammer and say I got this which obviously is not going to solve the problem and is probably going to make things a lot worse so an example of this from my personal experience is something I saw over and over again which was last year I had a lot of clients reaching out to me asking for help in building a custom chatbot or fine-tuning a chatbot for a particular use case and this was a classic example of the hammer problem because often what had happened was the client had seen the power of chat GPT and saw all the incredible innovations that have been happening in the space of natural language processing and large language models and was probably thinking something like I need one of these for my business however the Trap that you fall into with the technology first approach is that you can spend a lot of time and money building a solution for a problem that isn't very critical to your business and essentially this time and money is wasted however let's flip things around instead of starting with the technology what was starting with the problem look like so let's say we have a problem where our customer support line is overwhelmed well from here instead of jumping into building something you jump into problem solving because when you have a tool your instinct is to build but when you have a problem your instinct is to solve the problem so you might ask why are people calling if people are calling for some specific piece of information you can update your FAQs and if that doesn't cut it you can improve call routing to make sure that callers are getting sent to the right person and there isn't time wasted where customer support representatives are on the phone with someone just to transfer them to someone else and then maybe after exploring a few Solutions then you start thinking about building a chatot for your website but the key Point here is that when you start with a problem you don't jump to building a solution you jump to finding the root cause of the problem so you can find the best solution and ultimately When comparing these two approaches the technology first approach approach on the left and the problem first approach on the right you almost always want to go with the problem first approach because that is almost guaranteed to generate value in your business while the technology first approach might be intellectually stimulating and exciting is often something that doesn't drive any real value the next rule of thumb is to apply AI to problems you solve repeatedly and the reasoning behind this is that AI is just the continuation of the story of Technology since the beginning of time it's simply a tool to help make our lives easier so the problems that you're solving over and over again are great candidates to apply AI to for a few reason one if you can automate it with AI you no longer have to spend a lot of your time solving that problem or even if you reduce the amount of effort it takes you to solve that problem by some marginal amount it can still translate to some big gains other reasons are if you're solving a problem repeatedly you likely have a deep understanding of that problem which puts you in a good position to build good solutions to solve it and finally if you're already solving a problem that means you have an existing solution which is a fantastic starting place for building an AI solution so an example of this is something that I use in my own work which is a literature review assistant so I read a lot of papers about Ai and machine learning for both my content and my Consulting business and often when reading research articles I discover gaps in my my understanding and so this is a problem that I face over and over again I'm reading the paper and then I stumble across a sentence that seems obvious to the authors but is completely not obvious to me so for that I will turn to chat GPT I'll upload the PDF of the paper ask chat gbt what the paper is about then ask chat gbt specific questions until I have a clear understanding of what's going on so using Chad gbt in this way has significantly sped up how quickly I can read articles because now instead of spending a 30 minute tangent on Google trying to figure out what a particular term means or putting an idea in a larger context Chach PD does a pretty good job of explaining things and adding additional context where needed the next rule of thumb is find situations where the 70% solution is good enough where this is coming from is a model is simply an approximation of the real world thing no model is ever going to be perfect and there's a famous quote from statis George box which goes all models are wrong some are useful so the key thing is to accept that your model is not going to be perfect but pick the ones that are actually useful to you in whatever problem you're trying to solve and so a good example of this is Spam filtering the way that works is you have a bunch of spam emails flooding your inbox in this situation even an imperfect model is very valuable because even a 70% reduction in spam emails is very helpful that'll give a thumbs up for from any user another important rule of thumb is ensure that one failure doesn't erase nine successes and essentially what we're talking about here is find the low stakes or low exposure situations an example of this might be using chat gbt as a writing assistant is pretty low stakes if it gives nine good recommendations followed by one bad recommendation for writing it's no big deal you can just ignore that recommendation and move on with your life however if you're using using Chachi BT to make cancer diagnosis it doesn't matter if it's right nine out of 10 times that one time when it's wrong can have a tremendous negative impact so that is a situation where you probably don't want to use Ai and if you do you have to be very thoughtful about how it's implemented and then the final rule of thumb is to start simple fast and easy and each of these words simple fast and easy is important so starting simple is important because sophisticated Solutions are fragile and costly they'll cost you a lot of money to build and they have a high likelihood of failure because they are well complicated next you want to build fast because to build good Solutions you'll need to iterate so that means you'll need to try out a lot of different things and if it takes you a long time to do one iteration it's going to take you a long time before you implement a good solution and then finally you want to make it easy so you want the solution to kind of be on the way and not something way out of the the way for people because if it's hard to access no one's going to use it including you even if you're the one that's implementing the solution so let's look at a specific example let's say we're trying to implement a sales email sequence in our business the way this start simple fast and easy approach would play out is you'll start by writing all these emails by hand so what that looks like in my business is I'll have someone book a discovery call with me and I'll send them a follow-up email asking them a couple of follow-up questions based on their specific use case that's me doing the process by hand however after doing that for a bit I've naturally developed email templates for responding to someone booking a discovery call and like a post Discovery call email and then maybe another template for following up with people after 40 days or following up with people after 90 days and so on and so forth so over time instead of just writing emails by hand you start to develop templates and then over time those templates can get loaded into a CRM so you use a CRM tool to automatically fill in these emails with some bit of personalization like including people's names and maybe some other information that they provide but then you can take this one step further and use some kind of large language model or NLP solution to make the emails a bit more personalized so instead of just using a template and just filling in a name you can make the email sound more like a person so all that to say it's good to start here you know start by just doing things by hand and build toward that sophisticated solution often times when you're a small business you'll find that just doing it by hand or having some templates are more than suitable so for me I have a small Consulting business so I'm spending most of my time here I don't have a CRM but let's say you have like a 10p person business then you might want to be looking at a CRM then let's say you have a larger Enterprise and let's say you're working with hundreds or thousands of clients then maybe building the AI solution makes sense but the value in taking this simple fast and easy approach is that you don't artific officially just jump to the end you take it step by step and you only move on to the next level of sophistication if the value is there if it makes sense for your business we talked about a lot of different things so just to recap a few key terms we talked about Ai and how it's a computer's ability to solve problems and make decisions we also talked about models and how they help us make predictions and that they're a necessary part of any AI system and then finally we talked about machine learning which is a datadriven way computers can generate models from past examples as opposed to being programmed explicitly and then we talked about how we can use AI through five different rules of thumb focusing on problems not Technologies applying AI to problems you solve repeatedly seeking problems where the 70% solution is good enough identifying problems where one failure doesn't erase nine successes and taking this simple fast and easy approach to iteratively develop AI Solutions so while this was a pretty highlevel introduction I hope it gave you some clarity about what AI is and how you can start to use it in your business this is the first video in a larger series on how to use AI in business in future videos I'm going to dive into more the project management side of machine learning and model development so if you have any specific questions or anything specific you'd like to see in future videos of this series please drop those in the comment section below and as always thank you so much for your time and thanks for watching
jGn95KDWZMU,2024-01-11T22:12:29.000000,5 Questions Every Data Scientist Should Hardcode into Their Brain,data science is more than just building fancy machine learning models when you boil it down the key objective of data science is to solve problems the trouble however is at the outset of most data science projects we rarely have a well-defined problem in these situations the role of a data scientist isn't to have all the answers but rather to ask the right questions in this video I'll share five questions that every data scientist should hardcode into their brain to make ident identifying and defining business problems second nature and if you're new here I'm Shaw I make content about data science and Entrepreneurship and if you enjoyed this video please consider subscribing that's a great no cost way to support me in all the content that I make before diving into the questions I want to give some context for where they are coming from like many others when I started my data science journey I was hyperfocused on learning tools and Technologies while this technical Foundation is necessary to be a a successful data scientist focusing too much on tools creates the hammer problem which is when you have a really nice Hammer everything looks like a nail this often leads to projects that are intellectually stimulating yet practically useless I finally outgrew this approach when I joined a data science team at a large Enterprise the key lesson from that experience was the importance of focusing on problems rather than Technologies this means that one should gain a sufficiently deep understanding of the business problem before writing a single line of code and since as data scientists we don't typically solve our own problems we gain this understanding through conversations with stakeholders and clients getting this right is important because if you don't you can spend a lot of time and money solving the wrong problem this is where problem Discovery questions come in over the past 6 months I've developed a bit of an obsession with cracking these early stage discover y conversations with stakeholders and clients my approach to getting better at this has been twofold first I interviewed 10 seasoned data Freelancers about their best practices and how they approach these conversations and second I took as many Discovery calls as possible which ended up being around 25 the five questions I share here are the culmination of all these efforts while this is by no means a complete list these are questions that seem to come up over and over again so the first question here is what problem are you trying to solve while in theory this should be the only question you need to ask in practice things don't typically work out that way in most instances clients aren't super clear on the problem that they need to solve and even if they are I typically will need to do some catching up to better understand the business context either way this question is helpful because it ideally brings up follow-up questions which allow me to dig deeper into the client's world for example if a client says we tried creating a custom chapot with open AI but it didn't provide good results I might ask what was the chapot used for or what makes you say the results weren't good and a lot of times if a follow-up question doesn't come to mind I find a really helpful practice is just to rephrase and summarize what the client tells me most times this is another way to keep the conversation going and keep digging into the challenges that the client is facing a natural way to follow up the what question is why this is one of the most powerful questions you can ask a client because it can unlock the floodgates to the client's motivations goals assumptions and Beyond however why questions have a tendency to make people defensive which is why having multiple ways of phrasing this question can be helpful some examples of this are as follows why is this important to your business why do you want to solve this now what does solving this mean for your business how does this fit into the larger goals of your business why do you want to use AI to solve this problem the key benefit of asking the why question or any of its variants is that they allow you to dig more deeply into the client's problem and ultimately identify the root cause this is reminiscent of Toyota's five wise approach which teaches to get to the root cause of any problem one should ask why five times these first two questions of what are we doing and why are we doing it are two of the most fundamental questions in business so getting really good at asking what and why in many different ways can take you very far the next question is what's your dream outcome I like this question because it essentially combines the what and why questions and it tends to get people to speak to their vision of the project in a way that may not come through when asked directly having multiple ways of asking what and why is important because it often takes a few passes to really get to the root cause of a client's problem two related questions here are what does success look like and how would we measure it these are a bit more pragmatic than a dream outcome but are helpful for transitioning from asking what and why to how the next question is what have you tried so far this helps narrow down potential Solutions in two ways one it helps avoid wasting time on things that didn't work and two any new project should build upon existing work this latter point is based on the philosophy that data science projects should seek incremental Innovation therefore they should be simple and iterative for situations where the client hasn't built anything so far one can ask any of the following questions what is the existing solution how do you solve this problem now what have others done to solve a similar problem in either case these questions help set the stage for the project and help you avoid Reinventing the wheel the final question is one I got from Master negotiator Chris Voss which is why me asking this question is an effective way to reveal people's motivations for talking to you often this Sparks additional context of what led them to to you and how they see you fitting into the project which is helpful for next steps sometimes however people don't have good answers to this question which may indicate they don't actually want to work with you and they're holding back some deeper motive such as they're looking for free consulting or they're looking for a competing bid to take to the person they actually want to work with a key lesson for me these past 6 months was to learn these questions I.E hardcode them into my brain but then forget about them the point isn't to mindlessly go down a list of questions when talking to clients but rather get to the point where these questions naturally form in your mind during the flow of conversation this intuition is something that can only develop through practice toward that end here are three key takeaways that have been helpful to me in developing this skill set first don't just study these questions use them while this may result in a fair share of awkward moments it's all part of the learning process and don't worry I'm still learning to second is to stay curious the goal of these early stage conversations isn't to look smart or sell but rather to learn which brings up the final takeaway listen more than you talk my rule of thumb is to wait until the last 5 to 10 minutes of a 30 minute call to start offering recommendations and next steps prior to that my challenge is to ask questions rephrase and summarize client answers and to ask follow-up questions following my Natural Curiosity if you enjoyed this content please consider subscribing that's a great no cost way you can support me in all the videos that I make to read more about this topic check out the blog in TS data science which you can access using the friend Link in the description below like I said earlier this is by no means a complete list so if you have anything to add please drop those in the comments section below and as always thank you so much for your time and thanks for watching
scAxgeGadv4,2024-01-10T14:38:49.000000,2 Types of Data You Should Know #datascience,are two types of data that every data scientist should know about they are what we can call thin tailed data and fat tailed data thin tail data are gaussian like things they have the key property that no single observation will significantly impact the aggregate statistics of the data some examples of thin tail data include Heights weights and test scores fat tail data on the other hand are more parade like these data have the key property that a single observ can and often will significantly impact the aggregate statistics some examples of fat tail data include sales wealth Wars pandemics and so many other things that we care about so before you do any kind of analysis or build any kind of model ask yourself whatat kind of data am I working with
GvRPKPCg5no,2023-12-26T23:07:01.000000,How to learn anything #learning,looking dumb makes you smart this was one of my biggest takeaways from grad school and the point is once I realized that by not being afraid to look dumb not being afraid to ask questions I actually learned so much more so much faster than I did before and this is something that has served me very well even after grad school into the corporate world working as a data scientist it's natural to want to look smart or want to give off this Persona that you know what you're talking about and you're professional you kind of get everything that everyone's talking about but more often than not this holds you back
BGZu6WxevoM,2023-12-19T22:05:55.000000,How Much YouTube Paid Me in My First 6 Months of Monetization (as a Data Science Creator),"6 months ago I joined the YouTube partners program which provides ways for creators to earn money from their YouTube videos while you may have heard stories of big creators making hundreds of thousands or even millions of dollars on YouTube what you don't often hear are the stories of smaller creators who are just at the beginning of their YouTube journey in this video I'm going to break down how much money I made as a small Creator in my first 6 months of monetization my goal with this video is to help give those considering a similar Journey a realistic view of What monetization on YouTube looks like in the early days if you're new here my name is Shaw I make content about data science and Entrepreneurship and if you enjoy this content please consider subscribing that's a great noost way you can support me in all the videos that I make to give a little background I started making YouTube videos about 3 years ago while I was getting my physics PhD as someone who had survived grad school with the help of YouTube I wanted to give back by sharing content that would have been helpful to a past version of myself I did this for about 2 and 1/2 years before finally satisfying all the different criteria for monetization for those who aren't familiar there are three main criteria for getting monetized on YouTube the first is to upload three videos the second is to get 1,000 subscribers and the third is to get 4,000 watchtime hours as far as timing goes it took me about 3 months to hit that first Criterion 2 years to hit that second Criterion of 1,000 subscribers and then finally 2 and 1/2 years to hit that final Criterion of 4,000 watchtime hours YouTube's partners program provides creators five different ways of monetizing their content the three that I have set up are supers watch page ads and shorts feed ads supers are a native way that viewers can leave tips for creators while watch page ads and shorts feed ads are exactly what they sound like advertisers pay YouTube to run their ads on the platform of which creators get a portion the two monetization options I don't currently use are memberships and shopping while these are things I may set up in the future at this point I don't really have any good ideas of how to provide value through these means all right with all the context out of the way let's get down to it how much money did I make on YouTube after 2 and 1/2 years of waiting in my first month monetized on YouTube I made a grand total of drum roll $20.82 while this might sound like a very small number it is actually more than I expected I've actually heard stories of creators getting about 20 cents in their first month of monetization so one thing that makes a big difference in how much creators earn are CPM and RPM which are short for cost per Millie and revenue per Millie Millie just means thousand so cost per Millie is how much YouTube charges advertisers to run their ad one th000 times on a particular video while Revenue per Millie or RPM is how much YouTube pays the Creator for those 1,000 views so different topics draw different audiences which correspond to different cpms and RPMs my content touches on education technology and business which may have higher CPM and RPM than some other genres so while my first month of monetization wasn't so hot lucky for me this number went up over time in my second month of monetization I made $285 in my third month I made $366 so what we're seeing here is linear growth with about an $8 increase in Revenue per month however this growth trajectory started to change in October in my fourth month of monetization my Revenue doubled and hit $73 86 in the next month it almost quadrupled and hit $287 38 and so far in December I've hit $21.56 so today's December 15th if we wanted to guesstimate what this month's Revenue might be it'll probably be around $400 these numbers highlight a key property of content creation namely growth isn't linear it's exponential the catch however is exponential growth is painfully slow in the early days a great example of this is that it took me 2 years to hit 1,000 subscribers but then it took me 6 months to hit the next 1,000 subscribers and then after that it only took me 3 months to hit the next 1,000 subscribers and now in the past 3 months my subscriber count has more than doubled so a natural question is what the heck is driving this growth and we get a pretty clear answer when looking at the analytics so let's see what that looks like so here we have the channel analytics and I'm a data scientist so I like to think about analytics I like to think about explanations and models for why do things happen the way that they do YouTube analytics is super interesting because it's a great example of exponential growth and what we might call a fat tailed distribution and if you're not familiar with fat taals I actually got a video series all about it so go check that out but anyway I think the growth that I'm seeing on my channel has a pretty simple explanation and so if we see the top five videos for the past month a pattern starts to emerge where three out of five of these are all about large language models but in particular there's this video fine-tuning large language models that has kind of gone viral and gotten a lot of traction so I think there are two things kind of driving this growth so like one people love large language models it's a really hot topic right now everyone's talking about generative AI Etc the second thing I think is driving this is the fact that these three videos are all part of the same six-part video series and I think making content in series format helps drive this exponential growth and there's a really simple explanation for that which is Network effects basically if someone watches one video in this series it increases the probability that they'll watch another video in this series and if they watch another video in this series it increases the probability they'll watch yet another video in this series so you get this positive feedback loop this positive reinforcement and this is really Amplified by my strategy which was I made a six-part video series and I also made a six-part Blog series on medium which was published in towards data science and towards data science has their own distribution and then you have Network effects working over there so I think all these different factors came together to kind of drive this viral phenomenon in this exponential growth however there are two other videos here that are also performing really well which is how to make a data science portfolio completely for free with GitHub pages and how to create a custom email signature with Gmail and I think the thing with these these two is that these are just like Evergreen content and these solve two very particular pain points that I have experienced in the past so it's not super surprising that other people also have these pain points and are looking for this content I personally found a hard time finding good tutorials on these two topics so I decided to just make them because they would have been very helpful to me when I was first trying to solve these problems so adding up all my earnings from these first 6 months of monetization my grand grand total revenue from YouTube in 2023 is $648 34 while there's no way I can pay my bills with this kind of income it still blows my mind that I can post a video on YouTube and money just appears in my bank account three other ways I've monetized content creation this past year our earnings from medium which is a blogging platform sponsorships actually just one sponsorship and consulting which I'll explain more starting with the first one medium is a membership based platform form so the way monetization works there is that writers have the option to make their stories for Members Only from which they gain earnings from readership and engagement mediums partners program works a bit differently where I was able to start earning money pretty quickly when I started posting on the platform which means I've been earning on medium for the entirety of 2023 and so when I add this all up this comes to a grand total of $5,185 182 the next way I made money outside of YouTube earnings was through a sponsorship deal this consisted of including an affiliate Link in the description of my video on how to make a custom Gmail signature this generated $200 in earnings plus a 15% referral fee for anyone that signs up for the sponsor service today we don't have any signups so so far the earnings have just been $200 then finally I put Consulting on the list since I don't make enough money to support myself from content creation my main source of revenue for the near future is my independent Consulting business where I help businesses build Ai and analytics products currently all my leads for this business come via my content whether it's YouTube videos or blogs I post on medium sometimes people even directly book paid calls with me through my calendly link this happened five times this past year generating $365 in revenue and so adding together all three of these non youube Revenue sources this comes to a total of 5,000 7478 which again is nothing you can pay your bills with but this is mindboggling when I first started making content I never thought I would be able to generate this kind of Revenue however content creation has unlocked so many new opportunities even Beyond this direct monetization like I mentioned earlier it's my main lead source for my Consulting business but what I would say is more valuable than that is all the amazing connections I've been able to make with like-minded people who are on a similar Journey as me while I didn't have time to cover all aspects of my earnings and analytics if you have any specific questions please feel free to drop those in the comments section below I'm more than happy to share details that might be helpful and relevant to you and I hope this has been helpful for anyone curious about content creation while it might be easy to glamorize the life of a content creator the reality of being a small Creator which is actually most creators may not be what you expect and if you enjoyed this content please consider subscribing and sh sharing with others that's a great no cost way you can support me in all the videos that I make and as always thank you so much for your time and thanks for watching"
sS10PXKqm7o,2023-12-19T15:03:57.000000,Why You Need to “Play” #getpaidtolive #podcast,think of play I think explore you know I think mistakes I think no rules essentially and if you're not living your dream life already then I would assert necessarily you need to play because your current life is not your dream life right so how do you expect to Live Your Dream Life by just continuing to live your current life you know continuing to live by the same rules not exploring new possibil not willing to make mes you know and if you kind of constrain yourself into your current life and your current situation and the only way out of that is to play is to explore and so you at least have a chance of achieving your dream life yeah but the only way to do that is through play
0iFEtnHyzE0,2023-12-18T14:55:36.000000,Fine-tuning EXPLAINED in 40 sec #generativeai,I'm going to explain fine-tuning a large language model in less than 60 seconds fine-tuning is when you take a pre-trained machine learning model and adapt it for a particular use case the analogy I like to use for this is fine-tuning is like taking a raw diamond as we see on the left here and refining it into something more valuable as we see on the right so with this analogy the raw diamond would be a pre-trained model like gpt3 while the refined Diamond that you would would actually buy is going to be something like instruct GPT or chat GPT drop your fine-tuning questions in the comments below
15Kd9OPn7tw,2023-12-11T18:29:09.000000,4 Ways to Measure Fat Tails with Python (+ Example Code),"this is the third video in a largest series all about power laws and fat tails in the previous video of the series we saw how we can fit power law distributions to empirical data while this can be handy when working with fat tailed data in the real world this idea of fat tailedness is something that goes beyond just a power law distribution in this video I'm going to break down four ways we can quantify fat tails and then I'm going to walk through how you can Implement these Tech techniques in Python and if you're new to the channel welcome my name is Shaw I make videos about data science and Entrepreneurship and if you enjoyed this content please consider subscribing that's a great no cost way you can support me in the videos that I make so jumping right into it in past videos of the series we've been talking about fat tales which we defined as when rare events Drive the aggregate statistics of the distribution we saw a celian example of this in the Paro distribution where for example 80% of sales May Come From 20% of your customers but this idea of fat tails is much more General than any particular distribution fat tailedness is something that lives on a spectrum here we have a sort of distribution landscape ranging from thin tailed distributions all the way to very fat tail distributions we can see the Paro distributions on the right hand side here with increasing level of fat tailedness on the far left we see the gaussian distribution which is an Exemplar of thin Tails or not fat tail data and then we have this middle region which we saw is populated by the log normal distribution taking this view of fat tailedness as not some binary property of a distribution being fat tailed or not fat tailed but rather a quantity that lives on a spectrum this naturally begs the question how do we Define fat tailedness while there is no true measure of fat tailedness there are a handful of heris we can use in practice to help us quantify how fat tailed a given distribution is and so here I'm going to talk about four heris the first being the power law tail index next being curtosis third heris is the log normal distributions Sigma value and the finalistic uses a metric called Kappa defined by n TB whose ideas we talked about in the first video of this series so starting with heris one the power law tail index as we saw in the first video of this series the smaller a power laws tail index I.E Alpha the fatter its tail this is illustrated by this plot here where we see with increasing Alpha values you can see the tales of the distribution getting fatter and fatter so rare events become more and more common as Alpha increases these plots here are generated by this probability d density function where a is just some constant X is a specific value of a random variable and Alpha is the tail index so this is the parameter that controls the shape of these Tails here so if a smaller Alpha implies a fatter tail we can naturally use this as a way to quantify fat tailedness so in practice what this might look like is a twep process step one is to fit a power lot to your data and this is exactly what we did in the previous video of the series and then step two is to extract the estimate for Alpha so while this might be a simple approach it has one obvious limitation which is if a power lot doesn't fit your data well then this technique is going to break down because the alpha value is essentially meaningless another way we can quantify fat tailedness is via curtosis which is a measure of non-gaussianity so just taking a step back the Exemplar thin tail distribution is the Beloved gaussian which looks like this so a gaussian is thin tailed because the rare events are so rare they are essentially negligible for instance an event that is 5 Sigma away from the mean has a 1 in 3.5 million chance of happening which is in stark contrast to what we see in a paredo distribution taking this as a starting point the rough idea with heuristic 2 is saying okay gaussian equal thin tailed so something that is ungan or not gaussian should be fat tailed this motivates us to look toward these so-called non-gaussianity measures of which curtosis is the most popular we can Define curtosis according to this expression here where mu4 and mu2 are the fourth and second moments of the distribution which we can Define like this so the intuition with curtosis is that it will increase as as data accumulates in the tail so more data in the tail means a higher kosis so higher kosis means fatter tails however there's one major limitation with using cetosis namely when talking about Paro distributions with an alpha value less than or equal to four it is not defined so even if in practice you can always compute a value for kosis when working with very fat tailed paros those values are meaningless so here is sck number three is using the log normals Sigma value where the bigger the sigma the fatter the tail we got a flavor of the log normal distribution in the past videos of the series where we saw that the log normal distribution was a bit mischievous because sometimes it could appear more gaussian like and other times it could appear more parade of like based on this Sigma value an example of this is shown here where a log normal distribution with mual 0 and sigma = 0.2 looks a lot like a gaussian however a log normal distribution with the same mean but Sigma equal to 2 looks a lot like a Paro digging a little deeper this is the probability density function for the log normal distribution so this equation is what is generating these plots up here and sigma sits here and here in the expression and so this is what is driving the shape of this long tail this observation of bigger Sigma implies fatter Tails naturally motivates us to try to use Sigma as a way to quantify the fat tailedness of a distribution so we can do this in a very similar way as we did heuristic one where we follow a twep process Step One is we fit a log normal distribution to our data and then the second step is to extract the sigma estimate and this is exactly what we did in the previous video of this series while again this is a straightforward procedure it like a good job at explaining your data then this technique is going to break down the finalistic is tb's cetric this is probably the most robust of the three heris we can use however it is also the most sophisticated so I'm going to try to break it down step by step if you don't really care about the mathematical details feel free to jump ahead to the example code to see what working with these heris in practice looks like Kappa is defined according to this expression here where it's a value that sits between 0 and one where zero implies the distribution is maximally thin tailed and one implies the distribution is maximally fat tailed and just as a note this is for unimodal data with finite mean so there's a lot going on in this expression here so let's just break it down step by step first let's look at these values here n KN and n n KN and N are both integers that correspond to two samples we can call S Sub and not and S subn and so Kappa is actually a metric that compares two samples together SN KN and SN where SN is equal to the sum of N samples drawn from a particular distribution so for example if we have a gaussian distribution we would just take n samples and then we would sum them all together and that would give us s subn notice these are little S's and this is a big S these are Big S's and then the simplest example is S Sub one which you just draw one sample from this distribution if we choose n to be 30 and N not to be 1 we would just plug in 1 and 30 here we would plug in a 30 here a one here a 30 here and a one here but what does this thing mean taking a closer look at this term here M denotes the mean absolute deviation which is a measure of dispersion around the mean breaking down this equation here this notation is just representing the mean absolute deviation of a particular sample s subn and it's defined as follows we get the expectation value of the sample so we compute its mean then we subtract the mean from each value in the sample take its absolute value and then take the average of all these subsequent values this is broken down a bit more here e is denoting the expectation value s subn is equal to the sum of a bunch of subsamples and then S Sub I is just some subsample of a distribution the intuition here is that more data in the tail implies a larger mean absolute deviation because again it's a measure of dispersion the more dispersed the data are the larger the mean absolute deviation so let's look at a specific example of this say we choose n not to be 1 and N to be 30 in the thin tailed case m30 is going to be about equal to M1 so their ratio is going to be about equal to one on the other hand if you're working with fat tail data m30 is is going to be much larger than M1 which implies that their ratio is going to be much greater than one this is illustrated by these plots here where on the left we're seeing how the mean absolute deviation scales with increasing number of subsamples so on the left we have the sum of N Gans and on the right we have the sum of N paros you can see for the sum of gaussians the mean absolute deviation goes from about 1 up to about 8 however when you look at the sum of N Paro it starts around one and it ends up at around 500 and so with this observation that for thin tail data this ratio is about one however for fat tail data it's going to be much greater than one we can see how this influences the cetric defined earlier and so we have our thin tail data this is just plugging in values of n notal 1 and Nal 30 and so if this ratio is about one then this value is going to be very small that's because the log of one is equal to zero so as the ratio approaches one this term will approach zero so if the denominator is small that'll make this whole term here big and then when you subtract it from two it's going to make the whole thing small that's how we can see that zero implies maximally thin tailed however in the fat tailed case we see the exact opposite so if this ratio is Big then this denominator is going to be big making this whole term small and then making this whole term big that's where one implying maximally fat tailed data comes from so if this was all way too much math then you were hoping for here's the key takeaway big Kappa implies a fat tail small Kappa implies a thin tail so now let's dive into some example code here we're going to quantify the fat tailedness of data from my social media accounts this is the same data set we saw in the previous video of the series and the data set and code are available on the GitHub repo linked here and in the description below we'll start by importing some handy modules namely matplot lib pandas numpy the power Law Library we saw in the previous video and then we'll import curtosis from the scipi library next we'll load in the data again the data is on the GitHub repo stored in separate CSV files what I'm doing here is iteratively Reading in each CSV as a pandas data frame and then I'm storing each data frame into a dictionary it's always a good idea to look at your data no matter what data science project you're working on here I'm just looping through each of the different data sets again so they're all stored in this dictionary and then I'm just iteratively extracting the data frame plotting its histogram so this plot histograms function is a bit long and nothing really special it just uses mat plot lips hist function that functions on the GitHub if you're curious and then the figure is being saved another thing that makes sense to look at since we're trying to evaluate fat tails here we're going to look at the top five record records of each data set by percentage here are the histograms so that function actually plots two histograms the histogram of the raw values and then the histogram of the log of the raw values this data here are the number of medium followers gained on a month-to-month basis here we have YouTube earnings on a video basis and then finally here we have daily LinkedIn Impressions looking at all these histograms it's pretty clear that they're all fat tailed to some extent medium seems to be the most fat tailed and maybe LinkedIn and YouTube are following slowly behind with LinkedIn being a little less fat tailed that's why I like histogram so much you can extract so much information from just looking at the histogram and a log histogram of any data set next looking at the top five records by percentage here we can see medium followers is significantly fat tailed where 60% of total followers gained is just coming from two months and we see a similar thing with YouTube earnings where about 50% of earnings are coming from the top four videos however for LinkedIn Impressions just looking at the top five we don't see something as Extreme as what we see for medium followers in YouTube earnings so from this view we might say that Medium followers is the most fat tailed YouTube earnings is the next most fat tailed and then LinkedIn impressions are the least fat tailed of the three however let's make this a bit more quantitative by Computing each heuristic for all three data sets starting with heuristic one the power law tail index here I'm using a for Loop just to again Loop through each of the three data sets stored in this dictionary but the key line of code is just this one line here where we fit a power LW to our data and so this is exactly what we did in the previous video of the series and then these seven lines here is just printing the results and so what that looks like is this where medium followers we have an alpha value of 0.83 for YouTube earnings we have an alpha value of 0.9 and then for LinkedIn we have Alpha value of 1.47 also note with these fits is that you'll have a x Min value so this fit isn't for the entire distribution it's starting at some minimum x value and then the alpha value corresponds to all the values greater than that the power lot tail index matches our intuitions from looking at the top five records and from the histograms where medium followers are the most fat tailed because they have the smallest Alpha followed by YouTube earnings and then LinkedIn impressions are the least fat t moving on to heuristic 2 curtosis we do a very similar thing but here it's super easy because we can just compute curtosis in one line of code and it's already available off the shelf in scipi we just load in our data and then here we're just printing the results basically doing some string concatenation here so curtosis equals curtosis of our data and then fer equals true it's just like a convention to make it such that negative curtosis values indicate something something is even more thin tailed than a gaussian distribution while positive values how more fat tailed the data are from a gaussian the results look like this with medium followers having a cetosis of about 21 YouTube earnings 11 and then LinkedIn impressions of 46 this is telling us a different story because the larger the curtosis implies the fatter the Tails so this is saying that LinkedIn actually has the fattest tail followed by medium followers and YouTube earnings one cause for suspicion here is as we saw in heuristic 1 when we did the power law fit all three data sets had Alpha values that were very small which was below that Alpha equals 4 cut off for which the curtosis is not defined for a Paro distribution so it's probably wise to take these values with a grain of salt because if the data truly do follow a power law and the alpha values are as small as we saw in the previous slide any empirical cryosis calculation you do is going to be meaningless because the TR true ptosis is not defined heris stick number three this is very similar to heris stick one we just load in the data do the power law fit it's actually the same exact syntax here the only difference is that when we print the results instead of extracting the alpha and X Min value from the power law fit we're extracting the MU and sigma value from the log normal fit and so that's one cool thing about this power Law Library when using this fit method it'll automatically estimate the power law distribution parameter and the log normal distribution parameters the results look like this where medium followers has a sigma of 2.4 YouTube earnings 5.4 and then LinkedIn Impressions 2.3 and so we're again seeing another story here where since the larger the sigma the fatter the tail Here YouTube earnings is the most fat tailed followed by medium followers and then LinkedIn Impressions however something that stands out to me is that the Mew value here is negative which might tell me that this log normal fit may not do a good job job at explaining the data and as we mentioned previously one of the limitations of heris 1 and herotic 3 is that if the power law or log normal fit doesn't do a good job at explaining the data then these approaches tend to break down moving on to herisk number four since I couldn't find an off-the-shelf implementation of this metric this is going to take a few more steps first we need to implement the mean absolute deviation again this is the expression we saw before and then this is what it looks like in code code next we need to generate end samples from our data set in practice you don't have like a true distribution which you can sample so readily often you just have a set of values you just have an array of numbers and we need to somehow generate samples from that empirical data to do the Kappa calculation we're calling the definition of SN as you just take a bunch of samples from some underlying distribution and sum them together all I'm doing in this code here is instead of sampling from some true distribution I just sample from the empirical data set this is probably a bit naive and I'm sure there are some issues with it but this will be sufficient for our purposes here what this looks like is just initializing SN to be zero and then taking a random sample from our data X so X is just some array of values take a random sample from it with the same size this is a very verbose way of doing it and it's just an artifact that I was playing around with different sizes of samples but ended up just using the same length as X and then this just recursively adds to itself until we reach n number of samples with the mean absolute deviation defined and a way to generate samples defined we can Implement Kappa this function is restricted to Kappa values with n not equal to one and we'll just set that equal to the data that we're working with and then SN will be generated from that function we defined in the previous slide then we'll compute the mean absolute deviation for both of these samples and then we'll compute Kappa in just one line of code Now using this definition of Kappa we can compute the metric in a similar way as we did the other humanistics we need to define the number of samples in the Kappa calculation so we're going to compare S100 to S1 so we're just going to grab each data set and then we're going to compute Kappa in one line of code here so the results look like this where medium followers have a Kappa value 0.27 YouTube earning 0.39 and Linkedin impression 0.31 while this might seem to tell us a different story as we've seen with the other heuristics given the implicit Randomness in the Kappa definition namely when we defined s00 we're randomly sampling our empirical data 100 times so if you run that calculation three times you're going to get three different Capa values so instead of just running this calculation once and drawing conclusions from it the better idea to run this calculation many times before drawing your conclusion that's what I do here so I just set number of runs equal to 1,000 I just Define some Kappa dictionary to store Kappa values for each data set here I'm just grabbing one of the data sets I initialize a Kappa list because we're going to run this calculation 1,000 times and every time we run it we'll just append the Kappa value to this list here is where we actually do the Capa calculation so this Loops 1,000 times and then this list will store it in the Kappa dictionary with a key equal to the file name of that data set and then we just do this three times for each of the data sets the results look like this and so again when you run this calculation you're going to get all sorts of different values for Kappa but it'll start to look like a gaussian as you do more and more runs so the mean is a representative value of this distribution here we see the mean Kappa for medium followers is about 0.4 for YouTube earnings it's about 0.3 and for LinkedIn Impressions it's about 0.34 the story we're getting here is that Medium followers is the most fat tailed because again the larger the Kappa the fatter the tail followed by LinkedIn Impressions and Then followed by YouTube earnings based on all these theistic and looking at the plots and stuff it's pretty clear that Medium followers is the most fat tailed of the three distributions with YouTube earnings and Linkedin Impressions being kind of debatable which of the two is more fat t so if you want to dig a bit more into this topic check out the blog published in towards data science even though this is a member only story you'll be able to access the article completely for free even if you're not a medium member using the friend Link in the description below and this is the case for any video and blog combo that I make also check out the GitHub to get access to the data sets I looked at here as well as the example code and if you enjoyed this content please consider subscribing and sharing with others that's a great no cost way to support me in all the videos that I put out and as always thank you so much for your time and thanks for watching"
TyhlSNB5Ko0,2023-12-11T15:23:39.000000,How I’d learn data analytics (if I had to start over in 2024) #dataanalytics,I was going to start from scratch my dad has a business he has a car dealership if I was starting over I would go to him and be like hey is there any data that I can use to solve a problem for you so if I was trying to get into Data analysis I would take his sales data and make a dashboard and this is actually something I did in grad school but you know maybe your your dad doesn't have a business but if you know anyone that has a business or you know anyone that has data that's one Avenue another Avenue there's so many public data sources out there in the US we have the US Census so like working with that API to get that data and then to do some kind of analysis on US Census Data that could be another Avenue
rtUpRMWFu7k,2023-12-06T14:51:18.000000,How to Move Toward Your DREAM LIFE #getpaidtolive,wherever you are is a starting Place yeah maybe you're getting the money at your job but maybe it's not in the right industry or maybe you're not learning the right skills um or maybe it's not the right role that you want to be working in what you can do is like okay I checked one box but I have like three boxes unchecked is it possible for me to find another job maybe in just in the right industry or at the right company or in the right direction so I can just check one more box and then let me do that for some period of time and then let me re-evaluate and see if I can check another box you know this approach to it is just a checklist that you're ultimately just going down and you know you're not going to do it overnight but like over the course of years you can check off everything on your list and you this is your dream list this is your dream life exactly and just slowly just keep going because what else are you going to do in your life if not check everything off this list oh it's it's like your bucket list
8uwHRVaRsmY,2023-12-04T14:56:41.000000,PCA explained in 60 seconds #datascience,I'm going to explain principal component analysis in less than 60 seconds principal component analysis or PCA for short is a technique commonly used in data science and data analytics i' like to explain it through the following analogy so imagine you have the huge rock band talking a super group you got two drums you got the conas you got the background singers you got the saxophone the trumpets the guitars the piano the keyboard the strings and of course the lead singer while some great music can come out of all these players getting together together for the most part there's a lot of redundant music that gets generated from all these players this is where PCA can help us essentially what PCA can do is take all these players all the music that they're creating and translate it to a compressed representation with PCA you can basically play the same song but with less players but this analogy is saying if you have a high-dimensional data set you can use PCA to reduce the dimensionality of your data while still capturing the essence of what is
x5-IW1m3zPo,2023-11-30T23:54:09.000000,Detecting Power Laws in Real-world Data | w/ Python Code,"this is the second video in a larger series on power laws and fat taals in the previous video I gave a beginner-friendly guide to power laws and presented three problems with using our standard statistical tools and analyzing them while in awareness of these problems can help us avoid them in practice it's not always clear whether some data follows a power law or not in this video I will describe how we can detect power laws from Real World data and share example python code for how you can do this in analyzing real world data from my social media accounts and if you're new here welcome I'm Shaw I make videos about data science and Entrepreneurship and if you enjoyed this content please consider subscribing that's a great no cost way you can support me and all the videos that I make the title here is detecting power laws in data a maximum likelihood based approach with python if that doesn't make any sense hopefully it will in just a few minutes just a quick recap of what we talked about in the previous video the central issue that we discussed was that these things called Power laws break many of our favorite statistical tools in the previous video we talked about two general types of distributions the gaan distribution shown on the left hand side here and the power LW distribution shown on the right hand side qualitatively these distributions look very different and it turns out this qualitative difference raised three major problems with using our standard statistical tools in analyzing power laws namely we saw that the mean was meaningless as well as many other standard statistical tools we saw that regression doesn't work so well and then we also saw that payoffs can diverge from probabilities when working with power laws and this all boils down to a single core property of power laws which is they are driven by rare events and this rare event driven property of power laws is more generally described by so-called fat tail Tails if you're unfamiliar with power laws or fat tails check out the previous video because that's going to be a good primer for everything that we discuss here so this rare event driven property of power laws giving rise to these three problems here motivates us to want to detect power laws from Real World data and so one popular way of doing this is what I'll call the log log approach which essentially boils down to plotting a histogram of your data on a log log plot I'm going to walk through the math of where this is coming from from in the previous video we defined the power laws probability density function according to this expression here where P of Z is the probability density function Z is some random variable F of Z is a slowly varying function of our variable and then we have this Alpha index which is the so-called tail index which defines the shape of the long tail of the power law we also saw that power laws are defined according to a minimum value so everything here here has Z greater than some minimum what this log log approach does is you start with this probability density function and then you take the log of both sides so what happens is you can rewrite the log of the PDF as this and it turns out this is a linear model so we can see that more clearly here where the natural log of our probability density function is r y natural log of our random variable is our X and then this term here will be the slope and then this term here will be the Y intercept since the PDF is linear when you take the log and rearrange these things this implies that the histogram when plotted on a log log plot will look something like this you'll have this line essentially with a negative slope as we can see here because Alpha is always going to be positive and then you can take this one step further and actually do a linear regression and estimate values for the slope and Y intercept and then that'll allow you to estimate the Alpha parameter here so while this is a simple way to try to assess out whether your data follows a power law distribution or not there are a handful of limitations with this approach for one the slope that we estimate from the linear regression is prone to systematic errors another is that when fitting a linear model to data that doesn't follow a power law you can sometimes still get a good fit and then one thing that you'll notice here the data that I'm plotting here is truly a power LW distribution that I artificially generate however this line isn't exactly straight which also seems a bit problematic and so there are a few other issues that I won't get into but if you want to learn more check out reference number one where in the appendix they break down a handful of issues with this log log approach and both these references are available in the description below so an alternative to this and the main topic of this video is a maximum likelihood based approach maximum likelihood is a very popular technique in statistics statistics it's a method for inferring the best parameters for a model given some data and this consists of two main steps the first step is to obtain a likelihood function and then the second step is to maximize that likelihood function with respect to our model parameters hence the name maximum likelihood and so I'm going to walk through step by step what this looks like step one is to write the likelihood function and we do this as follows so we have the likelihood of function denoted by L here the arguments of the likelihood function are our model parameters so for a power law we have two parameters the tail index Alpha and then the minimum value for the distribution and the likelihood is defined as the product of the probability density function over all our observations given our model parameters so what this likelihood function will generate is a number which will tell us how likely some choice of model parameters are given our data next we can use a Paro distribution for our probability density function and you'll notice that this takes the form of our power laws probability density function that we defined previously here's the power law term and then l ofx in this case is just a constant which depends on the model parameters plugging this into our likelihood function we get something like this and then doing a little bit of algebra we get this expression here so again for some choice of model parameters we can plug those values in and then take the product over all of our observations to obtain a likelihood of that particular choice of model parameters next what I call step 1B often people will work with the log likelihood function because one it's a little easier to work with and two the log likelihood function and the likelihood function are maximized by the same choice of model parameters so what this looks like is we'll Define the log likelihood function as lowercase L same inputs as the likelihood function and it's just the natural log of the likelihood of function here and then from the previous slide this is the likelihood function here for a Paro distribution and we'll just take the natural log of that using the property of logs we can translate the product within a log function to the sum of log functions and then again using another property of logs we can bring down the exponents to the front which brings us to this expression here and then in step two we maximize the log likelihood function and so the standard way of maximizing or optimizing a function is we take its derivative and set that derivative equal to zero so what that looks like is we'll take the derivative of the log likelihood function with respect to our tail index which gives us an expression like this and then setting this equal to zero implies that Alpha is equal to this expression here and then rearranging this a little bit we get this expression this expression for Alpha it's called the maximum likelihood estimator for that parameter what this allows us to do is given some data and some value for XMen we can derive the optimal value for Alpha that's all the theory stuff let's see what this looks like in code I'll start with an example using artificial data just so we can get a sense of what to expect when we apply it to real world data first we're going to import some helpful libraries namely numpy matplot lib this power Law Library which implements this maximum likelihood based approach that we just reviewed we'll also import pandas and then we'll fix the random seed for numpy just so the results are repeatable to generate the artificial data we can do it like this here I'm generating data following a Paro distribution with Alpha value 2 X-Men value equal 1 generating 1,000 observations this is just an array x with 1,1 values in it and then we can use this function from the numpy library to generate our random sample from a Paro distribution and then similarly we can generate data following a log normal distribution with mean equal to 10 and sigma equal to 1 using again a function from the numpy library we briefly touched on log normal distributions in the previous video what we saw there is that log normal distributions are a little tricky because they can at times appear more gaussian like and then other times appear more like a power law depending on the value of Sigma just to show this graphically we see that a log normal distribution that is thin tailed can look like a gaussian so this is Sigma equal to 0.2 while at the same time a log normal distribution can also appear fat tailed so this is a sigma value equal to two this presents some more difficulties when working with data in the real world it might be hard to discern whether data follows a power law or maybe it just follows a fat tailed log normal distribution and so lucky for us the power Law Library that we're going to be using here has this functionality built into it so that when we do our fit it'll generate parameter estimates for both a power law and a log normal distribution as well as some other distributions it's always a good idea to look at your data whatever data set you're working with so here it's an artificial data set so we have a pretty good idea of what to expect nevertheless here's what the histogram looks like this is just our data plotted in a regular histogram you can see most of the data are here and then there are so few values in the tail you can't even see them in the histogram however when you take the log the tail becomes a bit more visible so you can see that these values start to pop up more and then we can do a similar thing for the log normal distribution qualitatively it can be kind of easy to misinterpret a log normal distribution as a power law distribution because the regular histogram looks very similar however when you take the log of data following a log normal distribution the histogram looks very different than that of the power Lot distribution so this almost looks like a gaussian distribution a lot of times in practice just doing something as simple as plotting histograms and plotting the log of a histogram gives you a pretty good idea of how fat tailed your data are and whether or not it follows a power law distribution however let's see how we can make this a bit more objective and quantitative using the power Law Library we can use the fit method to fit our data generated from a par distribution to a power log with just one line of code and then we can print the results so Alpha is our estimated tail index Xmen is the estimated X-Men value and then p is a quality score it's a number between zero and one that represents how good of a fit the power law distribution is the closer the value is to one the better the power law fit so that's telling us that this is a pretty good fit before comparing to the ground truth values it's important to point out that the power law libraries Alpha definition is different than the standard definition that I've been using throughout this video series so namely the library's Alpha value is equal to the standard Alpha Value Plus one so all we need to do is subtract one from the alpha value generated from the power law fit to make the comparison to the true Alpha value here the true Alpha value we used was two and so subtracting one from the estimate gives us 1.9 so that's a pretty good fit it got pretty close and then the true value of Xmen is 1 and the fit was 1.27 so overall the library did a good job of estimating these parameter values even though we don't have a whole lot of data here okay we can do the same exact thing for the log normal distribution and again we get an alpha value an xmin value an quality score so this should raise some eyebrows again we're getting a very good quality score but this is a log normal distribution why is the log normal distribution described very well by a power law distribution so the thing to point out here is that the X Min value is actually pretty far into the tail looking at this visually here's our histogram of the log normal data and the X-Men value starts about right here so another thing we can do is we can manually fix the xmin value to force the library to fit the distribution to all of the data not just the tail which best fits a power LW so what that looks like is this we can just set this Xmen argument in the fit method and then it'll generate parameter estimates for us of Alpha and Xmen another cool thing about the fit method is that it'll automatically generate estimates of the log normal distribution parameters so without any extra steps we can just plot the MU and sigma estimates for a log normal distribution and we can see that it does a pretty good job at estimating these parameters so comparing to the ground truth of 10 and one the fit method does a pretty good job but of course in practice we don't know what the true parameter values are so just from these results here we wouldn't be able to tell which is a better fit is it going to be the power law fit with these parameters or is it going to be the log normal fit with these parameters so for that we can actually go one step further and compute likelihood ratios between the power law fit and a list of other candidate distributions to give us a sense of which distribution best explains the data so what that looks like is this here I'm just defining a list of different candidate distributions so here we have a log normal exponential truncated power laws stretch exponential log normal positive then I just go through in a for Loop here and use this distribution compare method to compare the power law distribution to each candidate distribution in this list here and then I just print the results what the results look like is an R value and a P value this is different than the P value we saw earlier and this is a P value that we're more used to it is quantifying the significance level of the likelihood ratio R is denoting the likelihood ratio and the way to interpret this is that a positive r value means that the power law distribution is a better fit a negative r value implies that the second distribution is a better fit and then a likelihood ratio of zero means that there's really no difference between the two here in every single case we can see that all the likelihood ratios are negative meaning that all the other candidate distributions are preferred over the power law and we see that the P values are very low they're basically zero in reference number one they use the rule of thumb cut off of 0.1 and this is below that rule of thumb threshold here we see that the likelihood ratio for the log normal distributions has the largest magnitude so we can conclude that the log normal distribution is probably the best fit in this situation and indeed since we know the data was generated from a log normal distribution that it is the correct fit now that we've gone through this whole process with artificial data let's see what this looks like with Messy data from The Real World first we'll just grab this chunk of code cuz we'll use it in a bit here here we're going to be looking at three data sets all coming from my social media accounts we'll be looking at medium followers gained on a month-to-month basis we'll be looking at YouTube earning on a video by video basis and then we'll be looking at LinkedIn Impressions on a day-to-day basis here we just have histograms like we saw before with the artificial data we have the regular histogram on the left and then on the right we have the histogram of the log values it's pretty clear that all of these histograms have pretty fat tails most of the data are sitting in this first bin of the histogram here with a small minority of data in the tals however when we take the log of each value it's more reminiscent of what we saw with the log normal distribution where there is kind of like a typical value that the log of the data tends to be clumped around so based on this we might be thinking the data may be more like a log normal distribution than a power Lot distribution but let's keep going another thing we can do is look at the top five records by percentage what we can see here is that I gained 42% of all of my medium followers in just a single month and so that's the most followers gained in a single month the second place month accounted for about 18% of my medium followers so kind of bringing this together I gained 60% of my medium followers in Just 2 months even though I've been writing on medium for about 3 years now we see a similar thing on YouTube where my number one video by earnings generated 50% of my earnings across all my videos even though I have about 50 videos on YouTube these are very fat tailed distributions which might pull us in the other direction maybe medium followers and YouTube earnings are better explained by the power law and not the log normal distribution but on LinkedIn it's a different story we don't have such a huge distance between the top record and the second and third and so on so this is less fat tailed and I wouldn't be surprised if it's well fit by a log normal distribution based on these numbers and the histogram we saw in the previous slide but let's keep going let's see what the fit looks like the code for this I actually do this all in a for Loop and the full code and notebook is available at the GitHub repository linked here but this is just like a chunk of code from that notebook and it's essentially what we did before on the artificial data set where we're just using this fit method applying it to our data set so I'm doing it in a for Loop so this will kind of iterate between each of the three data sets and then I am manually setting the minimum value the xmin value equal to the smallest value in each data set to forc the fit to use all the data and then from there we can print the power law distribution parameters and the log normal distribution parameters looking at these numbers we see that these Alpha values are very small and again this Alpha value generated from the power law library is one greater than the standard definition of alpha so we actually would need to subtract one from all these values to translate it into the more standard Alpha definition so this would mean that the alpha value is 0.29 0.79 and 0.15 which are super small Alpha values you know typically Alpha values are going to be between 2 and three and if it's really fat tailed it's going to be greater than one but below Alpha equal to 1 as we're seeing in this case here it's what author Nim taleb called the forget about it domain where the mean for the power law distribution is not defined so it becomes very difficult to do any kind of analysis on that data and and just as easily we can get the log normal parameter values one thing that stands out is that the mean for the log normal fit is negative for YouTube earnings so that might be like a red flag that maybe the log normal fit isn't going to be so good for YouTube earnings and just like before we can compute the likelihood ratios to compare the power law fit to that list of other candidate distributions so this is just taking that same chunk of code we saw a couple slides ago and these are the results going through one by one we can see from medium followers the log normal fit is preferred over the power law and it has a significant P value and the magnitude of the likelihood ratio is largest for the log normal fit from this data we would say that the medium followers best follow a log normal distribution for YouTube earnings only one of the ratios is statistically significant and then for that one it's saying the power law distribution is a better fit than the exponential distribution while the other comparisons are inconclusive but kind of based on everything we saw with 50% of earnings coming from one video the negative mean value of the log normal fit I would say that the power law is the best candidate of all the distributions that we're looking at here for the LinkedIn Impressions we actually see something very similar as to what we saw with the artificial log normal data set where all the P values are significant and they're all preferring the non power law distribution So based on this I would say that the LinkedIn Impressions best follow a log normal distribution there's a small caveat here that the medium followers and YouTube earnings data that we're looking at here are a relatively small data set there are less than 100 observations in each and when you're talking about fat tails when you're talking about power laws where you have data driven by rare events small data is a killer because all it takes is one additional data point to completely skew the fits that we're looking at here so all it takes is maybe a couple more observations of like extreme values of medium followers to completely change this from best being described as a log normal fit to a power law fit and conversely maybe as I put more videos out and get more data on earnings I find that the data better follows a log normal fit than a power law fit so we should just take these results with a grain of salt which should always be your mindset as a data scientist skepticism is the default mindset for for a scientist okay so looking ahead to what's next while it is helpful to have a idea of whether some data follows a power law distribution or not this idea of fat tailedness that we described in the previous video is something more General than data following a power law or not following a power law and as we saw before fat tailedness is really on a spectrum from not fat tailed at all to very fat tailed this is why it can be handy to forget about this idea aidea of fitting data to particular distributions and just try to focus on quantifying the fat tailedness of some data set that's what we're going to talk about in the next video of this series through four different heris for quantifying fat tailed in this and then a couple other things I want to call out if you enjoyed this video and you want to dig a Little Deeper check out the blog published in towards data science this kind of goes into a bit more details that I may not have covered here even though this is a member only story you can access it completely for free even if you're not a medium member using the friend Link in the description below and this is the case for any one of my YouTube videos they will all have friend links in them so everyone who is trying to learn this stuff can access the blogs and then finally the code is available on the GitHub repository shown here and Linked In the description below so if you enjoyed this content please consider subscribing that's a great no cost way to support me in all the content that I generate if you have any questions questions or suggestions for future content please drop those in the comments section below and as always thank you so much for your time and thanks for watching"
0oBoHwwJYJQ,2023-11-27T17:29:19.000000,Do NOT become an entrepreneur #entrepreneurship,do not become an entrepreneur if you cannot handle these three things first and foremost failure yeah no one likes failure but not being able to handle failure is a different thing entirely being able to turn failures into success is a key part of Entrepreneurship second thing is uncertainty if you can't handle uncertainty entrepreneurship is going to be even more miserable than it needs to be and finally managing yourself this means being your own manager most of the time what entrepreneurship looks like is you working by yourself so you can't rely on the social structure you might see at a larger organization or Corporation however if you can endure these three things you can turn failures into success you can persevere through uncertainty and you have the discipline to manage yourself and Entrepreneurship might be for you
2Axas1OvafQ,2023-11-20T19:35:35.000000,DON’T study Gen AI #generativeai,don't study generative AI those early in their career will often ask me what's the best degree to get if I want to get into Ai and while I don't think there's a best path that's true for everyone I do think that there are some paths that are more fragile than others rule of thumb that I buy into to is that when it comes to developing a foundational knowledge about something the older the subject the better and this is based on something called the Lindy principle which B basically says that the life expectancy of a subject is proportional to its current age for example math has been around for thousands of years so we can probably expect math to be around for a few more Thousand Years conversely generative AI has been around for about a year so I expect it to stick around for about another year so the moral of the story is if you want to study something that's going to last study something that's been around for a while
Wcqt49dXtm8,2023-11-15T16:06:40.000000,"Pareto, Power Laws, and Fat Tails—what they don’t teach you in STAT 101","statistics is the Bedrock of Science and data analysis this is why we all learn about it in some form or fashion in school however many of our favorite statistical techniques are completely useless when applied to a certain type of data this specific type of data are called Power laws in this video I'll be giving a beginner friendly introduction to power laws and describe three problems that come up when trying to apply our standard statistical tools to analyze them if you're new to the channel I'm Shaw I make content about data science and Entrepreneurship and if you enjoyed this video please consider subscribing that's a great no cost way to support me in all the videos that I make and with that let's get into it so the official title of this talk is Paro power laws and fat tales what they don't teach you in statistics we'll start with the background information I'll talk about the gaussian distribution Fredo's 8020 rule introduce the power lock class class and describe the difference between weight and wealth then I'll move on to three big problems when trying to use traditional statistical approaches to analyze data following a power law distribution and then finally I will introduce the idea of fat tails which generalizes a key property of these power law distributions so many quantities in nature tend to Clump around a typical value one example of this is if you go to a busy coffee shop and measure the weights of all the customers coming in and out of the coffee shop you would eventually observe a pattern like the one shown here so in other words the weights would tend to Clump around some typical value and then Decay rapidly toward these Tails this is a distribution that most people are familiar with it's called a gaussian distribution also called a bell curve and the great thing about data that follows a gausian distribution is that we can capture a lot of the essential information of the underlying data with just a single number which is the mean and you can go even further and capture how spread out this distribution is via measures like the standard deviation and so these concepts of a gaussian the mean the standard deviation variance Etc these are all Concepts that people will learn in an introductory statistics course or a business statistics course and indeed these are powerful techniques for analyzing ing data solving problems and making decisions however not all data that we care about follows a distribution like a gaussian and a great example of this comes from the work of vredo paredo and so many people have probably heard of paro's principle or the 8020 Rule and typically how this is quoted is that 80% of sales come from 20% of customers however this idea did not originate from the business world or sales and marketing it actually originated from the work of an Italian economist IST and mathematician vredo paredo in his study of Italian land ownership where he found that about 80% of the land in Italy was owned by about 20% of the citizens this simple observation is indicative of Statistics that are very different from the gaussian distribution that we saw in the coffee shop and so what this 8020 rule or Paro principle implies is that the underlying data follows a Paro distribution which looks like this this just qualitatively this looks very different than the gaussian distribution from the previous slide and the biggest difference here is that there's no typical value around which the data is clumped so in the case of a gaussian the mean is very representative of the overall distribution however when looking at a Paro distribution the mean doesn't give you a whole lot of information so in this case the mean is going to be somewhere around here which doesn't tell you much about a lot of the dat data that's living in the so-called tail over here putting this another way while knowing the average weight of an Italian man gives you a good idea of what to expect on your next trip to Rome knowing the average population of an Italian city which is about 7500 is completely useless in grounding your expectations and the reason for this is that weight tends to follow a giian distribution while city populations tend to follow a parade of distribution so the parade distribution is actually part of a broader class of distributions called Power laws and so here are a few different Power laws in red we actually see a power law matching this 8020 rule like we saw in the previous slide making this a bit more General a power law is defined by this equation here so PDF is the probability density function X is a random variable Little X is some specific value of that random variable L of X is some slowly varying function and then Alpha is is just some number which defines the shape of the distribution here and another important note is that power laws are only defined Beyond a minimum value so in these plots here the minimum value is one but this value could be anything these two types of distributions the gaussian like distributions and now these like parol like power law distributions they give us these two conceptual anchors by which we can qualitatively categorize data that we observe in the the real world author Nim Nicholas TB in his book The Black Swan defines these two categories as mediocre Stan and extremist where mediocris are the gaussian like data while extremist are the Paro like data and so the key property of data from mediocris is that no single observation will significantly impact the aggregate statistics to see an example of this suppose on your trip to Rome you go visit the Coliseum and then again you have your scale with you and you decide to start weighing random strangers at the Coliseum so let's say you weigh a th000 people at the Coliseum and compute the average and it turns out to be 175 lb then suppose you add to this 1,000 person sample the heaviest Italian that you can find and so if you do this this will have very little impact on the mean the average might go from 175 lb to 175.2 lb and this is the key property of data from mediocre Stan which is again that no single observation will significantly impact the aggregate statistics there's going to be no person on Earth that you can add to this sample that will dramatically change the mean of the weight distribution however data from extremist on is different in this case a single observation can and often will drive the aggregate statistics so let's say instead of weighing people at the Coliseum you ask them what their net worth is again you get that same sample of 1,000 people and you compute their mean net worth and you find it to be about $300,000 and then let's say you add the richest Italian to the sample what's going to happen here is that the average net worth is going to go from about $300,000 to $7.5 million so about a 25x increase in the average from just a single observation and so that's the key property of data from extremist on and data following a Paro like distribution to get a bit more intuition about this here are some more examples from mediocris Stan and extremist respectively gaussian like data will be things like IQ weight height calorie consumption test scores car accidents mortality rates blood pressure on the other side data from extremist on will be things like wealth as we saw at the Coliseum sales as people talk about with the 8020 rule in business city populations which we mentioned earlier pandemics deaths in wars and terrorist attacks word occurrences and text a very small number of words will be used the most amount of times academic citations a very small number of researchers get the bulk of the citations and Company sizes there are very few number of companies that employ most of the world's Workforce as you can see the things that live an extremists on isn't some trivial set of things in fact you could argue that most of the things that we care about as a society and civilization are not gaussian likee at all while this may seem just like splitting hairs some like technical exercise of categorizing data as gaussian like or par like it turns out there are major limitations to our standard statistical Tools in analyzing data from extremist on and so here I'll highlight three such problems with using our so-called stat 101 techniques to try to analyze these quantities that we care about and so this all boils down to one thing the law of large numbers which basically says if we take n random samples the sample mean will approach the true mean as the number of samples goes to Infinity put another way if we start collecting data generated from a gaussian distribution as we collect more and more samples more and more observations the average that we compute from our sample will approach the true average of the underlying distribution this is also true for the Poo distribution and a uniform distribution and a log normal distribution any distribution that has a finite mean the law of large numbers is true however in practice we never have infinite data we can only have a certain number of observations and this results in some complications with the law of large numbers assumption if we take 10 observations we'll get a pretty accurate sample mean of a gaussian distribution however if we take 10 observations of something generated from a Paro distribution the sample mean is going to be biased this is all because the the law of large numbers Works more slowly for power laws than gaussian distributions which brings us to our first problem the mean is meaningless as well as many other metrics when it comes to working with finite sample sizes of data that follows a power law distribution is that it takes much longer for the mean to converge to the true value compared to a gaussian so we can see this from the plots shown here so on the left we have the number of samples on the x-axis and then on the y- AIS we have the sample mean so this black line here is the true mean and then the blue line is the mean that we compute when the data is generated from a gaussian while this orange line here is the mean that we compute when the data is generated from a Paro distribution as you can see the gussian is never too far off from the True Value you know maybe in the super small sample sizes you have a biased mean but pretty quickly it starts to get really close to the True Value however for the power lot we can see the sample mean is not only much more biased than the gaussian but it's also much more erratic and this extends to not just small sample sizes like 100 observations but to a th000 observations and even 10,000 observations this whole time the Paro sample me is much more erratic than the gaussian and much more biased this even extends to when we 10x the sample size even more to a 100,000 observations at this point the gaussian is right on the money the mean isn't changing at all with additional observations however with the power law the mean is still wiggling around and not quite the True Value and so we're seeing bias at 100,000 observations for the power law similar to what we were seeing at about 10 observations for the gaussian but this isn't limited to just the mean we see this for many other standard statistical quantities that's what's being shown here on the left hand side of these plots we have the respective quantities so we have the median the standard deviation the variance the mean the max first percentile the 99th percentile ptosis and entropy and then horizontally oriented we have 100 samples the th sample case and the 10,000 sample case so while some of these quantities are relatively stable like the median once you get to sufficient sample size it tends to level out the minimum value even in small sample size it's pretty accurate and the first percentile in small sample size is pretty accurate and stable some of these other quantities can't seem to land on a particular value so namely standard deviation variance the maximum the 99th percentile to some extent curtosis and then entropy seems to continually be changing Without End so the one quantity I want to highlight here is the maximum and that's because given this property that rare events Drive the statistics of power LW distributions as sample size increases we see a order of magnitude increase in the maximum value when we go from a th000 samples to 10,000 samples the danger here is that you could have a maximum value that seems stable in a relatively small sample size let's say you have 7,000 observations and the max value seems to have plateaued and it seems pretty stable but then as you collect more data you have this huge jump in the max value and so the danger here is that you can be in this period where it seems like things are stable and predictable but then all of a sudden you have this huge change in the data that you're observing so to connect this to the real world if this data were say deaths from a pandemic what this might look like is the deadliest pandemic in a 100-year time span will be in order of magnitude less severe than a pandemic in a Thousand-Year time span the deadliest pandemic in the past 100 years was the Spanish Flu which killed about 50 million people and we might think okay that was the deadliest pandemic it's not going to get any worse than that if the data is following a power law we can't be surprised if over a Thousand-Year time period the deadliest pandemic claims 500 million victims so this is highlighting this key property of data from extremist on which is that rare events Drive the underlying statistics however this doesn't stop with the mean and all the other standard statistical quantities that we see here it also impacts our ability to make predictions effectively which brings us to problem two regression doesn't work so what regression boils down to is predicting future events from past data and intuitively if your data is driven by rare events you may simply just not have enough past observations to make good predictions about the future and this problem is exacerbated when working with power law distributions so let's look at a particular example let's suppose that we want to do linear regression between the variable X and Y here x is a normally distributed random variable m and b are the parameters that we're trying to learn and e is a noise term that follows a power law distribution so one case where regression just completely breaks down is when this noise term has an alpha value that tail index we saw earlier when we defined power laws is less than or equal to two because in this case the power law has infinite variance so the variance of this noise term is going to be infinity and it turns out if the variance of this noise term is infinite then the variance of this whole equation will be infinite which makes the R 2 value go to zero there's a quick derivation of this in citation number two Linked In the description below in chapter 6.7 but of course you can't observe infinite variance in practice because your data is necessarily finite so what's going to happen when doing regression in practice is going to going be similar to what we saw before with the max value where the results might seem stable in small sample size but then break down as more data are collected we can see this through an example taking our normally distributed random variable with the added power law noise term and doing a linear regression with a 100 samples the results of our regression might look like this which looks pretty good you know maybe there's some outliers here but overall we get a pretty good fit and the r squ isn't bad however this is incorrect correct because the noise term has infinite variance which means r s should actually be zero in this case and indeed as we collect more and more data we can see the R squ value quickly deteriorating so we go from 100 samples to a th000 to 10,000 to 100,000 to a million to 10 million to 100 million and so on this is the danger of doing regression with data that follows a power LW your results might look deceivingly well in small sample size but then as you collect more data your model performance quickly deteriorates but at this point you might say Shaw what's the big deal you know so what if our model can't predict some super rare events like these like 1 in a th000 one in 10,000 Etc events the model can predict 99% of things pretty well why do we care about these super rare events and I agree with you when data are generated from a power law it's not hard to be right most of the time because most of the data do not live in this long t of the power law however when solving problems and making decisions in the real world probabilities are only half of the story the other half of the story are payoffs which brings us to problem number three payoffs diverge from probabilities in other words it's not just about how often you are right or wrong but also what happens when you're right or wrong so let's see what this might look like in a business context consider a software company with three key offerings offer one is they have a free software that has ads they have a premium offer which it's no ads with some monthly subscription and then they have a third offer which is a Enterprise level software with different customizations and add-ons and whatever those clients need and let's say that the 8020 rule is in play so 80% of sales comes from 20% of customers what this might look like is that 80% of customers go with offer one they just use the free version 16% of customers use the premium version and then 4% of clients are the Enterprise clients what this means for revenue is that 20% of the revenue comes from the free users 16% of the revenue comes from the premium users and 64% of the revenue most of the revenue comes from the Enterprise customers so let's say the software company wants to optimize the core service making it run 25% more efficiently and as any good company might do they're not just going to roll this out blindly they're going to ask the customers first they're going to ask their customers you like this update is this something that you need so they do a survey and they find that 95% of the customers like the update 4% of the customers don't really care and 1% of the customers said the update was bad seeing that the overwhelming majority of the customers like the update the company decides to move forward with the update but now fast forward 6 weeks and the company notices a 50% drop in Revenue so what happened it turns out that the company's three biggest clients dropped the service because the software update killed some Legacy data Integrations that were critical to their business while this is just like a madeup artificial example it's meant to illustrate the point that in extremist being wrong one time can erase the gains of being right 99 times and even Beyond if 1% of your customers are driving 50% of your Revenue that means that you can do something that 99% of your customers love and 1% of your customers hate and be much worse off and so now we're going to talk about about fat tales there has been a bit of controversy in extremist on an example of this is Illustrated around wealth going back to Paro this idea that 80% of the land is owned by 20% of the citizens has kind of been applied throughout economics with the prevailing sentiment being that wealth follows a parol like distribution so maybe you've heard something like this when it comes to income inequality where it's like the top 1% has like a third of the wealth or something like that but there's a bit of contr I around whether wealth truly follows a Paro distribution or power law distribution or not so the story goes something like this I'll summarize wealth distribution via the mean and standard deviation but of course if wealth is following this power law the mean and standard deviation are going to be useless because these are parameters for a gaussian distribution not so much helpful for a power Lot distribution so someone will say that's useless because wealth follows a power law but then you have someone else that's saying actually wealth fits a log normal distribtion tion better and then you'll have someone else that says Well Log normal behaves like a power log distribution for high Sigma so this kind of summarizes the controversy here and to just avoid this altogether instead of trying to say does some particular data set follow some particular distribution we can instead focus on fat tails this idea of fat tailedness we can Define as the degree to Which rare events Drive the aggregate statistics of the distribution so this Maps directly onto what we were talking about before with mediocris Stan and extremist where in mediocris Stan rare events do not drive the aggregate statistics while in extremist they do to kind of connect this to different distributions we have a sort of map of mediocris and extremist here so on the far left we have the Gan distribution that we all know and love and then more generally we can call these like student te distributions on the right hand side in extremist da we have the power law distributions that we've been discussing but then we have this land in between and we can Define this as the subexponential domain so an example subexponential distribution is the log normal distribution so we can see for low Sigma it kind of looks like a gaussian but for high Sigma it kind of looks like the Paro distribution and we can kind of index different Power lot distributions according to this Alpha parameter so if Alpha is greater than or equal to two the distribution has finite mean and variance which allows us to do some productive statistics with it if the alpha value is between 1 and two it has finite mean but infinite variance so now regression blows up but at least we have a mean we can work with however when the alpha value is below one the mean is infinite and this is what author Nim TB calls the forget about it domain you can't really do much when the power law has a tail as fat as this as you can see the space between mediocris and extremist ston between gaussian distributions and power law distributions is really a spectrum so instead of thinking of this as like a binary thing as like fat tailed or not this is really a quantity that lives on a spectrum from not very fat tailed to very fat tailed while there's no like true way to quantify fat tailedness there are a few heris that we can employ and so here's some ideas the first one is power Latin and we kind of saw this on the right hand side of that image in the previous slide where as the alpha parameter of the power law got smaller and smaller the tail got fatter and fatter so we can use this tail index to kind of quantify how fat the tails are in other words the lower the alpha value the fatter of the Tails and this is kind of demonstrated in this plot here on the other side instead of thinking of it as like power law we can think of it as like non gaussian there are measures for non-gaussianity the most popular being curtosis however the problem with curtosis is that it breaks down when the alpha value is less than or equal to four because it has infinite curtosis another idea is to use the variance of the log normal distribution and this kind of goes from what we saw in the previous slide where for low Sigma log normal distribution looks gaussian but for high Sigma it looks like a power LW so if you have a log normal distribution you can look at the variance to quantify the fat tailedness and then finally TB defines this Kappa metric which generalizes to any type of distribution where lower values have thin Tails or don't have fat tails and large values have fat tails and Kappa has a max value of one so if you want to learn more about that he talks about in reference number six Linked In the description below so that was a ton of information but to try to boil everything down when it comes to data that follows a power law distribution to Fat tailed data the central problem that comes up in practice is insufficient sample size essentially we don't have enough data to truly capture the underlying statistics to cope with this fact I want to leave the data practitioner with a few key takeaways that I like to think about when navigating these types of problems so first and foremost is to plot distributions plot histograms plot PDFs plot cdfs to get an impression of how fat tailed the data might be just kind of visually another takeaway is to ask yourself is this data from mediocris or extremist or somewhere in between maybe turning to some of those heris in the previous slide to try to quantify the fat tailedness another key take away is ask yourself what's the value of a correct prediction but just as importantly what is the cost of an incorrect prediction and then finally if working with fat tailed data don't ignore rare events don't just chop off outliers if 50% of your Revenue comes from 1% of your clients instead of this being something detrimental to your analytics figure out how you can come up with efficient interventions in that 1% to drive even more business and then a couple things I want to call out is if you enjoyed this video and you want to learn more check out the blog published in towards data science Linked In the description below there I cover a bit more details that I may not have covered in the video here all the code to generate the plots that I showed here are available on the GitHub repository linked here and if you enjoyed this content please consider subscribing to the channel that's a great no cost way to support me and the content that I generate and as always thank you so much for your time and thanks for watching"
JgWV1skSpEc,2023-10-25T16:44:10.000000,I Spent $716.46 Talking to Data Scientists on Upwork—Here’s what I learned.,"a few months ago I interviewed 10 top data science Freelancers on upwork and made a video summarizing my key learnings while this might sound like a very expensive way for me to learn I found it to be an unreasonably effective way to accelerate my entrepreneurial Journey it allowed me to fast forward Time by picking up years of hard- learn experiences through just a few hours of conversation this whole experience is summarized well by a quote from Benjamin Franklin who said for the best return on your investment pour your purse into your head to make this a bit more concrete I want to shed more light on some of the upsides that I've realized since the first round of interviews data science skills are highly valued it's not uncommon for experienced Freelancers to charge anywhere from $75 to $150 per hour and even more specialized Freelancers may even charge anywhere from $200 to $300 an hour this is why any tactical tip that can make you a more effective data science freelancer can quickly translate to a tremendous amount of value however all the lessons and knowledge that I gained from these interviews was not the only upside here are four other benefits that were a bit unexpected the first is that I've maintained relationships with many of the Freelancers that I spoke to which has been an enormous resource and support for me as someone who relies on data Consulting as their main source of income second many of the Freelancers I talked to ended up joining a community that I run called the data entrepreneurs and have shared their expertise through community events and workshops third one of the Freelancers I spoke to actually connected me with a Consulting opportunity which generated about $900 in revenue and then fourth and finally the blog that I wrote summarizing my key learnings from the first round of interviews has generated ated $472 in earnings as of making this video so even these lessons and connections and relationships put to the side my first round of conversations generated about $1,400 in Revenue which is more than double what I spend on the calls so that's why it was a super easy decision to get back on upwork and have a second round of interviews with top data science Freelancers however a key difference in the second round of interviews is I went for quality over quantity which basically means I spent more money to talk to less people more specifically I spent about $700 talking to four top Freelancers while many of the takeaways I talked about in my first video were reinforced in the second round of conversation new points were raised and nuances of past takeaways were revealed in this video I'll summarize these key points and nuances and if you find Value in this content please consider subscribing to the channel that's a a great no cost way to support me on this entrepreneurial journey and all the content that I generate from it so one of the key questions I asked in the second round of interviews was what's the number one reason Freelancers fail I find this question helpful because often success isn't just about doing things right but not doing things wrong this brought up a wide range of responses from the Freelancers which I'll summarize through three key points the first point is misalignment one of the the biggest challenges in data freelancing is a poorly defined business problem or project scope this often leads to miscommunications and a lot of times project failures this seem to especially be a risk when working with non-technical clients in other words working with clients that have little to know experience of data science and AI the most common strategy for navigating opportunities with poorly defined business problems is simply p passing on the opportunity while this is definitely a judgment call that depends on the details of the opportunity this does seem to be a common red flag that successful data Freelancers tend to avoid the second point is that Freelancers fail because they commit too early in other words those who are new to freelancing may feel compelled to give a number too early which means they might commit to something before they fully understand what the desired outcome is or before they fully understand what it'll take to get there one freelancer recommended the following line when clients press for a commitment prematurely they would simply say I do not commit to something I cannot do and the third reason that new data Freelancers will fail is because of unrealistic expectations while freelancing in data science comes with Incredible freedom and income opportunities it is not easy especially early on those who are new to freelancing and expect too much too quickly set themselves up for failure in other words they come in with super high expectations which sets them up for disappointment and giving up too early this reinforced a key Insight from the first round of interviews which was new Freelancers should focus more on repetition and reviews than money one of the key takeaways from the first round of interviews was to find a niche niching is powerful because it gives a freelancer services greater Clarity for prospective clients and it allows them to charge premium prices for their specialized expertise however in this second round of interviews some nuances of niching were brought up one freelancer advised the following don't pige and hole yourself into a single Tech stack or solution the more adaptable you are the more valuable you become in a freelance capacity another freelancer shared a similar sentiment and said a diversified Consulting business is more robust what this all boils down to is that niching comes with an inherent risk it works great as long as there's demand for that specific service or expertise however if that specialization becomes irrelevant niching can be catastrophic just ask any former Blockbuster executive while you might be confused now and ask wait Shaw do I Niche do I not Niche what am I supposed to do here's my takeaway from these conversations Niche to differentiate yourself but don't lose sight of other opport unities to expand your Consulting business another key takeaway from the previous video was to form alliances across the whole Tech stack the central reason for this is that data science skills can be limited in their business impact and value in other words it doesn't matter how good your r squar or Au is if you can't deploy Your solution into the real world this is why Freelancers from both round one and round two advise me not to only form alliances across the Tex stack but to learn it for myself and I didn't fully appreciate this point until this second round of interviews and heard it reinforced over and over again part of the reason I didn't fully see this is that learning the full Tex St is a tall order these days the Tex St involves data engineering data analysis data science ml engineering and Beyond which are all their own specializations and I definitely had a bit of apprehension and disbelief that one person could be an expert in everything however one freelancer shared good perspective which changed how I looked at the situation they said you don't need to learn everything you just need to learn enough to containerize your script in other words you don't need to be an expert in everything you just need to know enough to get the job done to make things a bit more concrete that same freelancer shared their specific text act with me which is as follows for most things they used AWS and for architecting the data backend they used RDS which is a way to implement a postgrad database and and S3 buckets for building a data pipeline they use tools like ECS ECR kubernetes airite Docker and R modules for building out computational infrastructure they use terraform for writing data science and data analytics code they would use R and for making web apps they used something called R shiny and while I'm not an R developer seeing this concrete example of a full Tex stack was super helpful to me and made this idea of being a full stack data scientist much more accessible the question that I've asked every single freelancer that I've interviewed is where is this going for those who were interested in scaling up their Consulting business two general paths seem to emerge which mirrored the two general career paths that I saw doing data science at a large Enterprise path one is the manager or leadership route this involves less technical work and more people work on the other side we have path 2 which Which is less people work and more technical work both of these paths can be super rewarding and typically come with increasing compensation what I realize through these conversations is that there are two very similar paths in data freelancing and Entrepreneurship so the freelancer version of path one was embodied by one of the people I interviewed their ultimate goal was to continue scaling their Consulting business into an agency where many consultants served many clients while some level of of technical expertise is required to be successful at this scaling this way relies much more on one's business Acumen managerial experience and communication skills conversely we have the freelancer version of path 2 which was embodied by another person I spoke with they had actually tried path one and realized it wasn't for them their preference was to continue doing the technical work and not having to worry about employees subcontractors and all the fixed costs that are associated with scaling a business business like that while scaling path 1 might seem obvious more clients means more money the way path 2 scales here is one simply increases their hourly rate and returning back to the idea of niching in freelance an interesting observation was the freelancer on path one was much more aligned with not committing to a niche and going after client demands while the freelancer on path 2 had found a strong Niche to operate in which allowed them to charge an hourly rate of $200 an hour however for many Freelancers including myself the long-term goal isn't to scale the Consulting business but rather build a product focused business this was the sentiment shared by the two other Freelancers I spoke to while I've received mixed advice from successful Founders on whether freelancing is an optimal path to product development it does check two important boxes for those trying to launch a product the first is flexibility freelancing allows one to turn up or turn down their workload load to accommodate for product development time the second box is that it generates immediate cash flow in other words freelance is a straightforward way entrepreneurs can translate their skills Into Cash with that being said one freelancer and former founder did warn me that freelancing can easily become a treadmill meaning that one can get so caught up in the cycle of Consulting of marketing closing contracts executing Services Etc that they don't end up building that product and long-term Equity this is why they recommended that I reserve time to take a step back and think strategically about how I spend my time and attention to wrap things up here I want to highlight three key takeaways to add to those from my previous video the first takeaway is that Clarity of scope is the most important thing when assessing a freelance opportunity don't commit to any opportunity until you have Clarity this will help avoid many of the challenges that arise in freelance work and help ensure that that the project provides value to both sides the second key takeaway is to never stop learning this goes for both the technical skills and the non-technical skills data science freelancing is unique because it involves ever evolving technology and perishable skills such as communication and negotiation which makes continual learning a requirement to be successful in this field and the third and final takeaway is to find a niche but always have back doors don't Niche yourself out of a job and don't try to be everything to everyone find that right balance that matches your goals and what the market needs if you got value from this content please consider subscribing to the channel that's a great no cost way of supporting me and the content that I generate if you have any questions or insights of your own as a data freelancer drop those in the comment section below and as always thank you so much for your time and thanks for watching"
Xn_Zw6KSxYU,2023-10-18T16:04:24.000000,I Have 90 Days to Make $10k/mo—Here's my plan,"3 months ago I left my sixf fig data science job to pursue entrepreneurship full-time since I have a video all about why I left that role I won't get into that however here I will talk about how it's going so far and my plan for the next 3 months for how I'm going to get back to my full-time salary but now as an entrepreneur this journey started for me on July 14th 2023 which was my last day as a data scientist at Toyota when I walked out the door I had my SES on three main goals which mapped to the three key pillars of my business Consulting my community called the data entrepreneurs and content my first goal for Consulting was to land one single client that came through an inbound lead so essentially a client found me through a Blog that I wrote or a YouTube video that I made and reached out to me because they wanted to work together on some data science project the second was to get 100 people in attendance across all Q3 events in the data entrepreneurs Community third and finally was to generate $11,000 in one month from my medium blogs and of these three goals I only managed to hit one of them which was Land one client that came through as an inbound lead ironically this was the one that I was most worried about because the relationship between content and Consulting clients is a very unpredictable one while I had always known that content was a great way to generate leads in land clients it was all theoretical until I finally landed that first contract and what surprised me about many of my inbound leads is that they did not come from my most popular content in fact the client that I'm working with currently found me through a Blog that I wrote about topological data analysis which is not only an esoteric data science technique but is not even an approach for using in the project that we're working on together however the most significant takeaway of this for me is seeing this idea of cont content turning into contracts become a reality which gave me a boost of confidence and a signal that I'm on the right path as for the second goal of getting 100 people in attendance across all Q3 events I fell short of this goal while we only had 62 people in attendance I still see this past quarter for the community to be a success one reason for that is this goal of 100 attendees was very much a stretch goal and A good rule of thumb when you're dealing with these types of stretch goals is that even getting 60 to 70% completion can be considered a success this goal accomplished a higher level objective of growing the community a few metrics that reflect that are the Discord Community almost doubled in membership our email and newsletter list almost quadrupled and then our YouTube subscribers increased by 42x so we went from five subscribers on YouTube to about 210 and at this point we're even past that so this failure or success however you want to look at it really inspired ired a new direction for the goals in Q4 which I'll touch on a little later in the video and the final goal of generating $1,000 in earnings in one month for my medium blog turned out to be a bit of a roller coaster ride which I feel is very representative of my life as an entrepreneur medium is a blogging website which I first discovered through technical articles coming from towards data science I've been writing on medium for almost 3 years now putting out content about data science entrepreneurship and other things that I find interesting and if you've been following me on YouTube you probably notice that most of my YouTube videos have an Associated blog with them in June 2023 I made almost $500 writing on medium and the way this works is that anytime a medium subscriber reads one of my blogs medium pays me a portion of their membership fee coming into July 2023 my rationale was since I won't be committing 40 hours of my week to my full-time job I'll have more time to make more blogs on medium so if I double the amount of blogs that I write right I should be able to double my earnings this sounded like a good idea until in August something unexpected happened medium changed their monetization structure which took my monthly earnings from $500 all the way down to $200 even though I was putting out more content on medium and so by the end of August I had basically given up hope that I was going to hit this $11,000 goal but I still move forward with the plan of making twice as much content on medium then something unexpected happened yet again my blog series all about large language models started getting a lot of traction in the final 2 weeks of September and generated about $800 in earnings which brought me $20 Within Reach of my $1,000 goal but of course $980 is less than 1,000 so I technically did not hit this goal my main takeaway from this experience is that entrepreneurship is a wild ride and what I find helpful in the face of all this uncertainty and volatility is to find the work intrinsically rewarding and for me making content isn't about making money while that is a nice upside to it the main benefit that I get from it is that it gives me a way to structure and continue my learning which is super important in a space that is as rapidly evolving as data science and AI That's How The First 3 months went as a full-time entrepreneur now here's my plan for the next 3 months I again set three main goals for the quarter corresponding to each key pillar of my business and if I manage to hit all these goals I'll be in a place where my income will match what I made working full-time as a data scientist which was a little more than $10,000 a month I'll go through these goals one by one and talk more about why that goal is important and how I'm going to achieve it so starting with the first landing 20 hours a week of client work going into q1 the reason that this goal is important is that the overwhelming majority of my Revenue comes through Consulting this is about 90% or more of my business's revenue is from Consulting engagements My Hope Is I can hit this goal of 20 hours a week of work relying solely on inbound leads coming from my YouTube videos blog posts and other content that I put out there however given the unpredictable nature of content this cannot be my only strategy that's why if things aren't looking great halfway through the quarter I'll start an outbound campaign to try to get more clients what this could look like is reaching out to other Freelancers in my network to see if they know of any prospective clients this could be cold emailing or cold dming local businesses and finally applying to contracts on upwork the next goal is to put on 9 to 12 community events and this is about double the number of events that we did in Q3 and the idea is if we double the number of events we'll also double the number of attendees and continue to grow all the different Community channels the reason growing the community is important is because the value of the community scales with the number of members so the more people in the community the more people you can learn from the more people you can collaborate with the more jobs are listed in our job board the more projects are listed in our project board the more people showing up to networking events and all the other amazing things that can happen when you're interacting with like-minded people who are trying to go to the same place as you and while the community doesn't generate any Revenue directly it actually costs me some money and a lot of time it provides me with tremendous value in other ways and hopefully it does the same for other people too three of the biggest benefits I found through running this community are as follows first and foremost is learning you can learn a tremendous amount from someone that's just a few steps ahead of you I've learned a tremendous amount about freelancing and Consulting from data scientists who have been doing this a bit longer than me and on the flip side passing along lessons to those who are a few steps behind you in a sense I find is intrinsically rewarding and helps me solidify my my understanding of these Concepts because a lot of the lessons of Entrepreneurship you're not going to find in the textbook anywhere and I find this type of information is best learned by talking directly to practitioners the second big benefit for me of this community is support in other words sometimes it's great to just have someone to tell you you're not crazy the entrepreneurs journey is definitely not the norm which often leads to entrepreneurs being constantly misunderstood by friends family and others however that one stimulating ation with someone that gets it with someone that is on a similar journey to you makes up for the 10 conversations with people that don't understand what you're doing and the third big benefit is alignment and collaboration in other words when you and the person next to you are going to the same place that alignment naturally generates opportunities for collaboration that takes you both further even faster than you could have gone alone and the third big goal for this quarter is to keep up the content more specifically what that means is posting two blogs a week or two videos a week or one blog in one video a week posting three to five times a week on LinkedIn and posting one to two times a week on Instagram and Tik Tok while content generates some revenue for me so about $1,000 a month at this point the Main Financial upside of making content as a data scientist comes from the inbound leads that it generates for me but of course the leads aren't the main benefit of making content like I said before the key benefit of making content for me is that it gives me a way to structure my learning and keep up with the rapidly evolving space of data science and AI not only does content force me to read articles watch YouTube videos write example code do projects using whatever new technique that I'm learning but it also forces me to synthesize these learnings into a narrative which gives me a tremendous amount of clarity and understanding of these topics there's one last thing that has given me a lot more clarity and a good perspective for looking at my goals for this quarter often times with the uncertainty and volatility of Entrepreneurship things can get hard and it can be easy for me to lose perspective of why am I doing this why don't I just go back to a full-time job where I don't have to worry as much I don't have to stress as much the income is guaranteed why put myself through all this trouble and so what helps me in keeping a positive mindset is looking at all these goals through the lens of learning and giving these two aspects of learning and giving makes it easier to maintain that positive mindset which could be elusive in the ups and downs of Entrepreneurship and so when things get hard and when things get uncomfortable I can just remind myself that it's hard because I'm learning it's hard because it's new and then when I'm putting in a bunch of work for the community or I'm putting in a lot of work for content and maybe it feels like it's not worth it reminding myself that you're helping someone you're helping someone understand a complicated subject you're helping someone level up their data science skills you're giving someone the opportunity to speak to the community about their passions about their expertise you're giving a freelance client access to AI giving them the opportunity to leverage these new technologies in their business and I found that to be kind of like a superpower you know if you're just doing something for yourself it's easy to give up but if you're doing something for other people it makes it easier to stay motivated and engaged in the work that you're doing and so I'm leveraging both of these mindsets learning and giving to help me stay engaged stay productive and stay motivated this quarter if you have any specific questions about my decision- making for transitioning into entrepreneurship or how it's been so far or any questions on why I set my Q4 goals as I did please drop those in the comment section below I'm happy to share anything I've picked up along the way and as always thank you so much for your time and thanks for watching"
ZLbVdvOoTKM,2023-10-05T19:01:23.000000,How to Build an LLM from Scratch | An Overview,"hey everyone I'm Shaw and this is the sixth video in the larger series on how to use large language models in practice in this video I'm going to review key aspects and considerations for building a large language model from scratch if you Googled this topic even just one year ago you'd probably see something very different than we see today building large language models was a very esoteric and specialized activity reserved mainly for Cutting Edge AI research but today if you Google how to build an llm from scratch or should I build a large language model you'll see a much different story with all the excitement surrounding large language models post chat GPT we now have an environment where a lot of businesses and Enterprises and other organizations have an interest in building these models perhaps one of the most notable examples comes from Bloomberg in Bloomberg GPT which is a large language model that was specifically built to handle tasks in the space of Finance however the way I see it building a large language model from scratch is often not necessary for the vast majority of llm use cases using something like prompt engineering or fine-tuning in existing model is going to be much better suited than building a large language model from scratch with that being said it is valuable to better understand what it takes to build one of these models from scratch and when it might make sense to do it before diving into the technical aspects of building a large language model let's do some back the napkin math to get a sense of the financial costs that we're talking about here taking as a baseline llama 2 the relatively recent large language model put out by meta these were the computational costs associated with the 7 billion parameter version and 70 billion parameter versions of the model so you can see for llama 27b it took about 180,000 th000 GPU hours to train that model while for 70b a model 10 times as large it required 10 times as much compute so this required 1.7 million GPU hours so if we just do what physicists love to do we can just take orders of magnitude and based on the Llama 2 numbers we'll say a 10 billion parameter model takes on the order of 100,000 GPU hours to train while 100 billion parameter model takes about a million GPU hours to train so how can we trans at this into a dollar amount here we have two options option one is we can rent the gpus and compute that we need to train our model via any of the big cloud providers out there a Nvidia a100 what was used to train llama 2 is going to be on the order of $1 to $2 per GPU per hour so just doing some simple multiplication here that means the 10 billion parameter model is going to be on the order of1 15 $50,000 just to train and the 100 billion parameter model will be on the order of $1.5 million to train alternatively instead of renting the compute you can always buy the hardware in that case we just have to take into consideration the price of these gpus so let's say an a100 is about $110,000 and you want to form a GPU cluster which is about 1,000 gpus the hardware costs alone are going to be on the order of like $10 million but that's not the only cost when you're running a cluster like this for weeks it consumes a tremendous amount of energy and so you also have to take into account the energy cost so let's say training a 100 billion parameter model consumes about 1,000 megawatt hours of energy and let's just say the price of energy is about $100 per megawatt hour then that means the marginal cost of training a 100 billion parameter model is going to be on the order of $100,000 okay so now that you've realized you probably won't be training a large language model anytime soon or maybe you are I don't know let's dive into the technical aspects of building one of these models I'm going to break the process down into four steps one is data curation two is the model architecture three is training the model at scale and four is evaluating the model okay so starting with data curation I would assert that this is the most important and perhaps most time consuming part of the process and this comes from the basic principle of machine learning of garbage in garbage out put another way the quality of your model is driven by the quality of your data so it's super important that you get the training data right especially if you're going to be investing millions of dollars in this model but this presents a problem large language models require large training data sets and so just to get a sense of this gpt3 was trained on half a trillion tokens llama 2 was trained on two trillion tokens and the more recent Falcon 180b was trained on 3.5 trillion tokens and if you're not familiar with tokens you can check out the previous video in the series where I talk more about what tokens are and why they're important but here we can say that as far as training data go we're talking about a trillion words of text or in other words about a million novels or a billion news articles so we're talking about a tremendous amount of data going through a trillion words of text and ensuring data quality is a tremendous effort and undertaking and so a natural question is where do we even get all this text the most common place is the internet the internet consist of web pages Wikipedia forums books scientific articles code bases you name it post J GPT there's a lot more controversy around this and copyright laws the risk with web scraping yourself is that you might grab data that you're not supposed to grab or you don't have the rights to grab and then using it in a model for potentially commercial use could come back and cause some trouble down the line alternatively there are many public data sets out there one of the most popular is common crawl which is a huge Corpus of text from the internet and then there are some more refined versions such as colossal clean crawled Corpus also called C4 there's also Falcon refined web which was used to train Falcon 180b mentioned on the previous slide another popular data set is the pile which tries to bring together a wide variety of diverse data sources into the training data set which we'll talk a bit more about in the next slide and then we have hugging face which has really emerged as a big player in the generative Ai and large language model space who houses a ton of Open Access Data sources on their platform another place are private data sources so a great example of this is fin pile which was used to train Bloomberg GPD and the key upside of private data sources is you own the rights to it and and it's data that no one else has which can give you a strategic Advantage if you're trying to build a model for some business application or for some other application where there's some competition or environment of other players that are also making their own large language models finally and perhaps the most interesting is using an llm to generate the training data a notable example of this comes from the alpaca model put out by researchers at Stanford and what they did was they trained an llm alpaca using structured text generated by gpt3 this is my cartoon version of it you pass on the prompt make me training data into your large language model and it spits out the training data for you turning to the point of data set diversity that I mentioned briefly with the pile one aspect of a good training data set seems to be data set diversity and the idea here is that a diverse data set translates to to a model that can perform well in a wide variety of tasks essentially it translates into a good general purpose model here I've listed out a few different models and the composition of their training data sets so you can see gpt3 is mainly web pages but also some books you see gopher is also mainly web pages but they got more books and then they also have some code in there llama is mainly web pages but they also have books code and scientific articles and then Palm is mainly built on conversational data but then you see it's trained on web pages books and code how you curate your training data set is going to drive the types of tasks the large language model will be good at and while we're far away from an exact science or theory of this particular data set composition translates to this type of model or like adding an additional 3% code in your trading data set will have this quantifiable outcome in the downstream model while we're far away from that diversity does seem to be an important consideration when making your training data sets another thing that's important to ask ourselves is how do we prepare the data again the quality of our model is driven by the quality of our data so one needs to be thoughtful with the text that they use to generate a large language model and here I'm going to talk about four key data preparation steps the first is quality filtering this is removing text which is not helpful to the large language model this could be just a bunch of random gibberish from some corner of the internet this could be toxic language or hate speech found on some Forum this could be things that are objectively false like 2 + 2al 5 which you'll see in the book 1984 while that text exists out there it is not a true statement there's a really nice paper it's called survey of large language models I think and in that paper they distinguish two types of quality filtering the first is classifier based and this this is where you take a small highquality data set and use it to train a text classification model that allows you to automatically score text as either good or bad low quality or high quality so that precludes the need for a human to read a trillion words of text to assess its quality it can kind of be offloaded to this classifier the other type of approach they Define is heuristic based this is using various rules of thumb to filter the text text this could be removing specific words like explicit text this could be if a word repeats more than two times in a sentence you remove it or using various statistical properties of the text to do the filtering and of course you can do a combination of the two you can use the classifier based method to distill down your data set and then on top of that you can do some heuristics or vice versa you can use heuristics to distill down the data set and then apply your classifier there's no one- siiz fits-all recipe for doing quality filter in rather there's a menu of many different options and approaches that one can take next is D duplication this is removing several instances of the same or very similar text and the reason this is important is that duplicate texts can bias the model and disrupt training namely if you have some web page that exists on two different domains one ends up in the training data set one ends up in the testing data set this causes some trouble trying to get a fair assessment of model performance during training another key step is privacy redaction especially for text grab from the internet it might include sensitive or confidential information it's important to remove this text because if sensitive information makes its way into the training data set it could be inadvertently learned by the language model and be exposed in unexpected ways finally we have the tokenization step which is essentially translating text into numbers and the reason this is important is because neural networks do not understand text directly they understand numbers so anytime you feed something into a neural network it needs to come in numerical form while there are many ways to do this mapping one of the most popular ways is via the bite pair encoding algorithm which essentially takes a corpus of text and deres from it an efficient subword vocabulary it figures out the best choice of subwords or character sequences to define a vocabulary from which the entire Corpus can be represented for example maybe the word efficient gets mapped to a integer and exists in the vocabulary maybe sub with a dash gets mapped to its own integer word gets mapped to its own integer vocab gets mapped to its own integer and UL gets mapped to its own integer so this string of text here efficient subword vocabulary might be translated into five tokens each with their own numerical representation so one two three four five there are python libraries out there that implement this algorithm so you don't have to do it from scratch namely there's the sentence piece python Library there's also the tokenizer library coming from hugging face here the citation numbers and I provide the link in the description and comment section below moving on to step two model architecture so in this step we need to define the architecture of the language model and as far as large language models go Transformers have emerged merged as the state-of-the-art architecture and a Transformer is a neural network architecture that strictly uses attention mechanisms to map inputs to outputs so you might ask what is an attention mechanism and here I Define it as something that learns dependencies between different elements of a sequence based on position and content this is based on the intuition that when you're talking about language the context matters and so let's look at a couple examples so if we see the sentence I hit the base baseball with a bat the appearance of baseball implies that bat is probably a baseball bat and not a nocturnal mammal this is the picture that we have in our minds this is an example of the content of the context of the word bat so bat exists in this larger context of this sentence and the content is the words making up this context the the content of the context drives what word is going to come next and the meaning of this word here but content isn't enough the positioning of these words is also important so to see that consider another example I hit the bat with a baseball now there's a bit more ambiguity of what bat means it could still mean a baseball bat but people don't really hit baseball bats with baseballs they hit baseballs with baseball bats one might reasonably think bad here means the nocturnal mammal and so an attention mechanism captures both these aspects of language more specifically it will use both the content of the sequence and the positions of each element in the sequence to help infer what the next word should be well at first it might seem that Transformers are a constrained in particular architecture we actually have an incredible amount of freedom and choices we can make as developers making a Transformer model so at a high level there are actually three types of Transformers which follows from the two modules that exist in the Transformer architecture namely we have the encoder and decoder so we can have an encoder by itself that can be the architecture we can have a decoder by itself that's another architecture and then we can have the encoder and decoder working together and that's the third type of Transformer so let's take a look at these One By One The encoder only Transformer translates tokens into a semantically mean meaningful representation and these are typically good for Tech classification tasks or if you're just trying to generate a embedding for some text next we have the decoder only Transformer which is similar to an encoder because it translates text into a semantically meaningful internal representation but decoders are trying to predict the next word they're trying to predict future tokens and for this decoders do not allow self attention with future elements which makes it great for text generation tasks and so just to get a bit more intuition of the difference between the encoder self attention mechanism and the decoder self attention mechanism the encoder any part of the sequence can interact with any other part of the sequence if we were to zoom in on the weight matrices that are generating these internal representations in the encoder you'll see that none of the weights are zero on the other hand for a decoder it uses so-called masked self attention so any weights that would connect a token to a token in the future is going to be set to zero it doesn't make sense for the decoder to see into the future if it's trying to predict the future that would kind of be like cheating and then finally we can combine the encoder and decoder together to create another choice of model architecture this was actually the original design of the Transformer model kind of what's depicted here and so what you can do with the encoder decoder model that you can't do with the others is the so-called cross attention so instead of just being restricted to self attention with the encoder or mask self attention with the decoder the encoder decoder model allows for cross attention where the embeddings from the encoder so this will generate a sequence and the internal embeddings of the decoder which will be another sequence will have this attention weight Matrix so that the encoders representations can communicate with the decoder representations and this tends to be good for tasks such as translation which was the original application of this Transformers model while we do have three options to choose from when it comes to making a Transformer the most popular by far is this decoder only architecture where you're only using this part of the Transformer to do the language modeling and this is also called causal language modeling which basically means given a sequence of text you want to predict future text Beyond just this highlevel choice of model architecture there are actually a lot of other design choices and details that one needs to take into consideration first is the use of residual connections which are just Connections in your model architecture that allow intermediate training values to bypass various hidden layers and so to make this more concrete this is from reference number 18 Linked In the description and comment section below what this looks like is you have some input and instead of strictly feeding the input into your hidden layer which is this stack of things here you allow it to go to both the hidden layer and to bypass the hidden layer then you can aggregate the original input and the output of the Hidden layer in some way to generate the input for the next layer and of course there are many different ways one can do this with all the different details that can go into a hidden layer you can have the input and the output of the Hidden layer be added together and then have an activation applied to the addition you can have the input and the output of the Hidden layer be added and then you can do some kind of normalization and then you can add the activation or you can have the original input and the output of the Hidden layer just be added together you really have a tremendous amount of flexibility and design Choice when it comes to these residual Connections in the original Transformers architecture the way they did it was something similar to this where the input bypasses this multiheaded attention layer and is added and normalized with the output of this multi attention layer and then the same thing happens for this layer same thing happens for this layer same thing happens for this layer and same thing happens for this layer next is layer normalization which is rescaling values between layers based on their mean and standard deviation and so when it comes to layer normalization there are two considerations that we can make one is where you normalize so there are generally two options here you can normalize before the layer also called pre-layer normalization or you can normalize after the layer also called post layer normalization another consideration is how you normalize one of the most common ways is via layer norm and this is the equation here this is your input X you subtract the mean of the input and then you divide it by the variance plus some noise term then you multiply it by some gain factor and then you can have some bias term as well an alternative to this is the root mean Square Norm or RMS Norm which is very similar it just doesn't have the mean term in the numerator and then it replaces this denominator with just the RMS while you have a few different options on how you do layer normalization the most common based on that survey of large language models I mentioned earlier reference number eight pre-layer normalization seems to be most common combined with this vanilla layer Norm approach next we have activation functions and these are non-linear functions that we can include in the model which in principle allow it to capture comp Lex mappings between inputs and outputs here there are several common choices for large language models namely gelu relo swish swish Glu G Glu and I'm sure there are more but glus seem to be the most common for large language models another design Choice Is How We Do position embeddings position embeddings capture information about token positions the way that this was done in the original Transformers paper was using these sign and cosine basic functions which added a unique value to each token position to represent its position and you can see in the original Transformers architecture you had your tokenized input and the positional encodings were just added to the tokenized input for both the encoder input and the decoder input more recently there's this idea of relative positional encodings so instead of just adding some fixed positional encoding before the input is passed into the model the idea with relative positional encodings is to bake positional encodings into the attention mechanism and so I won't dive into the details of that here but I will provide this reference self attention with relative position representations also citation number 20 the last consideration that I'll talk about when it comes to model architecture is how big do I make it and the reason this is important is because if a model is too big or train too long it can overfit on the other hand if a model is too small or not trained long enough it can underperform and these are both in the context of the training data and so there's this relationship between the number of parameters the number of computations or training time and the size of the training data set there's a nice paper by Hoffman at all where they do an analysis of optimal compute considerations when it comes to large language models I've just grabbed a table from that paper that summarizes their key findings what this is saying is that a 400 million parameter model should undergo on the order of let's say like 2 to the 19 floating Point operations and have a training data consisting of 8 billion tokens and then a parameter with 1 billion models should have 10 times as many floating Point operations and be trained on 20 billion parameters and so on and so forth my kind of summarization takeaway from this is that you should have about 20 tokens per model mod parameter it's not going to be very precise but might be a good rule of thumb and then we have for every 10x increase in model parameters there's about a 100x increase in floating Point operations so if you're curious about this check out the paper Linked In the description below even if this isn't an optimal approach in all cases it may be a good starting place and rule of thumb for training these models so now we come to step three which is training these models at scale so again the central challenge of these large language models is is their scale when you're training on trillions of tokens and you're talking about billions tens of billions hundreds of billions of parameters there's a lot of computational cost associated with these things and it is basically impossible to train one of these models without employing some computational tricks and techniques to speed up the training process here I'm going to talk about three popular training techniques the first is mixed Precision training which is essentially when you use both 32bit and 16 bit floating Point numbers during model training such that you use the 16bit floating Point numbers whenever possible and 32bit numbers only when you have to more on mixed Precision training in that survey of large language models and then there's also a nice documentation by Nvidia linked below next is this approach of 3D parallelism which is actually the combination of three different parallelization strategies which are all listed here and I'll just go through them one by one first is pipeline parallelism which is Distributing the Transformer layers across multiple gpus and it actually does an additional optimization where it puts adjacent layers on the same GPU to reduce the amount of cross GPU communication that has to take place the next is model parallelism which basically decomposes The Matrix multiplications that make up the model into smaller Matrix multiplies and then distributes those Matrix multiplies across multiple gpus and then and then finally there's data parallelism which distributes training data across multiple gpus but one of the challenges with parallelization is that redundancies start to emerge because model parameters and Optimizer States need to be copied across multiple gpus so you're having some portion of the gpu's precious memory devoted to storing information that's copied in multiple places this is where zero redundancy Optimizer or zero is helpful which essentially reduces data redundancy regarding the optimizer State the gradient and parameter partitioning and so this was just like a surface level survey of these three training techniques these techniques and many more are implemented by the deepe speed python library and of course deep speed isn't the only Library out there there are a few other ones such as colossal AI Alpa and some more which I talk about in the blog associated with this video another consideration when training these massive models is training stability and it turns out there are a few things that we can do to help ensure that the training process goes smoothly the first is checkpointing which takes a snapshot of model artifacts so training can resume from that point this is helpful because let's say you're training loss is going down it's great but then you just have this spike in loss after training for a week and it just blows up training and you don't know what happened checkpointing allows you to go back to when everything was okay and debug what could have gone wrong and maybe make some adjustments to the learning rate or other hyperparameters so that you can try to avoid that spike in the loss function that came up later another strategy is weight Decay which is essentially a regularization strategy that penalizes large parameter values I've seen two ways of doing this one is either by adding a term to the objective function which is like regular regularization regular regularization or changing the parameter update Rule and then finally we have gradient clipping which rescales the gradient of the objective function if it exceeds a pre-specified value so this helps avoid the exploding gradient problem which may blow up your training process and then the last thing I want to talk about when it comes to training are hyperparameters while these aren't specific to large language models my goal here is to just lay out some common choices when it comes to these values so first we have batch size which can be either static or dynamic and if it's static batch sizes are usually pretty big so on the order of like 16 million tokens but it can also be dynamic for example in GPT 3 what they did is they gradually increased the batch size from 32,000 tokens to 3.2 million tokens next we have the learning rate and so this can also be static or dynamic but it seems that Dynamic learning rates are much more common for these models a common strategy seems to go as follows you have a learning rate that increases linearly until reaching some specified maximum value and then it'll reduce via a cosine Decay until the learning rate is about 10% % of its max value next we have the optimizer atom or atom based optimizers are most commonly used for large language models and then finally we have Dropout typical values for Dropout are between 0.2 and 0.5 from the original Dropout paper by Hinton at all finally step four is model evaluation so just cuz you've trained your model and you've spent millions of dollars and weeks of your time if not more it's still not over typically when you have a model in hand that's really just the starting place in many ways next you got to see what this thing actually does how it works in the context of the desired use case the desired application of it this is where model evaluation becomes important for this there are many Benchmark data sets out there here I'm going to restrict the discussion to the open llm leaderboard which is a public llm Benchmark that is continually updated with new models un hugging faces models platform and the four benchmarks that is used in the open El M leaderboard are Arc H swag MML and truthful QA while these are only four of many possible Benchmark data sets the evaluation strategies that we can use for these Benchmark data sets can easily port to other benchmarks so first I want to start with just Arc helis swagen MML U which are multiple choice tasks so a bit more about these Ark and MML U are essentially great school questions on subjects like math math history common knowledge you know whatever and it'll be like a question with a multiple choice response A B C or D so an example is which technology was developed most recently a a cell phone B a microwave c a refrigerator and D an airplane H swag is a little bit different these are specifically questions that computers tend to struggle with so an example of this is in the blog associated with this video which goes like this a woman is outside with a bucket ET and a dog the dog is running around trying to avoid a bath she dot dot dot a rinses the bucket off with soap and blow dries the dog's head B uses a hose to keep it from getting soapy C gets the dog wet then it runs away again D gets into a bathtub with a dog and so this is a very strange question but intuitively humans tend to do very well on these tasks and computers do not so while these are multiple choice tasks and we might think it should be pretty straight forward to evaluate model performance on them there is one hiccup namely these large language models are typically text generation models so they'll take some input text and they'll output more text they're not classifiers they don't generate responses like ABC or D or class one class 2 class 3 class 4 they just generate text completions and so you have to do a little trick to get these large language models to perform multiple choice tasks and this is essentially through prompt templates for example if you have the question which technology was developed most recently instead of just passing in this question and the choices to the large language model and hopefully it figures out to do a BC or D you can use a prompt template like this and additionally prend the prompt template with a few shot examples so the language model will pick up that I should return just a single token that is one of these four tokens here so if you pass this into to the model you'll get a distribution of probabilities for each possible token and what you can do then is just evaluate of all the tens of thousands of tokens that are possible you just pick the four tokens associated with a B C or D and see which one is most likely and you take that to be the predicted answer from the large language model while there is this like extra step of creating a prompt template you can still evaluate a large language model on these multiple choice tasks and in a relatively straightforward way however this is a bit more tricky when you have open-ended tasks such as for truthful QA for truthful QA or other open-ended tasks where there isn't a specific one right answer but rather a wide range of possible right answers there are a few different evaluation strategies we can take the first is human evaluation so a person scores the completion based on some ground truth some guidelines or both while this is the most labor int ensive this may provide the highest quality assessment of model completions another strategy is we could use NLP metrics so this is trying to quantify the completion quality using metrics such as perplexity blue score row score Etc so just using the statistical properties of the completion as a way to quantify its quality while this is a lot less labor intensive it's not always clear what the mapping between a completions statistical properties is to the quality of that that completion and then the third approach which might capture The Best of Both Worlds is to use an auxiliary fine-tuned model to rate the quality of the completions and this was actually used in the truthful QA paper should be reference 30 where they created an auxiliary model called GPT judge which would take model completions and classify it as either truthful or not truthful and then that would help reduce the burden of human evaluation when evaluating model outputs okay so what's next so you've created your large language model from scratch what do you do next often this isn't the end of the story as the name base models might suggest base models are typically a starting point not the final solution they are really just a starting place for you to build something more practical on top of and there are generally two directions here one is via prompt engineering and prompt engineering is just feeding things into the language model and harvesting their completions for some particular use case another Direction one can go is via model fine-tuning which is where you take the pre-trained model and you adapt it for a particular use case prompt engineering and model fine tuning both have their pros and cons to them if you want to learn more check out the previous two videos of this series where I do a deep dive into each of these approaches if you enjoyed this content please consider liking subscribing and sharing it with others if you have any questions or suggestions for future content please drop those in the comment section below and as always thank you so much for your time and thanks for watching"
eC6Hd1hFvos,2023-10-01T20:19:38.000000,Fine-tuning Large Language Models (LLMs) | w/ Example Code,hey everyone I'm Shaw and this is the fifth video in the larger series on how to use large language models in practice in the previous video we talked about prompt engineering which is concerned with using large language models out of the box while prompt engineering is a very powerful approach and can handle a lot of llm use cases in practice for some applications prompt engineering just doesn't cut it and for those cases we can go one step further and fine-tune a existing large language model for a specific use case so Navy's question is what is model fine tuning the way I like to Define it is taking a pre-trained model and training at least one internal model parameter and here I mean the internal weights or biases inside the neural network what this typically looks like is taking a pre-trained existing model like gpt3 and fine-tuning it for a particular use case for example chatgypt to use an analogy here gpt3 is like a raw diamond right out of the earth it's a diamond but it's a bit rough around the edges fine tuning is taking this raw diamond and transforming it into something a bit more practical something that you can put on a diamond ring for example so the process of taking the raw base model of gpt3 and transforming it into the fine-tuned model of gbt 3.5 turbo for example is what gives us applications like chat GPT or any of the other incredible applications of large language models we're seeing these days to get a more concrete sense of the difference between a base model link gpt3 and a fine-tuned model let's look at this particular example we have to keep in mind that these Foundation large language models like gpg3 llama 2 or whatever your favorite large language model is these models are strictly trained to do word prediction given a sequence of words predicting the next word so when you train one of these launch language models on huge Corpus of text and documents and web pages what it essentially becomes is a document completer what that translates to in practice is if you plug into a lot of these base models like gpt3 the prompt tell me how to find tune a model a typical completion might look something like this where it's just listing out questions like you might see in a Google search or maybe like a homework assignment or something here when I prompted gpt3 to tell me how to fine-tune a model the completion was as follows how can I control the complexity of a model how do I know when my model is done how do I test a model well this might be reasonable for gpt3 to do based on the data that it was trained on this isn't something that's very practical now let's look at the fine-tuned model completion so now we have text DaVinci zero zero three which is just one of the many fine-tuned models based on gpt3 coming from open AI we give it the same prompt tell me how to fine tune a model and this is the completion fine-tuning a model involves a adjusting the parameters of a pre-trained model in order to make it better suited for a given task there are generally three steps involved to fine-tuning a model select a base model adjust parameters train the model while this completion may not be perfect it's much more aligned what we were hoping to get out of the language model compared to the base model's completion so if you want to learn more about model fine tuning and how open AI did their fine tuning their alignment tuning and instruction tuning check out the references in the description and comment section below so as we saw when comparing a base model to a fine-tuned model we see that the fine-tune model can generate completions that are much more aligned and desirable for our particular use case Beyond this performance there's actually a deeper reason why you might want to fine tune and that is the observation that a smaller fine-tuned model can often outperform a larger base model this was demonstrated by open AI in their instruct GPT model where they're small 1.3 billion parameter fine tuned instruct GPT model generated to completions that were preferred to gpt3 completions even though gpt3 had about 100 times as many internal parameters this is one of the biggest upsides of fine tuning you don't have to rely on some massive general purpose large language model to have good performance in a particular use case or application now that we have a better understanding of what fine-tuning is and why it's so great let's look at three possible ways one can fine tune an existing large language model the first is via self-supervised learning this is the same way these base models and Foundation large language models are trained so in other words you get your training Corpus of text and you train the model in a self-supervised way in other words you take a sequence of text like listen to your you feed it into the model and you have it predict a completion if we feed in listen to your it might spit out hard what would differentiate fine tuning with self-supervised learning versus just training a base model through self-supervised learning is that you can curate your training Corpus to align with whatever application that you're going to use the fine-tuned model for for example if I wanted to fine-tune gpt3 to write text in the likeness of Me Maybe I would feed it a bunch of my torch data science blogs and then that resulting fine-tuned model might be able to generate completions that are more like my style the second way we can fine tune a model is via supervised learning this is where we have a training data set consisting of inputs and Associated outputs or targets for example if we have a set of question answer pairs such as who was the 35th President of the United States and then the answer is John F Kennedy we can use this question answer pair to fine-tune an existing model to learn how to better answer questions so the reason this might be helpful as we saw before if we were to just feed in who was the 35th President of the United States into a base model the completion that it might generate is who was the 36th president of the United States who was the 40th President of the United States who is the speaker of the house so on and so forth but through having these question answered pairs we can fine tune the model to essentially learn how to answer questions but there's a little trick here these language models are again document completers so we actually have to massage these input output pairs a bit before we can feed it into our large language model for training one simple way we can do this is via prompt templates for example we could generate a template please answer the following question where we input the question here so our input would go here and then we input the target here so the answer would go here and then through this process we can translate our training data set to a set of prompts and generate a training Corpus and then go back to the self-supervised approach and the final way one one can fine tune an existing model is via reinforcement learning while there are many ways one could do this I'm going to just focus on the approach outlined by open Ai and generating their instruct GPT models which consisted of three steps the first was supervised fine tuning so essentially what we were talking about in this second way to fine-tune a model this consists of two steps one curating your training data set and then two fine-tuning the model The Next Step was to train a reward model and all this is is essentially a model that can generate a score for a language model's completion so if it generates a good completion it'll have a high score it generates a bad completion it'll generate a low score so what this looked like for the instruct GPT case was as follows you start with a prompt and you pass it in to your supervised fine-tuned model from here but you don't just do it once you actually do it many times so you generate multiple completions for the same prompt then you get human labelers to rank the responses from worst to best and then you can use that ranking to train the reward model which is indicated by this Square here and then the final step is to do reinforcement learning with your favorite reinforcement learning algorithm in the case of instruct GPT they used proximal policy optimization or PPO for short what this looks like is you take the prompt you pass it in to your supervised fine-tuned model and then you pass that completion to the reward model and then the reward model will essentially give feedback to the fine-tuned model and this is how you can update the model parameters and eventually end up with a model that's fine-tuned even further I know this was a ton of information but if you want to dive deeper into any one of these approaches check out the blog in towards data science where I go into a bit more detail on each of these approaches okay so to keep things relatively simple for the remainder of the video we'll be focused just on the supervised learning approach to model fine tuning here I break that process down into five steps first choose your fine tuning task so this could be text summarization it could be text generation it could be binary classification text classification whatever it is you want to do next you prepare your training data set if you're trying to do text summarization for example you would want to have input output pairs of text and the desired summarization and then you take those input Alpha Pairs and generate a training Corpus using prompt templates for example next you want to choose your base model there are many Foundation large language models out there or there are many existing fine-tuned large language models out there and you can choose either of these as your starting place next we can fine tune the model via supervised learning and then finally we evaluate model performance there's certainly a lot of details into each of these steps but here I'm just going to focus on step number four the fine tuning the model with supervise learning and here I want to talk about three different options we have when it comes to updating the model parameters the first option is to retrain all the parameters given our neural network given our language model we go in and we tweak all the parameters but perhaps obviously this comes with the downside when you're talking about billions tens of billions hundreds of billions of internal model parameters the computational cost for training explodes even if you're doing the most efficient tricks to speed up the training process retraining a billion parameters is going to be expensive another option we can do is transfer learning and this is essentially where we take our language model and instead of retraining all the parameters we freeze most of the parameters and only fine tune the head namely we fine tune the last few layers of the model where the model embeddings or internal representations are translated into the Target or the output layer and while transfer learning is a lot cheaper than retraining all parameters there is still another approach that we can do which is the so-called parameter efficient fine tuning this is where we take our language model and instead of just phrasing a subset of the weights we freeze all of the weights we don't change any internal model parameters instead what we do is we augment the model with additional parameters which are trainable and the reason why this is advantageous is that it turns out that we can fine tune a model with a relatively small set of new parameters as can be seen by this beautiful picture here one of the most popular ways to do this is the so-called low rank adaptation approach or low raw for short like I mentioned in the previous slide this fine tunes a model by adding new trainable parameters here we have a cartoon of a neural network but let's just consider one layer the mapping from these inputs to this hidden layer here we can call our inputs X and then we can call the hidden layer essentially some function of X and to make this a bit more concrete we can write this as an equation h of X is just equal to X we can think of it as a vector to keep things simple and some white Matrix which is just some two-dimensional Matrix to see this a bit more visually we have our weight Matrix which is some d by K Matrix we have X which will just take to be a vector in this case the multiplication of these two things will generate our hidden layer and for the mathesonatos here the spaces that these objects live in and so this is what the situation looks like without Lora where if we're going to do the full parameter fine-tuning what this will look like is all the parameters in this weight Matrix are trainable so here W naught is a d by K Matrix and let's just say d is a thousand K is a thousand this would translate to one million trainable parameters which may not be a big number but when you have a lot of layers this number of triangle parameters can really explode now let's see how low raw can help us reduce the number of trainable parameters again we're just going to look at one of the layers but now we're going to add some additional parameters to the model what that looks like mathematically is we have the W naught times x is equal to H of X like we saw in the previous slide but now we're adding this additional term here which is Delta W Times X this is going to be another weight Matrix the same shape as W naught and looking at this you might think Shaw how does this help us we just doubled the number of parameters yeah sure if we keep W naught Frozen we still have Delta W with the same number of parameters to deal with but let's say that we Define delta W to be the multiplication of two matrices b and a in this case our hidden layer becomes W naught times X Plus ba times x looking at this more visually we have W naught which is the same same weight Matrix we saw in the previous slide but now we have b and a which have far fewer terms than W naught does and then what we can do is through matrix multiplication generate a matrix of the proper size namely Delta W add it to W naught multiply all that by X and generate our h of X looking at the dimensionality of these things W naught and Delta W live in the same space they're matrices of d by K B is going to be a matrix of d by R A is going to be a matrix of R by K and then h of X is going to be D by 1. the key thing here is this R number what the authors of this method called the intrinsic rank of the model the reason that this works and we get the efficiency gains is that this R is a lot smaller than D and K to see how this plays out unlike before where W naught was trainable now these parameters are going to be Frozen and B and a are trainable and maybe as you can just tell visually from the area of this rectangle versus the areas of these two rectangles b and a contain far fewer terms than W naught to make this a bit more concrete let's say d is equal to a thousand K is equal to a thousand and our intrinsic rank is equal to two what this translates to is 4 000 trainable parameters as opposed to the million trainable parameters we saw in the previous slide this is the power of low raw it allows you to fine tune a model with far fewer trainable parameters if you want to learn more about Laura check out the paper Linked In the description below or if you want something that's a bit more accessible check out the blog and towards data science where I talk about this a bit more let's dive into some example code and how we can use low raw to fine tune a large language model here I'm going to use the hugging face ecosystem namely pulling from libraries like data sets Transformers p e f t and evaluate which are all hugging face python libraries also importing in pi torch and numpy for some extra things with our Imports the next step is to choose our base model here I use distill burnt uncased which is a base model available on hugging faces model repository this is what the model card looks like we can see that it only has 67 million parameters in it and then there's a lot more information about it on the model card here we're gonna take distilbert uncased and we're gonna fine tune it to do sentiment analysis we're going to have it take in some text and generate a label of either positive or negative based on the sentiment of the input text so to do that we need to Define some label Maps so here we're just defining that 0 is going to be negative and one is going to mean positive and vice versa that negative means zero and positive means one now we can take these label maps and we can take our model checkpoint and we can plug it into this Nifty Auto model for sequence classification and class available from the Transformers library and very easily we import this base model specifically ready to do binary classification the way this works is that hugging face has all these base models and has many versions of them where they replace the head of the model for many different tasks and we can get a better sense of this from the Transformers documentation as shown here you can see that this Auto model for sequence classification has a lot of Base models that it can build on top of here we're using distilber which is a smaller version of bird here but there are several models you can choose from the reason I went with distillbird is because it only has 67 million parameters and it can actually run on my machine the next step is to load the data set so here I've actually just made the data set available on the hugging face data set repository so you should be able to load it pretty easily it's called IMDb truncated it's a data set of IMDb movie reviews with an Associated positive or negative label if we print the data set it looks something like this there are two parts to it there's this train part and then there's this validation part and then you can see that both the training and validation data sets have 1000 rows in them this is another great thing about model fine tuning is that while training a large language model from scratch may require trillions of tokens or a trillion words in your training Corpus fine-tuning a model requires far fewer examples here we're only going to be using a thousand examples for model fine tuning the next step is to pre-process the data here the most important thing is we need to create a tokenizer if you've been keeping up with this series you know that tokenization is a critical step when working with large language models because learner networks do not understand text they understand numbers and so we need to convert the text that we pass into the large language model into a numerical form so that it can actually understand it so here we can use the auto tokenizer class from Transformers to grab the tokenizer for the particular base model that we're working with next we can create a tokenization function this is a function that defines how we will take each example from our training data set and translate it from text to numbers this will take in examples which is coming from our training data set and you see we're extracting the text so going back to the previous slide you can see that our training data set has two features it has a label and a piece of text so you can imagine each row of this training data set has text and it has a label associated with that text so when we go over here the examples is just like a row from this data set and we're grabbing the text from that example and then what we do is we Define the side that we want to truncate truncation is important because the examples that we pass into the model for training need to be the same length we can either achieve this by truncating long sequences or padding short sequences to like a predetermined fixed length or a combination of the two so we're just choosing the current truncation side to be left and here we're tokenizing the text here's our tokenizer that we defined up here passing in the text we're returning numpy tensors we're doing the truncation and we defined how to do that here and then we're defining our max length and this will return our tokenized inputs since the tokenizer does not have a pad token this is a special token that you can add to a sequence which will essentially be ignored by the large language model here we're adding a pad token and then we're updating the model to handle this additional token that we just created finally we've applied this tokenize function to all the data in our data set using this map method here we have our data set and we plan this map method we pass in our tokenized function and it'll output a tokenized version of our data set to see what the output looks like we have another data set dictionary we have the training and validation data sets but now you see we have have these additional features we don't only have the text in the label but we also have input IDs and we also have this attention mask one other thing we can do at this point is to create a data collator this is essentially something that will dynamically pad examples in a given batch to be as long as the longest sequence in that batch for example if we have four examples in our batch the longest sequence has 500 but the others have shorter ones it'll dynamically pad the shorter sequences to match the longer one and the reason why this is helpful is because if you pad your sequences dynamically like this with a collater it's a lot more computationally efficient than padding all your examples across all 1000 training examples because you might just have one very long sequence at 512 that is creating unnecessary data that you have to process next we want to define a valuation metrics so this is how we will monitor the performance of the model during training so here I just did something simple I'm going to import the accurac see from the evaluate python Library so we can package our evaluation strategy into a function that here I'm going to call compute metrics and so here we're not restricted to just using one evaluation metric or even just using accuracy as an evaluation metric but just to keep things simple here I just stick with accuracy here we take a model output and we unpack it into a prediction and label the predictions here are the logits and so it's going to have two elements one associated with the negative class and one associated with the Positive class and all this is doing is evaluating which element is larger and whichever one is larger is going to become the label so if the zeroth element is larger the ARG Max will return zero and that'll become the model prediction and vice versa if the first element is the largest this will return a one and then that will become the model prediction and then here we just compute accuracy by comparing the model prediction to the ground truth label so before training our find tuned model we can evaluate the performance of the base model out of the box so let's see what that looks like here we're going to generate a list of examples such as it was good not a fan don't recommend the better than the first one this is not worth watching even once and then this one is a pass then what we do is for each piece of text in this list we're gonna tokenize it compute the logits so basically we're going to pass it into the model and take the logits out then we're going to convert the logits to a label either a zero or one and so the output looks like this we have the untrained model predictions it was good the model says this has a negative sentiment not a fan don't recommend the model says this as a negative sentiment so that's correct better than the first one the model says this has a negative sentiment even though that's probably positive this is not worth watching even once model says it's a negative sentiment which is correct and then this one is a pass and the model signs a negative sentiment to them as you can see it got two out of five correctly essentially this model is as good as chance as flipping a coin it's right about half the time which is what we would expect from this unfine-tuned based model so now let's see how we can use Lora to fine-tune this model and hopefully get some better performance the first thing we need to do is Define our low configuration parameters first is the task type we're saying we're going to be doing sequence classification next we Define the intrinsic rank of the trainable weight matrices so that was that smaller number that allowed b and a to have far fewer parameters than just W naught next we Define the lower Alpha value which is essentially a parameter that's like the learning rate when using the atom Optimizer then we Define the low raw Dropout which is just the probability of Dropout and that's where we randomly zero internal parameters during training finally we Define which modules we want to apply low raw to and so here we're only going to apply it to the query layers and then we can use these configuration settings and update our model to get another model but one that is ready to be fine-tuned using low raw and so that's pretty easy we just use this get p e f t model by passing in our original model and then our config from above then we can easily print the number of trainable parameters in our model and we can see it's about a million out of this 67 million that are in the base model so you can see that we're going to be fine-tuning less than two percent of the model parameters so it's just a huge cost savings like 50 times fewer model parameters than if we were to do the full parameter fine tuning next we're going to Define our hyper parameters and training arguments so here we put the learning rate as .001 we put the batch size as four and the number of epochs as 10. next we say where we want the model to be saved here I dynamically create a name so it'll be the model checkpoint Dash low raw text classification learning rates what we defined before batch size is what we put before find weight Decay as 0.01 then we set the evaluation strategy as Epoch so every Epoch is going to compute those evaluation metrics the safe strategy is every Epoch is going to save the model parameters and then load best model at the end so at the end of training it's going to return us the best version of the model then we just plug everything to this trainer class trainer takes in the model and takes in the learning arguments it takes in our training and validation data sets it takes in our tokenizer it takes in our data collator and it takes in our evaluation metrics put that all into this trainer class and then we train the model using this dot train method so during training these metrics will be generated so we can see the epochs the training loss the validation loss and the accuracy so as you can see the training loss is decreasing which is good and the accuracy is increasing which is good but you can see that the validation loss is increasing so this is a sign of overfitting which I'll comment on in a bit here now that we have our fine-tuned model in hand we can evaluate its performance on those same five examples that we evaluated before 5 fine tuning basically same code copy pasted but here's the different output the text it was good is now correctly being classified as positive not a fan don't recommend is correctly classified as negative better than the first one correctly classified as positive and then this is not worth watching even one correctly classified as negative and then this one this one is a pass it's classified as positive but this one's a little tricky even though we don't get perfect performance on these five like baby examples we do see that the model is performing a little bit better and so returning back to the overfitting problem this example is meant to be more instructive than practical in practice before jumping to low raw one thing we might have tried is to Simply do transfer learning to see how close we can get to something that does sentiment analysis well after doing the transfer learning then maybe we would use low Rod to fine tune the model even further either way I hope this example was instructed and gave you an idea of how you can start fine-tuning your very own large language models if you enjoyed this content please consider liking subscribing and sharing it with others if you have any questions or suggestions for future content please feel free to drop those in the comment section below and as always thank you so much for your time and thanks for watching
FEEvnpfD16c,2023-09-30T13:03:33.000000,The REALITY of entrepreneurship. #entrepreneurship #startup #smallbusiness,when people think about entrepreneurship it's typically something like VC funding or Tech startup or scaling the business to the moon but if you look out into the real world the overwhelming majority of businesses do not look like that at all typically what entrepreneurship looks like is a mom-and-pop shop it's a family-owned business it's a small boutique consulting firm it's a e-commerce business someone making wedding cakes out of their kitchen on the weekend in other words entrepreneurship isn't this like highbrow sophisticated super high status thing in fact entrepreneurship is like the lowest status thing because literally anyone can do it but funny enough even though anyone can do it most people don't do it and that's for one simple reason entrepreneurship is very uncomfortable and most reasonable people do not enjoy being uncomfortable but if you're very unreasonable
0cf7vzM_dZ0,2023-09-26T22:38:42.000000,Prompt Engineering: How to Trick AI into Solving Your Problems,hey everyone I'm Shaw and this is the fourth video in the larger series on using large language models in practice today I'm going to be talking about prompt engineering and now before all the technical folks come after me with their pitchforks let's just address the elephant in the room so if you're a technical person like Tony Stark here you might be rolling your eyes at the idea of prompt engineering you might say prompt engineering is not engineering or prompt engineering is way overhyped or even prompt engineering is just a complete waste of time and when I first heard about the concept I had a similar attitude it didn't seem like something worth my time I was more concerned with the model development side of things like how can I fine-tune a large language model but after spending more time with it my perspective on prompt engineering has changed my goal with this dog is to give a sober and practical overview of prompt engineering and the technical people out there who are rolling their eyes like this version of Tony Stark maybe by the end of this you'll be more like this version of Tony Stark oh wow imprompt engineering will be another tool in your AI data science and software development Arsenal so since this is kind of a long session I apologize in advance first I'll talk about what is prompt engineering then I'll talk about two different levels of problem engineering what I call the easy way and the less easy way next we're going to talk about how you can build AI apps with prompt engineering then I'll talk about seven tricks for prompt engineering and then finally we will walk through a concrete example of how to create an automatic grader using Python and Lang chain what is prompt engineering the way I like to Define it is it's any use of an llm out of the box but there's a lot more that can be said about prompt engineering here are a few comments on prompt engineering that have stood out to me the first comes from the paper by white at all which defines prompt engineering as the means by which llms are programmed with prompts and this raises this idea that prompt engineering is a new way to program computers and this was something that was really eye-opening for me when I first saw tragedy PT and heard this idea of prompt engineering it felt like oh this is just like a chat bot kind of thing but as I dove deeper into it and I read this paper and consumed other resources out there the deeper picture here is that large language models provide a path to making programming and computation as easy as asking a computer what you want in natural language another definition comes from the paper by Hugh at all which defines prompt engineering as an empirical art of composing and formatting the prompt to maximize a model's performance on a desired task the reason this one stood out to me is because it highlights this aspect of prompt engineering then it's at this point it's not really a science it's a collection of heuristics and people throwing things against the wall and accidentally stumbling across techniques and then through that messy process it seems like some tricks and heuristics are starting to emerge and this might be part of the reason why people are so put off by prompt engineering because it doesn't seem like a serious science and that's because it's not a serious science it's still way too early in this new paradigm of large language models that we're operating in it's going to take a while for us to understand what these models are actually doing why they actually work and I think with that we'll have a better understanding of how to manipulate them how to throw stuff at them and get desired results out and the final comment that I really liked about prompt engineering comes from Andre carpathy in his state of GPT talk from Microsoft build 2023 where he said language models want to complete documents and so you can trick them into performing tasks just by arranging fake documents I feel like this captures the essence of prompt engineering language models are not explicitly trained to do the vast majority of tasks we ask them to do all these language model want to do is to predict the next token and then predict the next one and the next one and the next one and so I love this concept of tricking the AI into solving your problems and that's essentially all prompt engineering is constructing some text that generates the desired outcome from the large language model and so the way I like to think about it is that there are two levels of prompt engineering the first level is what I call the easy way which is essentially chat GPT or something similar so now Google has barred out there Microsoft has Bing chat all these different applications provide a very user-friendly and intuitive interface for interacting with these large language models and so while this is the easiest and cheapest way to interact with large language models it is a bit restrictive in that you can't really use chat GPT to build an app maybe it'll help you write some code but you can't integrate chat gbt into some piece of software or some larger application that you want to build out that's where the less easy way come comes in the less easy way is to interact with these large language models programmatically and so you could use Python for this you could use JavaScript or whatever programming language the key upside of the less easy way here is that you can fully customize how a large language model fits into a larger piece of software this in many ways unlocks a new paradigm for programming and software development and that brings us to building AI apps with prompt engineering like I just said the less easy way unlocks a new paradigm of software development and to demonstrate this let's just look at a specific use case suppose we wanted to make an automatic grader for a high school history class and while this might be easy enough if the questions are multiple choice or true false this becomes a bit more difficult when the answers are short form or even long form text responses and so an example of this is as follows consider the question who was the 35th president of of the United States well you might think that there's only one answer John F Kennedy there are many answers that are reasonable and could be considered correct and so here's a list of a few examples so there's John F Kennedy but JFK a very common abbreviation of his name could also be considered correct there's also Jack Kennedy which is a common nickname used for JFK there's John Fitzgerald Kennedy which is his full name and someone probably trying to get extra credit and then there's John F Kennedy where the student may have just forgotten to put one of the ends in his last name let's see how we can go about making a piece of software that can do this grading process automatically first we have the traditional Paradigm this is how programming has always been done and here it's on the developer to figure out the logic to handle all the variations and all the edge cases this is the hard part of programming it's like writing a robust piece of software that can handle all the different edge cases so this might require the user to input a list of all possible correct answers and then that might be hard you know with homework with a bunch of questions you can't anticipate every possible answer that a student is going to write down and traditionally if you're trying to evaluate texts against some like Target text you probably would be using some kind of like exact or fuzzy string matching algorithm but now let's look at this new paradigm where we can incorporate large language models into the logic of our software and here you can use an olm to handle all the logic of this automatic grading task using prompt engineering instead of coming in with some code that does exact matching or fuzzy matching and figuring out the logic that gives you the desired outcome you could just write a prompt and so what this might look like is you write the prompt you are a high school history teacher grading homework assignments based on the homework question indicated by q and the correct answer indicated by a your task is to determine whether the student's answer is correct grading is binary therefore student answers can be correct or wrong simple misspellings are okay then we have this template here where we have q and a as indicated by The Prompt and these curly brackets are indicating where a question is going to be placed in and where the single correct answer is going to be placed in and then we also have a place for the student answer all this can be fed to a large language model and the language model will generate a completion that says the student answer is correct or the student answer is wrong and maybe it'll give some reasoning behind why this student answer is wrong taking a step back and comparing these two approaches to this problem approach one was to manually sit down think and write out a string matching algorithm that tried to handle all the different edge cases and variations of potentially correct answers I'm an okay programmer at best so it would probably take me a week or so to get a piece of software that did an okay job at doing that comparing that to how long it took me to write this prompt which is about two minutes think of the time saving here I could have spent a week trying to use string matching to solve this problem or I could have spent a couple minutes writing a prop this is just like the core logic of the application this isn't including all the peripherals the user interfaces the boilerplate code and stuff like that but that's the cost savings we're talking about here we're talking about minutes versus days or weeks of software development and so that's the power of prompt engineering and this kind of new way of thinking of programming and software development so now let's talk about best practices for prompt engineering here I'm going to talk about seven tricks you can use to write better prompts and this is definitely not a complete or comprehensive list this is just a set of tricks that I've extracted from comparing and contrasting a few resources if you want to dive deeper into any one of these tricks check out the blog published and towards data science where I talk more about these tricks and different resources you can refer to to learn more about any of these so just running through this the first trick is to be descriptive even though so in a lot of writing tasks less is more when doing prompt engineering it's kind of the opposite more is better trick twos give examples and so this is the idea of few shot learning you give a few demonstrations of questions and answers for example in your prompt and that tends to improve the llm's performance trick three is to use structured text which we'll see what that looks like later trick four is Chain of Thought which is essentially having the llm think step by step trick five is using chatbot personas so basically assigning a role or expertise to the large language model trick six is this flipped approach where instead of you are asking the large language model questions you prompted to ask you questions so it can extract information from you to generate a more helpful completion finally trick 7 is what I summarize as reflect review and refine which is essentially having the large language model reflect on its past responses and refine them either by improving it or or identifying errors in past responses okay so let's see what this looks like via a demo here I'm going to use Chad GPT and it's important to know what large language model you're using because optimal prompting strategies are dependent on the large language model that you're using Chachi PT is a fine-tuned model so you don't really have to break your back too much on the prompt engineering to get reasonable responses but if you're working with a base model like gpt3 you're going to have to do a lot more work on the prompt engineering side to get useful responses and that's because gpg3 is not a fine-tuned model it only does word prediction while chat GPT is a fine-tuned model it was trained to take instructions and then on top of that they did this reinforcement learning with human feedback to refine those responses even further trick one is to be descriptive so let's compare and contrast an example with and without this trick so let's say I want to use chatgpt to help me write a birthday message for my dad the naive thing to do would be to type been to chat GPT the following prompt write me a birthday message for my dad and so it's gonna do that and so while this might be fine for some use cases I don't write messages that are verbose like this and the response is a bit generic you know like Dad you've been my rock my guide my source of inspiration throughout my life your wisdom kindness and unwavering support has shaped me into the person I am today for that I am eternally grateful oh that's very nice I tend to be a bit more cheeky when it comes to these kinds of birthday messages and whatnot another thing we can do is to employ this trick of being descriptive and getting a good response from chat you PT what that might look like is you type in write me a birthday message for my dad no longer than 200 characters okay so now we don't want it to be as verbose this is a big birthday because he's turning 50 so now we're giving more context to celebrate I booked us a boy's trip to Cancun more context and then be sure to include some cheeky humor he loves that so I'm giving jack gbt more to work with to tailor the response to something closer that I would actually write so let's see what this response looks like okay so it's a lot more concise which I like it says happy 50th dad time to Fiesta like you're 21 again in Cancun cheers to endless Adventures ahead hashtag dad and Cancun that's actually pretty funny maybe I want to use this exactly but I could see it as like a starting point for actually writing a birthday message so the second trick is to give examples let's compare prompts without and with this trick without giving examples we might prompt chat gbt as follows given the title of a torch data Science Blog article write a subtitle for it here we're putting in the title as prompt engineering how to trick AI into solving your problems which is the title of the blog associated with this video and then we leave the subtitle area blank so the completion that it spits out is Unleash the Power of clever prompts for more effective AI problem solving yeah pretty nifty let's see what this looks like if we give a few more examples to try to capture the style of the subtitle that we're looking for and so here the prompt is pretty similar but now I'm putting in the title and subtitle for preceding blogs in this larger Series so here put a practical introduction to llms three levels of using llms in practice then we have cracking open the openai python API a complete beginner friendly introduction with example code and then finally we have the same prompt as we saw before so let's see what it spits out now mastering the art of crafting effective prompts for AI driven Solutions well at face value this might not seem much different than the completion that we saw before I kind of prefer this one over this one here and the only reason is because again I don't like verbose text and this is more concise than this previous one here so I think maybe that's what Chad GPT picked up on it's like oh these subtitles here have these number of tokens let's make sure that the next subtitle has about the same number of tokens just speculating but regardless that's how you can incorporate examples into your prompt the next trick is to use structured text let's see what this looks like in action so I suppose this is our prompt for tragedy BT we don't have any structured text here we're just putting in prompt without structured text so we're asking it to write me a recipe for chocolate chip cookies gives a pretty good response gives us ingredients gives us instructions and gives us some tips if Chachi PT was not fine-tuned it may not have spit out this very neat structure for a chocolate chip cookie recipe and so this is another indication of why what large language model you're working with matters because I could be happy with this response here there may not even be a need to use structured text here but still let's see what this could look like if we did use structured text in our prompt here the prompt is a little different create a well organized recipe for chocolate chip cookies use the following formatting elements the key difference here is we're now asking it specifically to follow this specific format and we're giving it kind of of a description of each section that we want so let's see what this looks like so one subtle difference here is that in the completion where we use structured text you notice that it just kind of gives the title and the ingredients and so on this is something that you could easily just copy paste onto like a web page without any alterations well if we go here there's no title which could be fine but you have this certainly here's a classic chocolate chip cookie recipe for you so now it's trying to be more conversational and may have required some extra steps if this is fitting into a larger like automated pipeline but other than that it doesn't seem like there's much difference between the other aspects of the completion one interesting thing is that here the tips are a bit more clear and bolded well here there's just some like quick bullet points next we have trick four which is Chain of Thought and the basic idea with Chain of Thought is to give the llm time to think and this is achieved by breaking down a complex task into smaller pieces so that it's a bit easier for the large language model to give good completions without using Chain of Thought this is what the prompt might look like write me a LinkedIn post based on the following medium blog and then we just copy paste the medium blog text here through some text in here so it does a pretty good job again this feels way too long for LinkedIn post and it feels like it's just summarizing the text that I threw in there but I mean it's not bad this could be a really good starting place but now let's see what this can look like using Chain of Thought instead of just having it write the LinkedIn post based on the text here I'm trying to explicitly list out my personal process for turning a Blog into a LinkedIn post and trying to get the llm to mimic that so here I put write me a LinkedIn post based on the step-by-step process and medium blog given below so here step one come up with a one line hook relevant to the blog step two extract three key points from the article step three compress each point to less than 50 characters step four combine the hook compress key points from step three and add a con to action to generate the final output and then we put the medium text here okay looking at this this seems a lot more reasonable for a LinkedIn post each line is just one sentence it's not way too much text no one likes reading a wall of text or at least I don't like reading a wall of text so this is much more helpful to me in making a LinkedIn post okay trick five is to use these chatbot personas the idea here is to prompt the llm to take on a particular Persona so let's see a concrete example of this without the trick let's just say we want chat gbt to make me a travel itinerary for a weekend in New York city so it spits out something that looks pretty good so now let's see what this could look like with a Persona so here instead of just asking it straight up for an itinerary I say act as an NYC native and cabbie who knows everything about the city please make me a travel itinerary for a weekend in New York City based on your experience don't forget to include your Charming New York accent in your response okay so let's see what this does comparing this response with the other response there seems to be a lot of overlap and maybe there's not a practical difference between these two but it does feel like there are things here that you don't get here start your day with the classic New York breakfast at a local dinner Cafe well this one will just say start with the bagel Central Park stroll Museum and grab a bagel again yeah it's just eat Bagels every single day oh that's funny I like how it injected a bit of humor here yep you guessed it another Bagel fuel of your final day maybe you really have to like read through these to get a sense of the subtle differences but maybe just from this Bagel example this just gives you two different flavors of itineraries and maybe one matches your interests a bit more than the other trick number six the flipped approach and so here instead of you asking all the questions to the chat bot you prompt the chatbot to ask you questions to better help you with whatever you're trying to do so let's see this without the trick let's say you just want an idea for an llm based application you give it that prompt and it's just gonna generate some idea for you here's generating a idea for us edu bot pros and intelligent educational platform that harnesses the power of llms to offer personalized learning and tutoring experience for students of all ages and levels so this could be a great product idea the problem is maybe this isn't something that you're passionate about or that you really care about or this idea is not tailored to your interests and skill set as someone that that wants to build an app let's see how the flipped approach can help us with this so here instead of asking for an idea just straight up we can say I want you to ask me questions to help me come up with an llm based application idea ask me one question at a time to keep things conversational you can see right off the bat what are your areas of expertise and interest that you'd like to incorporate into your llm based application idea I didn't think to say oh yeah maybe I should tell the chat bot what I know and what I'm interested in so we can better serve me and maybe there are a bunch of other questions that are critical to making a good recommendation on an app idea that I just wouldn't think of and that's where the flip approach is helpful because the chatbot will ask you what it needs to know in order to give a good response and those questions may or may not be something that you can think of all up front the seventh and final trick is reflect review and refine and so this is essentially where we prompt the chat bot to look back at previous responses and evaluate them whether we're asking it for improvements or to to identifying potential mistakes so what this might look like is here we have the edu bot Pro response from before let's see what happens when we prompt it to review the previous response so here I'm saying review your previous response pinpoint areas for enhancement and offer an improved version then explain your reasoning for how you improved the response so I haven't tried this so we're both seeing this for the first time it looks pretty similar but since we asked it to explain how it improved the responses it gave us this extra section here so reasoning for enhancements Clarity and conciseness emphasizing personalization enhanced language and then monetization strategies the monetization section provides more detail on viable strategies okay cool well I'm not going to read through this but this prompt or something like it you can basically copy paste this as needed to potentially improve any chat completion so I know that was a ton of content and I flew through that but if you want to dive into any particular trick a bit more check out the Torches data Science Blog where I talk about each of of these a bit more insight resources where you can learn more everything we have just talked about is applicable to both the easy way and the less easy way of prompt engineering but now I want to focus more on the less easy way and I'm going to try to demonstrate the power of prompt engineering the less Easy Way by building out this automatic greater example we were talking about before using the langchain python Library first as always we're going to do some imports so here we're just importing everything from langchain and then here we're going to be using the openai API so that requires a secret key if you haven't worked with the open AI API before check out the previous video that talks all about that there I talk about what an API is talk about open ai's API and give some example python code of how you can use it here we're just importing our secret key which allows us to make API calls here we're going to make our first chain the main utility of Lang chain is that it provides a ton of boilerplate code that makes it easy to incorporate calls to large language models within your python code or some larger piece of software that you're developing and it does this through these things called chains which is essentially a set of steps which you can modularize into these so-called chains so let's see what that looks like the first thing we need is our chat model so here we're going to incorporate open ai's GPT 3.5 turbo the next thing we need is a prompt template so essentially this is going to be a chunk of text that we can actually pass in inputs and dynamically update with new information so for example this is the same prompt we saw from the previous slide for the automatic grader we'll be able to pass in these arguments question correct answer and student answer into our chain and it'll dynamically update this prompt template send it to the chat bot and get back the response to put this chain together it's super simple the syntax looks like this you have llm chain you define what your llm is which is chat model which is the open AI model we instantiated earlier The Prompt is prompt which is the prompt template we created on the previous slide and you combine it all together into this llm chain and we Define it as chain what this looks like in action is as follows we Define the inputs so here we're going to define the question who was the 35th President of the United States of America we Define the correct answer John F Kennedy and we Define the student's answer FDR and so we can pass all these inputs to the chain as a dictionary so we have this questions correct answer student answer keywords and then we plug in these values that we Define up here and then this is what the large language model spits out students answer is wrong so it correctly grades the student answer as wrong because FDR was not the 35th President of the United States however there's a small problem with our chain right now namely the output from this chain is a piece of text which may or may not fit nicely into our larger data pipeline or software pipe line that we're putting together it might make a lot more sense instead of outputting a piece of text the chain will output like a true or false indicating whether the student's answer was correct or not with that numerical or Boolean output it'll be much easier to process that information with some Downstream task maybe you want to sum up all the correct and incorrect answers of the homework and generate the final grade of the entire worksheet we can do this via output parsers so this is another thing we can include in our chains that will take the output text of the large language model we'll format in a certain way extract some piece of information or convert it into some other format as we'll see here here I'm defining our output parser to determine whether the grade was correct or wrong and I just use a simple piece of logic here I have it returned a Boolean of whether or not the word wrong is in the text completion as an example before the completion was the student answer is wrong so this word wrong appears in the text completion this parser here will return false because wrong is in the completion and so this knot will flip that and it'll make it false so as you can see like we haven't automated all the logic out of programming you still need to have some problem solving skills and programming skills here but then once we have our parser defined we can just add it into our chain like this so we have our llm same as before our prompt template same as before and then we add this output parser which is the grade output parser that we defined right here and then we can apply this chain so let's see what this looks like in for Loop so we have the same question and correct answer as before who's the 35th President of the United States and then the correct answer is John F Kennedy and now we're defining a list of student questions that we may have received which are John F Kennedy JFK FDR John F Kennedy only one n John Kennedy Jack Kennedy Jacqueline Kennedy and Robert F Kennedy also with one end we'll run through this list in a for Loop we'll run our chain just like we did before and we'll print the result and so here we can see that John F Kennedy is true indicating a correct response JFK is true FDR is false John F Kennedy spelled incorrectly is true because we specifically said misspellings are okay John Kennedy is true because we're just dropping the middle initial Jack Kennedy's true it's a common nickname Jacqueline Kennedy is false that was his wife and then Robert F Kennedy is false because that's his brother and as always the code is available at the GitHub repo for this video series which is linked down here feel free to take this code adopt it or maybe just give you some ideas of what's possible with prompt Engineering in this way I would be remiss if I did not talk about the limitations of prompt engineering which are as follows like I said in before optimal prompt strategies are model dependent what is the optimal prompt for chat GPT it's going to be completely different than what's a optimal prompt for gpt3 another downside is that not all pertinent information may fit into the context window because only so much information can be passed into a large language model and if you're talking about a significantly large knowledge base that's not something that prompt engineering may be able to do most effectively another limitation is that typically the models we use to do prompt engineering are these like huge general purpose models and if you're talking about a particular use case this might be cost inefficient or even overkill for the problem you're trying to solve and another version of this is that smaller specialized models can outperform a larger general purpose models an example of this was demonstrated by open AI When comparing their smaller instruct GPT model to a much larger version of gpt3 so this brings up the idea of model fine tuning and that's going to be the topic of the next video in this series there we're going to break down some key fine-tuning Concepts and then I'm going to share some concrete example code of how you can fine tune your very own large language model using the hugging face software ecosystem so I hope this video was helpful to you if you enjoyed it please consider liking subscribing and sharing with others if you have any questions or suggestions for future content please feel free to drop those in the comments section below and as always thank you so much for your time and thanks for watching
LRLH_yIxHrI,2023-08-29T19:25:48.000000,"Why I Quit My $150,000 Data Science Job",I've never wanted to climb the corporate ladder I never wanted to be some cxo or director at a big Corporation you were getting paid well you're doing data science and you were part of a great team why would you ever leave that my goal was just to start a business and to learn am I doing what I want to be doing am I spending my time on what I want to spend my time on you can create your ideal job you can create your ideal life hey everyone I'm Shaw and this video is going to be a little different from what I typically post last month I made a pretty significant life change I decided to leave my full-time data science role and to go all in on entrepreneurship and my business I don't really know how this video is going to turn out I don't have slides like I usually do I don't have a structure I just kind of have my story and the reasons why I made my decision here I'm gonna basically talk about the reasons why I decided to make the transition and hopefully people watching this contemplating a similar decision maybe my story and reasoning gives you some helpful perspective in either deciding to stay in your current role or maybe make a transition a little bit of context so I was working as a data scientist at Toyota Financial Services I was part of a big data science group and it was a fantastic experience because never before had I worked with so many data scientists and Technical people prior to starting at Toyota I was in grad school I was getting my physics PhD and there who I worked with was essentially my research Group which was about 12 people and we're all about at the same level you know we're all grad students trying to figure out all this stuff coming into Toyota where there are many layers of experience and backgrounds it was a very rich team to be a part of and you had so many different people that you could walk up and talk to and get a new perspective ask questions learn from and for me learning is very important to me that's a big reason why I make YouTube videos and write articles about data science and other things so other than just like the broader team the team I worked with was awesome I worked with people I admired which not a lot of people can say and I feel like I grew so much and learned so much from the smaller team that I was working on this is probably making it even more confusing like Shaw you were getting paid well you're doing data science and you were part of a great team why would you ever leave that role that situation and so let's get into that the main reason like super big picture that I decided to leave was working in that role did not align with my long-term goals I've never wanted to climb the corporate ladder I never wanted to be some cxo or some director at a big Corporation ever since early days of grad school my vision for myself has always been to start a business to build a team and I eventually realized it's very difficult to do that part-time people who start businesses it's not uncommon for them to be working 60 80 hours a week trying to make that thing work so now imagine trying to do that while working 40 hours a week in a company and then trying to start this business and build this business in that additional 10 or 20 hours you can work realistically in a week so if we were to just take a step back if your goal is to start a business which of the two strategies is better suited to achieve that goal strategy one working 40 hours a week at a company and working on the business 10 hours a week or option two working 40 hours a week on the business and then trying to get money through gigs and other means to support yourself 10 hours a week I just came to the realization that option two made a lot more sense for me so that's reason number one working in the role no longer made sense given my long-term goals and aspirations the second big reason why I left was the same essentially I was too comfortable like I said before learning and growth is very important to me I think of it like this growth is essential to life so just think of a tree what does a tree do a tree grows and if a tree stops growing what is it doing it's probably dying so the way I see it growth is essential to a meaningful life and the thing about growth is that it's uncomfortable the converse of that is if you're comfortable you're not growing you're not learning there's no stress from the environment that's telling you that hey you need to do something you need to learn something but don't get me wrong when I first started the role it was very uncomfortable there was a lot of new things I had never worked corporate before there were a lot of new Norms new people New Concepts new problems it was a lot of learning early on but what happened was as time went on it started to like asymptote to a level of comfort and I'm not saying that I figured everything out and saw evolved data science or solved corporate or anything like that there's still a ton I could learn about being a data scientist doing financial services but kind of taking that step back again the question I have to ask myself is do I want to be a data scientist doing financial services at a big Corporation is this a skill set that I care to get to that expert level ultimately the answer was no because again my goal wasn't to become the best data scientist ever my goal wasn't to become VP of data science or anything like that my goal was just to start a business and to learn looking at the two options on the table corporate data scientists versus entrepreneur which option better aligns with those goals of starting a business and learning and ultimately it was just option two because taking the leap and trying to figure it out with no guarantee of income seemed a lot more uncomfortable than trying to figure it out making six figures this is a really important point because because if humans are good at anything it's adapting to their environment if your environment is risk-free comfortable having more money than you need in your bank account you're going to adapt to that environment on the other hand if you are very uncomfortable and need to figure out how to generate income to pay your bills and basic necessities you're going to be a lot more motivated in the second scenario this is the lesson that I kind of learned observing my parents who came to the U.S from Iran with very little money and opportunity but they just figured it out and it's funny because I was talking to my mom a while back about how life was when we were growing up it's like how'd you do it my dad was working full-time she was working full-time she was going to school and she was raising two kids I'm like how did you do it and she said I don't know I don't know how I did it and I think that's a testament to what we're capable of doing when we have no other options when we're backed into a corner we unlock block a level of performance that we did not know we were capable of and the best way to bring that out is to put yourself in a situation where you have no other option but to make it work so the third big reason and I'll probably just cut it here is the timing seemed right at work there was a bit of a lull we were transitioning projects and so it was less dependencies on me and the work could be better distributed to other people so it felt like a good time to make the transition because I've worked on a lot of different teams in a lot of different situations and I've observed when someone abruptly leaves and people are left like holding a bag and like scrambling trying to figure it out I mean there's always going to be that to some extent there's never going to be a perfect transition but it felt like good time to make the transition without throwing anyone into a bad situation perhaps more importantly I saw an opportunity so a bit more background in grad school I started doing freelance data science work and that's when I first realized that I don't need to work in a full-time role to make income I'm able to go to upwork apply to jobs get gigs that way that was an important point for me because I knew that even if I didn't have a full-time job I can always make money by going to upwork and picking up gigs another thing I started doing in grad school was making YouTube videos and writing blogs doing that for about three years now and to my surprise I've actually seen some traction and grown a small audience on these platforms an opportunity that this unlocks is that I often get people reaching out to me that want to hire me to do data science Consulting work and so I was incrementally seeing the number of reach outs increasing near the end of my time at Toyota so I knew I could make money on upwork I was seeing more traction of inbound leads coming from my content and on top of that I was generating a little bit of Revenue as part of the the medium partners program and YouTube's partners program and so all these elements together you know the content and the freelance it kind of added up to okay I can survive with the income that I generate through content and freelance work but on top of that since my income at Toyota was way more than my expenses I was able to save a Year's worth of expenses so even if I don't make a single dollar for 12 months I'll still be able to survive all these elements I can always go to upwork I got inbound leads coming in I've got a Year's worth of expenses in savings these all contributed to making going all in on entrepreneurship now seem like the right decision and a good opportunity so I cut the reasons there even though there was more that went into the decision just to recap the three biggest reasons why I decided to make the transition to entrepreneurship was one it was more aligned with my long-term goals to do is seem like a much more effective way to grow and develop the skills that I actually care about and want to develop and three the timing and the opportunity just seemed right so I've been full-time entrepreneur for about a month now you might be wondering am I dead yet what's going on how are things going another big surprise to me is that things are going according to plan this was my sketch of a plan leave and then in that first three months make as much content as possible because through making content I get more inbound leads that's good because it's much better if a client reaches out to you than if you apply to a job on upwork as a freelancer just because well one it's kind of less work you don't have to sift through a bunch of job postings and apply to them and go through interviews and all that kind of stuff but typically when clients reach out to you they're typically a better fit with your skill set and mindset just because I think you can pick up a lot lot about someone through watching them talk in a video or reading an article that they write and the goal with that is by the end of Q3 so right now it's August 2023. the goal is to land one single client that came through inbound seems like that's actually going to happen earlier than that someone that I've been talking to wants to move forward and start working together so that's good that's the first three months and the plan for Q4 is okay try to keep up the content but start prioritizing the freelance work a bit more just so you can maintain Financial Security and all that kind of stuff but ultimately for me the goal isn't to just be a freelancer while freelancing is a business and can be rewarding because you have a lot of freedom on the types of projects you take on who you work with where you work how you work etc My ultimate goal is to build a product and grow a business around that product I think a big reason why that sounds so appealing to me is because it feels like a big Challenge and I want to see if I can do it on some level but also I'm a big believer in the value of a team I truly believe that a good team where everyone's working cohesively and moving in the same direction can accomplish anything I've been part of some amazing teams and it's something that is so satisfying and fulfilling I think that Taps into something very fundamental about being human we are incredibly social creatures so if I'm a solo freelancer I miss out on that team aspect but I feel like if I can build a product growing a business around that product will require a team because I obviously can't do everything and so that's a plan the rough timeline is try to get something within two years right now freelance is just to pay the bills I'm freelancing just so I can survive but the main goal is figure out the product figure out that thing that I can feel build a business around so one month then I definitely do not regret the decision maybe I'll feel differently later but right now things are good everything's going according to plan somehow and it really feels like I'm living my dream life we'll see if I feel the same way in 8 to 12 months where the number and the savings starts getting smaller and smaller but really I think the realization for me is that you don't need to be a millionaire to live your dream life you don't need to wait 20 years to live your dream life you can just Live Your Dream Life now maybe it's not a hundred percent of what you envision but even if it's like 20 I think that's worth pursuing because at every step you can just gain an inch and just slowly incrementally get to your ideal life all that to say that my dream life was a lot closer than I may have thought because it doesn't really take a lot for us to be happy once all the basic needs are met then the next obstacle is am I doing what I want to be doing am I spending my time on what I want to spend my time on while some are lucky enough to work in a job where all the work that they do is exactly what they want to work on and what they want to give their attention to and what they want to dedicate their life to I feel the majority of people do not fit nicely into any existing job that's why I feel entrepreneurship is such a natural thing because you can create your ideal job you can create your ideal life based on your goals and your values if it's really important for you to travel all the time you can build a business that allows you to travel all the time if you want to work very odd hours from like 6 p.m to midnight every single day then you can build a business that allows you to do that I truly believe that you can create a role that is a hundred percent aligned and 100 matches your unique skill skill set Ambitions values goals Etc and it's just a matter of going toward that I feel like there are a lot of people that are in a job maybe they don't like it or they just kind of like it but it's not their ideal role it's not their ideal life that they're living and they have this entrepreneurial mindset they want to start a business but there's so many different things that could be holding them back one of the big ones is just doubt and uncertainty and a motivation for me is okay I'm going to do this and I'm going to document the journey and hopefully people who are on the fence and in a similar situation they'll be like this isn't so crazy this random guy on YouTube did it why can't I do it and that is exactly right why can't you do it you can do it we can all do it anyone can be an entrepreneur anyone can be successful it's just a matter of going after it and putting in the work and being willing to be uncomfortable okay so I'll stop there that was a long rant got an hour of footage here we'll see what it gets edited down to but if you've made it to the end I hope you found some sort of value through this rant and monologue that I just went on do you have a similar story similar Journey I'd love to hear it please drop it in the comments section below and as always thank you so much for your time and thanks for watching
jan07gloaRg,2023-08-10T19:48:23.000000,The Hugging Face Transformers Library | Example Code + Chatbot UI with Gradio,hey everyone I'm Shaw and I'm back with the third video in the series on using large language models in practice in this video I'm going to be breaking down a hugging face Transformers Library which is a python library that makes working with open source large language models super easy I'll start by explaining some key Concepts before diving into some concrete example code and then at the end of the video we're going to see how we can spin up our very own chatbot UI using Transformers and gradio and with that let's get into the video so in the previous video of the series we were talking all about the open AI python API and what this API allows you to do is to programmatically interact with open ai's language models so you can build tools or if you want to build some kind of product or service however one obvious downside is that API calls cost money so in some situations where this cost might be too prohibitive we can turn to open source Solutions one way we can do this is via the hugging face Transformers Library which is what I'm going to talk about today so you're probably wondering what is hugging face well it's more than just an emoji on your phone hugging face is actually an AI company and they've become a major hub for open source machine learning in the past few years so there are three key elements to hugging faces ecosystem which is largely contributed to its recent popularity the first one is its models there are hundreds of thousands of pre-trained Open Source machine learning models freely available on hugging face second is their data sets repository so these are open access data sets that developers and practitioners can grab to train their own machine learning models or to fine-tune existing models and finally is hugging face spaces which is a platform that allows users to build and deploy machine learning applications so while these three aspects of hugging faces ecosystem have made developing machine learning models more accessible than ever before or there's still another key element of the ecosystem worth mentioning which is the Transformers Library so Transformers is a python Library developed by hugging face that makes downloading and training machine learning models super easy so while the library was originally developed specifically for natural language processing its current functionality spans all different domains from computer Vision Audio processing multimodal problems and more so just to give you a flavor of how easy it is to get started with the Transformers Library let's look at a concrete example suppose we want to do sentiment analysis you can imagine that there could be a lot of steps involved in doing sentiment analysis so first you need to find the model that is able to do this classification task then you'll need to take some raw text and convert it into a numerical representation that you can pass into the model and then you need to kind of decode the numerical output of the model to get a label for the text input and so while this might sound like many different steps and could be very complicated this can all be done in one line of code in the Transformers library and so this is possible with the pipeline function as you can see the syntax is super simple here so we have this pipeline function we just need to specify the task we want it to do so here we put sentiment analysis and then we pass to the pipeline function a text input so here I put love this and then if we run that the output of this line of code here is a label of positive and a score associated with that label so this makes sentiment analysis super easy but of course sentiment analysis is not the only thing we can do with the pipeline function you can also do summarization translation question answering feature extraction text generation and many many more if you want the full list it's available on hugging faces documentation this is the link down here and I'll also drop it in the description below going back to this one line example here it almost feels like magic because we didn't give it a model we just said hey do sentiment analysis and apply it to this text here we could have been a bit more explicit here and specified the model that we wanted to use to do sentiment analysis and so to do that it's very simple we just specify a model using the syntax here we're using distillber base uncased fine-tuned SST to English so this is actually the default model in the Transformers library that was used before so that's why we have the same exact output but what really makes Transformers powerful is we could have put any one of the thousands of text classification models available on hugging face and so to explore these models we could have gone to huggingface.co models which is a growing repository of pre-trained Open Source machine learning models for things such as natural language processing computer vision and much more so let's see what this looks like so we'll click on here and I'm going to zoom in a little bit we can see here there are currently 200 184 000 models on the platform and if we look on the left here these are for all different types of tasks they're these categories multimodal computer vision natural language processing audio tabular reinforcement learning and so just now we were doing sentiment analysis which is a type of text classification so we can see what other models we could have used here so just zooming in there are over 28 000 models we could have used for text classification but notice we can add additional filters to narrow down the models we want to pick so let's say we want to make sure we can use the model easily with the Transformers library in that case we can just click on this Transformers filter and then it'll narrow down even more so we went from 28 000 models to about 27 000 models so still a lot of different options we can also go further we can specify the data sets we want it to have been trained on so for example if we wanted to be trained on PubMed because we wanted to use this for a medical use case and you know so on languages licenses so I guess this is important if you have a specific use case in mind for example you want to develop a product for commercial use you want to make sure that the license kind of aligns with what you're trying to do and then there's some other filters as well so let's just say that we're fine with any text classification model that is compatible with the Transformers library then we can kind of explore from here right now it's sorted by trending but we can also sort by number of likes number of downloads recently updated so the number one most trending one looks like it's from the bloke and the model name is llama270b guanaco Q Laura fp16 so we can go ahead and click on that and if we click on that we get the model card just to kind of explore this a bit here's the organization or the individual that created the model this is the model name we can easily copy the model name if we want to paste it into our pipeline function we can do that very easily also we have all the different tags for the model here we have the text classification tag it's available in Transformers which is what we filtered on but also this model is compatible with pytorch this is an important point because the models on hugging phase aren't only for the Transformers Library they are also for many other popular machine learning Frameworks so if you're not using the Transformers library but using pi torch hugging faces models repository can still be a very helpful resource so another cool thing about the model card is that there are these like quick start buttons here so we can get a quick start for like fine-tuning the model on Amazon sagemaker this will give you some code to jumpstart your model fine tuning and let's say you don't want to just do text classification you want to do text generation or token classification this will give you some example code to get you started also there's deploy so you can deploy this model using Amazon sagemaker and then also you can use in Transformers so this is the same syntax we saw in the slides where you're just specifying the task you wanted to do and then just specify model and then coming out of that there's just a bunch of general information about the model laid out here now you can imagine that we have these model cards for hundreds of thousands of models on this platform if you're trying to build some kind of machine learning app it's really never been easier to get started you can just use Transformers to load in any of these state-of-the-art open source language models and just start building from there so hopefully you have a little bit of a sense of what you can do with the Transformers Library your next thought might be like how do I get this on my machine and start using it so the standard way to install the library is via PIV and hugging face has a great guide on their website on how to do this and so I'm not going to walk through the PIP installation steps here however I will walk through a conda installation specifically for the example code that we will see in the following slides there's two steps the first step is head over to the GitHub repository and download the HF Dash EnV yaml file so here's the GitHub this is linked in the description below here we have the HF Dash env.yaml file it's a yaml file listing out all the dependencies for the example code and here we of course have the Transformers Library along with all the other dependencies and so once you download that this next step is to execute the following two commands in your terminal or anaconda command prompt depending on the machine you're using two commands here one is you're going to change directories into wherever you have this HF env.yaml file saved and then you can create a new content environment using this command here so conda envcreate dash dash file HF env.yaml it'll probably take a few minutes to download all the dependencies but once that's done you should be good to go and you shouldn't get any kind of Errors running the following example code so while Transformers does more than just NLP these days and the example code here we're just going to focus on NLP tasks first we'll start with sentiment analysis so before we saw something like this where we use the pipeline function to do sentiment analysis and we specified the model but what's different here is instead of just doing it all in one line we use the pipeline function to create this classifier object and then we can take this object and pass in text to it and it'll generate an output like this so if we pass in the text hate this it'll spit out a label negative and a score associated with that label however you're not limited to just passing one input at a time into this classifier object you can actually pass in a list and it'll do a batch prediction for all the elements in that list so for example we have a text list here so this is great thanks for nothing you've got to work on your face your beautiful never change and so we pass these into the classifier and we get the following labels so it says the first one is positive since the second one is positive even though I sent some sarcasm there the third one is negative and then the fourth one is positive but of course there are more models than just this default one on hugging face that we could have used so one example is Roberta bass go emotions by Sam Lowe so the difference with this model and the default model is that the model here has several Target labels that it uses for text classification and so just using the same exact syntax we can create a classifier object using the pipeline function and then we can apply the classifier object to the first element in our text list defined before and so you can see now instead of just positive or negative there are tons of labels so there's admiration approval neutral excitement gratitude Joy curiosity so on and so forth and actually there are even more but I just cropped the image because there's just too many super easy to do sentiment analysis with Transformers and super easy to swap out this model with any other model you like on the hugging face platform another thing we can do of course is summarization so even though this is a completely different task the syntax is very similar so we use the pipeline function to specify the task and the model we want to use and create this summarizer object then we Define the text that we want as input into this object then we pass it in along with some other input parameters so here we're defining a minimum length the maximum length and then we do a couple things to retrieve just the summary text so this is text from the blog associated with this video it's just talking about hugging face and what it is and it takes this couple paragraphs of text and it reduces it to the following two sentences hugging face is an AI company that has become a major hub for open source machine learning they have three major Elements which allow users to access and share Machine learning resources and of course you can chain together multiple objects so for example you can bring together summarization and sentiment analysis by passing the summarize text into our classifier from before and we can generate all these different outputs and so really these become like Lego blocks and you can just piece together very easily very quickly these different NLP tools for whatever particular use case that you're working on another NLP task we can do is conversational text the syntax is slightly different because in a conversation like with a chatbot there's a bit of back and forth but we started with the same exact syntax so we can use the pipeline function to specify the model that we want to use to create this chat bot object and then we can use this conversation object to handle the back and forth between the user and the chat bot how we do this is we pass in the initial user prompt into this conversation object and then we save it as conversation then we can update this conversation object by passing it into this chat bot object and we can print the result and so what this looks like is the user says hi I'm Shaw how are you and then the chatbot says I'm doing well how are you doing this evening I just got home from work then to keep the conversation going we can use this add user input method so here we're adding a follow-up question where do you work and again we pass the conversation into the chat bot and then Auto magically the chat bot will generate a response and it'll update the conversation object and we can print it all out here so follow up where do you work then the chatbot says I work at a grocery store what about you what do you do for a living and all of this is just running locally on my machine no need for API calls no need for cloud resources these models are small enough that it's just running locally okay so while this is giving us very powerful functionality this is a very awkward way to interact with a chat bot so let's see how we can spin up a user interface to make this a bit more intuitive we can actually do this very easily using a library called gradio in just a few lines of code so first I'm initializing these two lists so one list is to store the user inputs and another list to store the chat bot responses and then we Define a function I call it vanilla chat bot here and it has two inputs it has the message which is essentially the user input and the history which is just the history of everything that's happened in this python script it is automatically generated and so in this vanilla chatbot function we use the same exact syntax we used in the previous slide so we have this conversation object we're passing in the message so this is the user input but also we can pass in the context of the conversation so the chat bot knows what the back and forth has been up until this point the way we do this is we pass in the message list and the response list so all this goes into conversation and we pass the conversation to the chatbot who will generate a response appendix response to the conversation and then we will return the latest generated response from the conversation object and then in basically one line of code we can spin up the chat interface with radio they have this chat interface base object that we can readily use so the first input of this object is the function we just defined the vanilla chat bot and then we can Define some other things like the title of the user interface and then a description for the user interface so all this gets stored into this demo chatbot object and then we simply can just launch it if we do that it'll start running locally at this URL here so if it doesn't automatically pop up in your browser you can just copy paste this into your browser also you can create a public link to this chat interface which is pretty cool you know you can spin this up and then you can send the link to someone and they can actually access the application that you made locally so let's see what this demo looks like here's the chat bot running if you run it in Jupiter lab it'll actually spin up the UI in the notebook or you can just click on this which will open a new tab and then you can start talking with the chatbot so let's just talk to the chat bot gotta response pretty quickly hello how are you doing today I just got home from a long day work so this chatbot for some reason always wants to throw it in our face that it had a long day at work and it seems like it wants me to ask what it does for a living so this one might take a bit more time but we'll see oh Works in a warehouse it's pretty boring but pays the bills how about you there's some other things here you can like have it redo its output you can undo you can clear it it gave a different response that's cool I'm gonna cashier to the grocery store isn't the most exciting job in the world so it doesn't really matter what this chatbot does for a living it just doesn't enjoy its work so that's pretty cool however we can take this one step further and instead of Hosting this chat bot locally we can host this chatbot on huggingface Via hugging face spaces essentially spaces are just git repositories that are hosted by hugging face and they allow you to create machine learning applications and so let's see how we can do this go to space's website you know we'll actually find a lot of existing applications so let's see this is a really popular one so this is the open llm leaderboard so we can see them ranked here with all these different metrics and you can filter as you like so that's pretty cool these are just open source machine learning applications that anyone can access so you can actually look at the source code by going over to files here and then you can see how they did it you can also clone the repository you can run with Docker so it makes it really easy to not only deploy your applications but find applications that are already on Spaces to use as a starting point but going back to spaces if we want to create a new space we just go here click create new space you can create a name we'll call it vanilla chat bot license doesn't matter we'll use radio as our SDK we'll make it public since this is hosting the application it needs computational resources so you have a few different options here you can have gpus or just CPUs of course A lot of these cost money so we'll just stick with the free version for this example we'll hit create space next you'll see something like this you it's giving us some nice instructions on how to upload our application to spaces so first it wants us to clone the repository create our app.pi file so this is where our application is going to go and then pushing our code to the repository and so an important note here is you need to add requirements basically what libraries are necessary to run your application when you do to make the push so let's see what this looks like we can clone the repository just copy paste this into terminal and we got it here so I already have the files ready to go in a different folder called my first face and so you can see that they're there if we look in the repo we can see it has a readme file already so all we need to do is add this app.py file and this requirements.txt file so I'll do that on my other screen we do that again we should see those files there okay so we clone the repo we added our app file and requirements file and now we just commit and push so here OBS decided to stop working so I lost audio but you can see me pushing the code to the git repository now I'm waiting around for the image and app to spin up then it finally did spin up and we see that we have this nice interface completely hosted on hugging face spaces the chatbot indeed does work but it was significantly slower here than it was when I was running it locally and since this is publicly available you can actually go and interact with this very same chat bot using the link on the screen and also in the description below so that's basically it I hope this video and demo has been helpful to you and given you a flavor of what's possible with the hugging face ecosystem while it does seem like we covered a ton of information and content we've only really scratched the surface of what's possible with the hugging face Transformers library and broader ecosystem so with that being said in future videos of this series we will explore more sophisticated use cases of the hugging face Transformers Library such as how to fine tune a pre-trained large language model as well as how to train a language model completely from scratch couple other things I'll call out is that there is a Blog associated with this video published in towards data science they're definitely details in there that I probably left out of this video and of course as always all the code that I covered in this video is available on the GitHub repository linked here and in the description below and if you enjoyed this content please consider liking subscribing and sharing with others and as always thank you so much for your time and thanks for watching
czvVibB2lRA,2023-07-28T15:24:51.000000,The OpenAI (Python) API | Introduction & Example Code,hey everyone I'm sure and I'm back with the second video in the series on using large language models in practice in this video I'm going to talk all about open ai's python API so I'll start with an overview of what an API is and other key Concepts before diving into some example code and showing you how to make your very own chatbot so with that let's get into it like I just mentioned this video is going to be all about open ai's python API and the goal of this is to serve as both a complete and beginner-friendly guide so here's what we're going to talk about today I'm going to start with what's an API then I'm going to move on to specifically open ai's python API I'm going to talk about how to get started setting up an account and other things like that and then finally we'll get into some example code going over both the basics and showing you how to make a chat bot with the API so the million dollar question what is an API I feel like this is just thrown around in so many different contexts said people often be familiar with the term but not so familiar of what it actually means and so an API stands for application programming interface and here I'll Define it as a way to interact with a remote application programmatically so while that might sound very technical and very scary it's not let's just consider the following analogy suppose this is you and you've got a craving for some Salvadorian pupusas that you had during your Latin America trip from last summer however you're back home and you don't know where you can find pupusas in your hometown but lucky for you you have a super foodie friend that knows every single restaurant in town and so if you were trying to figure out where you can go find great pupusas you might send your friend a message hey any good pupusa spots in town and then you know maybe a few minutes later your super foodie friend would respond yes flavors of El Salvador has the best pupusas and so basically what just happened here is you sent your friend a request and then a little bit later your friend sent back a response and while this might sound completely irrelevant to programming in Python and what we're talking about here this is essentially how apis work what this can look like in the context of open ai's python API is instead of texting your friend your request you can send open AI a request using Python and then openai will send back a response which you can capture and use for whatever Downstream task you like for example let's say you want to send a API request to one of openai's language models and if you saw the previous video of the series you saw that a core task of language models is given a string of words to predict the very next word and so let's say we send open AI this string listen to your and we want to get back the very next word the response we may receive could be hard and so as we can see this is very similar to the chat GPT web interface but instead of going to their website typing in a prompt in the UI you're essentially doing the same thing but you're sending over your prompt using Python and this API as opposed to natural language and using their web interface and so you might think what's the upside of doing this way this just sounds like the same thing with more steps while there is a lot of overlap there are unique features in the API that are just not available in the easy user interface that is chat apt one example is a customizable system message this is essentially a message that you can use to set the tone for your chat bot basically set the context give it its motivation its task for chat GPT what this might look like is I am Chad GPT a large language model trained by open AI based on the GPT 3.5 architecture my knowledge is based on information available up until September 2021 today's date is July 13th 2020 23 and so you can imagine that you want to make your own chatbot and you don't necessarily want it to be called chachyvt and you maybe you wanted to do something more specific than this and so you can really set that with the system message some other things are you can adjust input parameters such as the maximum length of the response that is sent back from the language model also the number of responses that you get back so in chagy BT you'll type in a prompt and you'll just get one response but what if you want to get 10 responses and compare them programmatically this is something that would be a bit more time consuming using chat apt as opposed to using Python and then finally you can adjust the temperature which is like the randomness of the response generated by the language model but we'll get more into that later in the video also you can process other types of input not just text so images or other file types you can extract helpful word embeddings for Downstream tasks if you go to chat gbt you input text it'll spit text out but let's say you want a numerical representation of the input text so you can use that for some Downstream processing you can also pass an audio to the API for transcription or translation tasks and then finally there's some model fine-tuning functionality built into the API so even though it may seem like more work to use Python to interact with GPT 3.5 or gpt4 you get all these additional features using the API that just are not available with the chat GPT user interface and then it's not just GPT 3.5 or gbt4 that is available to you there are also a bunch of other models and so here's a quick snapshot of them feel free to pause the video if you actually want to read this this is also available in the blog associated with this video if you want to read more about these models additionally there is a full documentation on open ai's website linked here before getting into how to actually use the API I think one thing at the top of a lot of people's minds when it comes to a paid service is how much is this thing going to cost me and so in order to understand that we first need to understand the concept of a token which here I'll Define as a set of words and characters represented by a set of numbers so suppose we have the text the end with a period at the end while you and I as humans can read this and understand it unfortunately a language model or more specifically a neural network does not understand this text directly language models understand numbers so we need to translate this text into a numerical representation so that the language model can actually understand it and do something with it so let's say we translate the end into this list of integers so what we did is we took text and we translated it into numbers but what do these numbers actually mean and so let's say the relationship between the text and the integers here is shown by this mapping here so the word the is represented by 73 a space followed by the word end is represented by the number 102 and then the the period is represented by a six these three things that make up this text are tokens obviously these won't be the only tokens we have we'll actually have a huge set of these words and characters and their corresponding numerical representations and this will make up the set of all our tokens and the vocabulary that we're using and so the reason this is relevant to pricing is that pricing with openai's API is based on the number of tokens sent to the API and the number of tokens returned so that means bigger prompts and bigger responses will have larger costs additionally there are different costs per token associated with different models and you can read more about that on openai's website linked here okay so now let's start to see how we can actually use the API to do something but before we get into the example code we first have to do a few steps to get our account set up and get into a place where we can actually start making API calls and so there are four steps here first we need to make an account second we need to add a payment method third we can set usage limits and then finally we need to get our secret key to actually make the API calls and so for this I'm going to switch over to open ai's website which is here so the URL is platform.openai.com overview and the first thing we want to do is make an account the way to do that super simple go up to this top right corner click the sign up button and then you can create an account using your email address or continue with any of these continue with Google Microsoft or apple if you've used Chad gbt in the past odds are you already have an openai account and if that's the case you can just hit this log in button here and since I already have an open AI account I'll just go ahead and log into mine here we go now that you've made your account or that you're logged in now we can go to the next step which is to add a payment method how to do that is you can click on your profile in the top right corner and then you can click on manage account to add a payment method go to this billing tab here and then then there's this thing called payment methods so you can click on that and add a payment method pretty easily the next step is to set usage limits so this is something I recommend this will help you set hard and soft limits to how many API calls you can make this will just ensure that you don't spend any more money than you expect on API calls there are two options here there's a hard limit and a soft limit the hard limit basically once you hit that number so in my case I put it as five dollars once I hit five dollars any additional API call will get rejected so you will not be charged more than this number here and then there's also a soft limit and basically when you hit the soft limit openai will send you an email hey you've hit your soft limit threshold and so this kind of helps you make sure you're not going off the rails with API calls for a little context I spend a few days playing around with the API writing example code for this video and so on and I spent a whopping three cents so the API calls are pretty cheap unless you're deploying a product where you have many users making API calls on a specific account that's when this number will probably go up a lot but I would imagine for personal use you probably will not hit more than a dollar if you're not really using this API a whole lot and then finally you need to get your secret key and so for that click on this API Keys thing and you'll see a screen like this if you've never done this before you won't see any API Keys here so you need to create a new one and the way to do that is really easy you just click this you can say new secret key I'll give it whatever name you want then you can click this create secret key button and then it's going to create this secret key for you it's important that you copy this and save it somewhere safe and accessible to you because this is the only time you're going to get access to this key just showing that if I hit done the key I just made it just shows me the last four digits of it so I don't know anything else in between so if I didn't copy it then I wouldn't be able to use it so now that we've made our account added a payment method set usage limits and obtained a secret key and we can now finally start start coding so starting with the basics we need to import the open AI python Library if you haven't installed this super simple just pip install open Ai and it should download it automatically in your terminal this line here I'm importing my secret key so what I did is I created a python file called SK dot pi and in there I just Define one variable which is called my SK and I set that equal to my secret key we can see what that looks like here so I have this Jupiter notebook and then I have this SK dot Pi if we open that we have this python file with just one variable my SK and it's equal to a string and so here is where you will copy paste your string I copied it from earlier so I'll just throw it there so you you should never do this you shouldn't just make your secret key visible to people on a public medium like YouTube but I'm just going to revoke this key right after so you won't get the chance to use my five dollars of API credits so this is just like a clean way to do it you just import the key and then you can copy paste whatever key you want in here you can revoke access create a new one paste it here without ever having to change your code and then you can set what API key you want to use using the syntax here so openai.api key you just set it to my secret key alternatively if you don't want to go through all this I've created another file and import the secret key like this you can always just manually paste your secret key right here so we got that set up and now we can make our first API call the way to do this is really simple so we have this chat completion module and then there's this method called create so here we are just passing in two inputs we pass into the model we want to use and when you pass in the messages taking a closer look at this messages thing here messages is going to be a list of dictionaries here we have a list with only one element so this element is a dictionary and then the dictionary has two key value pairs so the first one is we define the role of whoever is saying this message so here here we say user alternatively you could put system to define the system message or you could put assistant to Define some text generated by the chatbot then you define the content so this is essentially what that role said and so here keeping with the listen to your example we're going to pass that in as content from the user essentially this is our request to the API it is all baked into this method here what we get returned back is actually the response from the API and so this will be in a Json format which is essentially like a dictionary so we can print it and we see that it has a bunch of different fields in it there's this ID object created model choices usage so on and so forth most of these are not important to what we're talking about here if you want to know what each of these mean I have a definition for each of these in the blog associated with this video published in towards data science but the only important field for the discussion here is this choices feel because that is what is going to hold the response from the language model in this case GPT 3.5 turbo if we take a closer look we can see that choices will in of itself have an index field and a message field and then the message field will have role and content fields in them basically we just want to extract the content and to do that we can use the syntax here chat completion is this dictionary like object here we can access the choices which is this first field here we can get the First Choice essentially so index 0 then we can access the message of this first choice so now we're here and then finally we can grab the content which is this line here it is very hierarchical but that kind of helps keep all this information very organized so that was our very first API call but this wasn't anything special this is equivalent to going to chat GPT and just typing in listen to your this doesn't make use of a lot of the other features of the API so let's see what else we can do so one is we can set the input parameters Max tokens which will adjust the maximum number of tokens that are allowed in the response from the language model let's say I just want a one word response so what I can do is have the same exact API request but I'll add this Max tokens parameter and set it equal to one if we do that and print it out we actually get the same exact response but notice there's no period so what that tells us is heart is actually a token in the vocabulary of GPT 3.5 turbo and the period is another token let's see what else we can do next we can set this n parameter which controls the number of responses sent back from the language model so in this case we have essentially the same API request as before now we're setting the max tokens equal to two and then we're sending n equal to 5 and so what this will do is return back five chat completion for us we can get those back and we can print them in this for Loop here and then we can see we get heart heart and heart again a new line character I'm guessing then heart comma a new line character Heart comma and so notice these aren't all the same thing and this could be a good thing or a bad thing depending on the use case for example if you're trying to create some AI tool that'll help you in some creative tasks you know maybe you do want these to be a little random and out there on the other side maybe you're automating some business process or standard process and you want it to be very deterministic you want the output to be very predictable and identical every single time it's called and so it turns out we can control this degree of Randomness or predictability using the temperature parameter temperature is this input parameter that essentially controls the randomness of the response and this will take values between 0 and 2. so a temperature value of zero will make the responses very deterministic so very predictable while on the Other Extreme a temperature equal to 2 will make the responses very random and out there to see what this looks like we have the same exact API call from the previous slide but now we're setting the temperature equal to zero so we're making this very predictable and deterministic and lo and behold we get the same exact thing for all five responses heart period heart period heart period basically this is the most probable two tokens that will follow the text listen to your but let's see what happens when we crank the temperature up so let's set temperature equal to two same exact API call and now we get some really interesting chat completions first we get listen to your judgment make sense listen to your advice okay that's good listen to your dot inner awareness interesting listen to your heart so notice we'd still got this most probable outcome so raising the temperature doesn't necessarily mean you get low probability and why old responses it just means that those low probability responses have a higher likelihood of actually showing up in your chat completion and then finally we have gingist which I don't know what that means so listen to your gangist okay so those were the basics of open ai's python API now let's see what a more advanced example could look like the code that we just went through and the code that we're about to go through is all available on the GitHub repository and so this is essentially a code we just ran through and here's the setup our first API call the max tokens the number of chat completions the temperature stuff and now into the demo let's say for whatever reason we wanted to create a lyric completionist system this could be some service where people know the first line of a song but don't remember the lyrics for the rest of the song so they'll just put in the first line and then this lyric completion assistant will fill it in or maybe they're writing their own music and they are kind of getting stumped on what the next line in the lyrics should be so they can use this lyric completion assistant to help them do that creative task with this chat completion API making a custom chatbot is like stupid easy basically all we have to do is Define these messages that will precede the conversation that the user will have with the assistant so there are a couple ways we can do this one we can use the system message to set the context for the chat bot or we can have some example back and forth between the user and the assistant so the chatbot gets an idea of how to respond to user requests or of course we can do both which is what we're doing in this example here we set the system message as I am Roxette lyric completion assistant when given a line from a song I will provide the next line in the song so that's the system message and then we have two example back and forth between the user and the assistant so the user will say I know there's something in the wake of Your Smile the assistant will say I get a notion from the look in your eyes yeah the user will then say you've built a love but that love falls apart and then the assistant says your little piece of heaven turns too dark then we'll will pass in one last message which is the same message we've used throughout this entire demo which is listen to your and then we can see what the chat bot spits out so this code is essentially doing what we did before you know we're making the API call using GPT 3.5 turbo instead of a list with just one dictionary we have this messages list which actually has five dictionaries in it here we set the number of responses to one and we set the temperature to zero so it becomes very predictable here's where a little magic is happening so we're going to print the response from the chatbot but then we're gonna take the response from the chatbot and we're gonna append it to the end of the messages list which will then get fed right back into the chat bot so essentially the chatbot will just loop on as long as we want going off its previous responses and so here's what the output of all that looks like so we passed and listened to your and then the chatbot said heart when he's calling for you listen to your heart there's is nothing else you can do I don't know where you're going and I don't know why but listen to your heart before I tell him goodbye these are powerful lyrics from this chat bot and they actually correspond exactly to the hit Roxette song listen to your heart and whose actual lyrics we can see here but here's a little bonus so let's see what happens when we crank the temperature you know what if The Roxette song went a little differently so here we put temperature equal to zero so we have listened to your I reach into the Shadows summon sweet Elaine pointing all steel values fails if friends remote empty reply new line character image existing long seconds confirm flesh pressed secretly remember Saint talk dying to unfamiliar pieces father blessed speech keeps passing shape raises you travel feeling Shadows thriven body swept Spirit consume so not entirely sure what that means so this could be some you know new genre of Art postmodern techno chat bot AI art or it could just be meaningless gibberish so you can't really tell with this kind of stuff so again example code is available on the GitHub repository shown here and linked down in the description below and so what's next in this video series you know one obvious downside of using open ai's API is that you have to pay for these API calls which if you're building some product or service it may not scale well because if you have thousands tens of thousands or more users these costs can start adding up pretty quickly so in these instances we can turn to open source Solutions where you don't have to pay for API calls you have some other way of Hosting these language models so this is where the hugging face Transformers Library can help hugging face is a AI community that is building the future this is just a screenshot from their website which is also listed here hugging face maintains this python Library called Transformers which essentially makes downloading and training powerful pre-trained language models really easy and so in the next video this series I'm going to be talking all about that and sharing example code and so if you enjoyed this video please consider liking subscribing and sharing with others if you have any questions about large language models or the openai API please feel free to drop those in the comment section below this whole series is part of my Learning Journey and process so I do find a lot of value in the questions that I receive because it prompts me to dive more deeply into the stuff which I really enjoy and as always thank you so much for your time and thanks for watching
4oUOJ37GKYE,2023-07-24T13:50:51.000000,"The more they hurt you, the stronger you get #antifragile",story is the story of the Hydra monster which is the monster that Hercules fought and whenever he would chop off one of its heads three more would grow in its place the Hydra is anti-fragile because the more you harm it the stronger it gets
tFHeUSJAYbE,2023-07-22T14:45:18.000000,A Practical Introduction to Large Language Models (LLMs),everyone I'm Shah and I'm back with a new data science Series in this new series I'm going to be talking about large language models and how to use them in practice in this video I will give a beginner friendly introduction to large language models and describe three levels of working with them in practice future videos in this series will discuss various practical aspects of large language models things like using open ai's python API using open source Solutions like the hugging face Transformers Library how to fine-tune large language models and of course how to build a large language model from scratch if you enjoyed this content please be sure to like subscribe and share with others and if you have any suggestions for me to include in this series please share those in the comments section below and so with that let's get into the video so to kick off the video series in this video I'm going to be giving a practical introduction to large language models and this is meant to be very beginner friendly and high level and I'll leave more technical details and example code for future videos and blogs in this series so a natural place to start is what is a large language model or llm for short so I'm sure most people are familiar with chat GPT however if you are enlightened enough to not keep up with new cycles and Tech hype and all this kind of stuff chat GPT is essentially a very impressive and advanced chat bot so if you go to the chat GPT website you can ask it questions like what's a large language model and it will generate a response very quickly like the one that we are seeing here and that is really impressive like if you were ever on AOL Instant Messenger also called aim you know back in early 2000s or in the early days of the internet there were chat Bots then there have been chat Bots for a long time but this one feels different like the text is very impressive and it almost feels human-like a question you might have when you hear the term large language model is what makes it large what's the difference between a large language model and a not large language model and this was exactly the question I had when I first heard the term and so one way we can put it is that large language models are a special type of language model but what makes them so special and I'm sure there's a lot that can be said about large language models but to keep things simple I'm going to talk about two distinguishing properties the first quantitative and the second qualitative so first quantitatively large language models are large they have many many more model parameters than past language models and so these days this is anywhere from tens to hundreds of billions of parameters the model parameters are numbers that Define how the model will take an input and generate the output so it's essentially the numbers that Define the model itself okay so that's a quantitative perspective of what distinguishes large language models from not large language models but there's also this qualitative perspective and these so-called emergent properties that start to show up when Lang language models become large and so emergent properties is the language used in this paper cited below a survey of large language models available in the archive really great beginner's guide I recommend it but essentially what this term means is there are properties in large language models that do not appear in smaller language models and so one example of this is zero shot learning one definition of zero shot learning is the capability of a machine learning model to complete a task it was not explicitly trained to do so while this may not sound super impressive to us very smart and sophisticated humans this is actually a major innovation in how these state-of-the-art machine learning models are developed so to see this we can compare the old state-of-the-art Paradigm to this new state-of-the-art paradigm the old way and not too long ago we can say like about five ten years ago the way the high performing best machine learning models were developed was strictly through supervised learning what this would typically look like is you would train a model on thousands if not millions of labeled examples and so what this might have looked like is you have some input text like hello Ola how's it going nastabian so on and so forth and you take all these examples and you manually assign a label to each example here we're labeling the language so English Spanish so on and so you can imagine that this would take a tremendous amount of human effort to get thousands if not millions of high quality examples so let's compare this to the more recent Innovation with large language models who use a different Paradigm they use so-called self-supervised learning so what that looks like in the context of large language models is you train a very large model on a very large Corpus of data and so what this can look like is if you're trying to build a model that can do language classification instead of painstakingly generating this labeled data set you can just take a corpus of English text and a corpus of Spanish text and train a model in a self-supervised way so in contrast to supervised learning self-supervised learning does not require manual labeling of each example in your data set the so-called labels or targets for the model are actually defined from the inherent structure of the data or in this context of the text so you might be thinking to yourself how does this self-supervised learning actually work and so one of the most popular ways that this is done is the next word prediction Paradigm so suppose we have this text listen to your and we want to predict what the next word would be but clearly there's not just one word that can go after the string of words there are actually many words you can put after this text and it would make sense in this next word prediction Paradigm what the language model is trying to do is to predict the probability distribution of the neck next word given the previous words what this might look like is listen to your heart might be the most probable next word but another likely word could be gut or listen to your body or listen to your parents and listen to your grandma and so this is essentially the core task that these large language models are trained to do and the way the large language model will learn these probabilities is that it'll see so many examples in this massive Corpus of text that is trained on and it has a massive number of internal parameters so it can efficiently represent all the different statistical associations with different words and an important Point here is that context matters if we simply added the word don't to the front of this string here and it changed it to don't listen to your then this probability distribution could look entirely different because just by adding one word before this sentence we completely change the meaning of the sentence and so to put this a bit more mathematically and I promise this is the most technical thing in this video this is an example of a auto regression task so Auto meaning self regression meaning you're trying to predict something so what this notation means is what is the probability of the nth text or more technically the nth token given the preceding M token so n minus 1 and minus two and minus three and so on and so forth and so if you really want to boil everything down this is the core task most large language models are doing and somehow through this very simple task of predict the next word we get this incredible performance from tools like chat GPT and other large language models so now with that Foundation said hopefully you have a decent understanding of what large language models are and how they work and a broader context for them now let's talk about how we can use these in practice here I will talk about three levels in which we can use large language models these three levels are ordered by the technical expertise and computational resources required the most accessible way to use large language models is prompt engineering next we have model fine tuning and then finally we have build your own large language model so starting from level one prompt engineering here I have a pretty broad definition of prompt engineering here I Define it as just using an llm out of the box so more specifically not touching any of the model parameters so of these tens of billions or hundreds of billions of parameters that Define the model we're not going to touch any of them we're just going to leave them as is here I'll talk about two ways we can do this one is the easy way and I'm sure is the way that most people in the world have interacted with large language models which is using things like chat GPT these are like intuitive user interfaces they don't require any code and they're completely free anyone can just go to the Chad gbt website type in a prompt and it'll spit out a response so while this is definitely the easiest way to do it it is a bit restrictive in that you have to go to their website this doesn't really scale well if you're trying to build a product or service around it but for a lot of use cases this is actually super helpful so for applications where the easy way doesn't cut it there is the less easy way which is using things like the open AI API or the hugging phase Transformers library and these tools provide ways to interact with large language models programmatically so essentially using python in The Case of the openai API instead of typing your request in the chat GPT user interface you can send it over to openai using Python and their API and then you will get a response back of course their API is not free so you have to pay per API call another way we can do this is via open source Solutions one of which is the hugging phase Transformers Library which gives you easy access to open source large language models so it's free and you can run these models locally so no need to send your potentially proprietary or confidential information to a third party and open AI so future videos of the series we'll dive into all these different aspects I'll talk about the openai API what it is how it works share example code I'll dive into the hugging face Transformers Library same situation what the heck is it how does it work and then sharing some python example code there I'll also do a video talking about prompt engineering more generally how can we create prompts to get good responses from large language models and so while prompt engineering is the most accessible way to work with large language models just working with a model out of the box may give you sub-optimal performance on a specific task or use case or the model has really good performance but it's massive it has like a hundred billion parameters so question might be is there a way we can use a smaller model but kind of tweak it in a way to have good performance on our very narrow and specific use case and so this brings us to level two which is model fine tuning which here I Define as adjusting at least one internal model parameter for a particular task and so here there are just generally two steps one you get a pre-trained large language model maybe from open AI or maybe an open source model from the hugging phase Transformers library and then you update the model parameters given task specific examples kind of going back to the supervised learning versus self-supervised learning the pre-trained model is going to be a self-supervised model so it will be trained on this simple word prediction task but in step two here's where we're going to do supervised learning or even reinforcement learning to tweak the model parameters for a specific use case and so this turns out to work very well models like Chachi BT you're not working with the raw pre-trained model the model that you are interacting with in chat GPT is actually a fine-tuned model developed using reinforcement learning and so a reason why this might work is that in doing this self-supervised task and doing the word prediction the base model this pre-trained large language model is learning useful representations for a wide variety of tasks so in a future video I will dive in more deeply into fine tuning techniques popular one is low rank adaptation or low raw for short and then another popular one is reinforcement learning with human feedback or rlhf and of course there is a third step here you'll deploy your fine-tuned large language model to do some kind of service or you know use it in your day-to-day life and you'll profit somehow and so my sense is between prompt engineering and model fine tuning you can probably handle 99 of large language model use cases and applications however if you're a large organization large Enterprise and security is a big concern so you don't want to use open source models or you don't want to send data to a third party via an API and maybe you want your large language model to be very good at a relatively specific set of tasks you want to customize the training data in a very specific way and you want to own all the rides have it for commercial use all this kind of stuff then it can make sense to go one step further Beyond monofine tuning and build your own large language model and so here I Define it as just coming up with all the model parameters so I'll just talk about how to do this at a very high level here and I'll leave technical details for a future video in the series first we need to get our data and so what this might look like is you'll get a book Corpus a Wikipedia Corpus and a python Corpus and so this is billions of tokens of text and then you will take that and pre-process it refine it into your training data set and then you can take the training data set and do the model training through self-supervised learning and then out of that comes the pre-trained large language model so you can take this as your starting point for level two and go from there and so if you enjoyed this video and you want to read more be sure to check out the blog in towards data science there I share some more details that I may have missed in this video this series is both a video and blog Series so each video will have an Associated blog and there will also be tons of example code on the GitHub repository story The goal of the series is to really just make information about large language models much more accessible I really do think this is the technological innovation of our time and there's so many opportunities for potential use cases applications products services that can come out of large language models and that's something that I want to support I think we'll be better off if more people understand this technology and are applying it to solving problems so with that be sure to hit the Subscribe button to keep up with future videos in the series if you have any questions or suggestions for other topics I should cover in this series please drop those in the comments section below and as always thank you so much for your time and thanks for watching
4-Byoa6BDaQ,2023-07-20T14:24:32.000000,"When you’re robust, your environment can’t hurt you #antifragile #resilience",Second Story is the story of the Phoenix which is a bird who when it dies it rises again from its ashes and this highlights the concept of robustness the Phoenix doesn't care if it lives in a stable tranquil environment or a constantly changing environment worst case scenario if the Phoenix dies it'll Rise Again from its actions the Phoenix is apathetic toward its situation
hB27yAkJLC8,2023-07-19T13:57:49.000000,Being fragile means you have more downside than upside. #antifragile #mythology,first story is the sword of dimicles is the story about a servant who sees the life of a king and wants to be in his position so for one day the servant trades places with the King and what he soon realizes is that it's not so great being King because when you have everything everyone wants to take your stuff everyone wants to take your power and the sword sitting above the King's throne is a symbol for the constant threat you are under when you are in a position of power so being King is an example of being fragile because when you have everything you have nothing to gain and everything to lose so any changes will probably not benefit you
Ty2mi994yfE,2023-07-17T16:02:05.000000,How to learn Causal Inference with #python #dataanalysis #datascience,one I use the python libraries called do y that was like one of the first I saw in this space it's a library made by Microsoft what do y and all of these other python libraries that do causal inference they usually have really good documentation so and a lot of times they'll have like techniques that I haven't even heard of I'm like what the heck is this so if you click on the more information in the documentation they usually have a pretty nice write-up on what that technique is they'll link some resources so code documentation is a great starting point
WzL3USLPwmY,2023-07-16T18:31:57.000000,A physicist edited this video #physics  #datascience #curiosity,a long tradition of physicists poking around where they shouldn't be you'll see physicists everywhere economics biology psychology statistics wherever I think a lot of people that I've met that have studied physics have this Natural Curiosity and they tend to pull on the threat of curiosity and just see where it takes them and so that's why I think you find physicists in all sorts of disciplines I identify with that classically trained in physics but I've worked at a car dealership I work at Toyota in financial services I'm really into entrepreneurship I love business I'm fascinated by nature and natural systems and even going back to Isaac Newton he was getting into finance and there's a famous quote from him he said he could predict the emotions of the stars but he can't predict the madness of man so I think there's always that Curiosity of the physicist to try to explain the world through some kind of physical model or at least something like that
ejwVSYKo7jk,2023-06-30T20:19:24.000000,Lessons from Spending $675.92 to Talk to Top Data Scientists on Upwork #freelance #datascience,are my four takeaways I'm spending 675 dollars on upwork talking to top data science Freelancers the first one is to do good work while this might sound obvious this is what allowed a lot of the most successful Freelancers to operate on referrals from past clients alone that's a really good place to be if you're a freelancer second is to find a niche finding a niche makes you a big fish in a small pond also when you built a niche not only do you specialize in a certain type of work and get really good at it but it becomes a lot easier to sell and Market yourself to the people that are looking for those skills the third is to form alliances across the tech stack no one can be a specialist in anything for example I'm a data scientist so I'm really good at data sciency type stuff I can build you machine learning models I can play with data I can spin up some data visualization but there are a lot of other skill sets that are relevant when it comes to using data science in the real world to solve the
_Wjn0gm4g20,2023-06-23T18:10:33.000000,I Spent $675.92 Talking to Top Data Scientists on Upwork—Here’s what I learned,hey everyone I'm Shaw and in this video I'm going to share what I learned talking to top data science Freelancers on upwork [Music] so this journey started about eight months ago for me when I began my search for mentorship I was looking for people on LinkedIn that operated in the intersection of data science and Entrepreneurship well I had a little bit of success with this it mainly consisted of people not responding or ghosting me and I don't take this personally I actually kind of expect it because I feel the ideal Mentor for me is someone that owns and operates a business in the data science space and they probably don't have time to be talking to complete strangers on LinkedIn however game changer for me is when I took my search off of LinkedIn and on to upwork and for those who are not familiar upwork is a platform for Freelancers basically how it works is clients post jobs on the platform and then there are Freelancers that can apply those jobs and submit proposals so the big realization for me is that entrepreneurs are on upwork while on LinkedIn you'll definitely find serious data scientists and other people that work in the data space most of professionals you find on LinkedIn are not entrepreneurs that's simply because of the fact most people are just not entrepreneurs but that's not the case on upwork on upwork everyone's an entrepreneur because freelancing and Consulting is a business so this little switch of just moving from LinkedIn to upwork for looking for mentors unlocked a whole new range of possibilities so this kicked off a series of 10 calls with top data science Freelancers on upwork I searched for the calls into three parts which was past present and future basically what got them into data science what got them into freelancing how do they operate now how do they get clients and where do they see this going for themselves what's their Vision I'm going to structure this video in a similar way and I'm going to walk through some of the key questions I asked in these interviews and summarize and synthesize the responses I got from all the different Freelancers I talked to and if you enjoyed this content please consider subscribing for more videos on data science and Entrepreneurship and if you have any questions please feel free to drop those in the comments section below I I do read all the comments and try to answer all the questions that I received so with that let's get into it so the first question was what got you into data science and one of the most remarkable things that jumped out at me through these interviews is that every single one of the Freelancers had a different educational background so I'm just going to list off some of these backgrounds biomedical engineering industrial engineering biostatistics electrical engineering computer science Finance physics data science marketing AI economics math and MBA and a data science boot camp so I listen to have 12 things there and there are only 10 interviews and that's for two reasons one most people had training Beyond just a bachelor's degree whether that was a graduate degree master's degree PhD or some kind of boot camp or something like that but the second reason is what I said at the top no two of the 10 Freelancers had the same exact trajectory maybe they did bachelor's degree in computer science but then they did a master's in statistics and then someone else may have done a bachelor's in statistics then they got their PHD in biomedical engineering my big takeaway from this is that there is no correct trajectory or journey to get into data science and this theme of diversity no two people fitting into the same exact mold was the theme that I saw coming up over and over again during these interviews so putting this simply it seemed like different was the norm here so the second big question was how did you start freelancing and again in keeping with this different is the norm theme everyone had a unique story to tell but there were still two things that seemed to come up over and over again which was when people were first getting started there was a strong focus on learning and building a reputation and so when I say learning this is more than just the Hands-On keyboard work this is more than just the technical side of data science that people had to learn it was also the business side of freelancing and Entrepreneurship which is you gotta worry about selling yourself you got to worry about finances you got to worry about juggling multiple projects and clients so that's what learning means here in this content and the second point of building a reputation for any business having a history of positive reviews and testimonials makes it a lot easier to get new clients and grow your business and so when first starting out common strategy is to do a high volume of relatively small projects and this helps with both of those points that I mentioned earlier one since you are doing a lot of projects you have the opportunity to do a lot of reps and explore the space and learn as fast as possible and two if you're working with a lot of different clients you in turn have a lot of opportunities to get client testimonials and positive reviews and so while this small and wide strategy seemed to be great for getting started it did not seem to be a common or good long-term strategy which brings us to the next question how do you operate now and so again different is the norm here people were all over the match so we had people freelancing full-time with people freelancing part-time while trying to build a business or generating income some other away we had people working full-time and freelancing on the side there were people working full-time in a contract world as a 1099 employee also there was someone that was transitioning out of freelance into a full-time W-2 role so to me this really highlights the flexibility of freelancing there are so many ways you can fit freelancing into your work in a way that aligns best with your long-term goals but for those who are not seeking full-time work whether that's W-2 or 1099 it was common to try to constrain weekly time commitments to each client and so typically what this would look like was constraining the amount of hours for each client to between 5 to 20 hours a week or 10 hours a week seem to be a pretty good sweet spot and so with this way of allocating time Freelancers would typically have two to three clients at a time the next question was how do you get new clients from these interviews there were three common ways of getting new clients the first and most common was applying to contracts on upwork or other freelancing platforms the second way was inbound leads essentially clients would find the Freelancers and requests for them to submit a proposal so a key driver of these inbound leads was the reputation a freelancer had on upwork did they have a high job success rating did they have a long list of positive reviews and testimonials these were the types of things that would drive that traffic and then the Third Way which was common among the most experienced or those who have been freelancing the longest is operating on referrals alone and so typically this happens when the freelancer has a great reputation but also builds really strong relationships and does really good work to where their clients become their biggest Advocates and so this is really like the ideal state of a freelancer because you don't have to worry about sales you don't really have to worry about marketing you can just focus on doing a good job and building strong relationships and your work will speak for itself and this brings us to the final question I asked in these interviews which is where is this going and again different is the norm here 10 Freelancers gay gave 10 different answers to this question but I'm going to put their responses into three different buckets the first book it will say is to keep freelancing and to scale up their Consulting business these were the people that said they can see themselves freelancing forever and this makes sense you know when you're a freelancer there is so much freedom and flexibility you can work when you want on what you want with who you want where you want and you can turn up or turn down the volume of work you're doing in case you have other priorities you know maybe you want to spend more time with family or you want to take more vacation and so it's not surprising to hear that some Freelancers can see themselves doing this forever also in this bucket I put those who want to scale up their Consulting business and so this seems to happen organically where a freelancer starts getting more work than they can do alone so they start finding subcontractors to help them fulfill this work and this can naturally transition into a more structured and formalized business so the second bucket of long-term goals I'll say is to generate passive income and to build a product oriented company all the freelancing can be very lucrative and you have so much freedom and flexibility at the end of the day as a freelancer you are trading your time for money basically I pay you to do a job for some hourly rate so while training time for money isn't so bad many people will prefer to trade a little time for a lot of money so for example build a product once and sell it many times that's the common thread in this bucket here trading a little time for a lot of money and so that's why I'm lumping together generating passive income and building a product oriented business so some ways the Freelancers talked about doing this was to develop digital products or sell courses online there were others who were actively trying to build trading Bots or building other trading tools for personal use others were considering building software solutions for mid-size companies leveraging The Experience they've gained and the clients that they've worked with to essentially automate a service that they've done for clients in the past so while no one I interviewed had fully made this transition yet it is something I'm really optimistic about and then finally the third bucket was people that said they want to transition into a full-time role while there is a lot of flexibility and freedom when it comes to freelancing there are people that eventually want to transition into full-time roles and there was even a theme of people transitioning in and out of full-time roles they'll work with a handful of clients but then they'll work for one particular client seems to ramp up or they open up a role for them and then they spend some period of time working full-time for that client and then eventually maybe they want to go back to the freedom of flexibility of freelance so they transition out of that full-time role and the cycle starts all over again so I'll read off a few reasons why Freelancers wanted to transition into full-time roles one of the big ones was you can make a bigger impact when you're part of a larger Enterprise and not operating on your own another was when you're part of a larger team working full-time there's more social interaction and opportunity for collaboration a big one was also you get more certainty in income stability and then finally there might be greater opportunities for career growth both in a larger Enterprise where you are working full-time so just to wrap up here there are four key things that I've taken away from these interviews the first one is do good work so you can operate off of reputation and referrals if you don't have to spend so much time selling and marketing yourself this is typically because you have such high demand and that's a really good place to be as a freelancer because you can leverage that to ensure that the work that you do take on is aligned with your long-term goals the second takeaway which I haven't mentioned yet but something that came up over and over again in these interviews is to find a niche the key to the success of a lot of these Freelancers was finding a niche which essentially made them a big fish in a small pond my reflection on this is when you try to Market yourself as everything in anything data science it's often difficult for clients to wrap their heads around how they can use you but if you find a niche and you're very specific on your expertise and your offering it typically makes you a more appealing candidate to those that are seeking those services so the third big takeaway which I haven't mentioned yet but is also something that came up over and over again is to form alliances across the tech stack so me as a data scientist I have a really strong set of skills around data science but there are many other necessary skill sets that come up when it comes to implementing data science Solutions in the real world so a big piece of advice I got from a lot of these Freelancers was to either get exposure to the full Tech stack so you can do every aspect of it you can do the web scraping you can do the data cleaning you can do the database stuff you can do the modeling you can do the machine learning but you can also productionalize your model you can also spin up a website you can also develop some software you can develop some ads so get some exposure to the full Tech stack or form alliances with Specialists across the tech stack so you don't necessarily need to know how to build a website because you have a strong partnership with someone who's an expert in that and ideally I think it's important to do both get exposure to the full text stack and additionally form Alliance is with experts and Specialists and these other things that you're not an expert in the fourth takeaway is to develop a personal brand and so having a strong presence on social media can make it a lot easier to land new clients whether this drives inbound leads like clients reaching out to you or giving you more credibility when you do apply to contracts if you're applying to a contract to do some kind of NLP work and you have a set of videos or blogs on the subject that you can share with the client that really helps close the deal so I hope you found this video valuable and helpful if you have any questions again please feel free to drop those in the comments section below if you enjoyed this content please consider liking subscribing and sharing with others and as always thank you for your time and thanks for watching
NjMD1bGBNqw,2023-05-26T23:04:47.000000,How to Create a Custom Email Signature in Gmail (2024),hey everyone I'm Shaw and in this video I'm going to walk through how to customize your email signature in Gmail so this is a continuation of a recurring theme in my past few videos which is all about leveling up how you present yourself as a professional in the data space so I talked about how to build a free website portfolio using GitHub pages I broke down the resume I used to get me hired in my current full-time role and here I'm going to talk about how to level up your email signature which may sound like a very small part of the story of you as a professional but it is one that I believe is very important so I did a deep dive into customizing email signatures when I started freelancing back in grad school so as a freelancer you're by yourself and you typically don't have a big name brand to back you up and give you credibility and this is where I feel a solid professional looking email signature can make a real difference so having a custom professional looking email signature is an easy way to give yourself credibility and an easy way way to share your online platforms and key links to anyone you send an email with with that let's see how to do this alright so here we have Gmail opened so for those who are not familiar you can make custom email signatures in Gmail and so what that looks like every time you hit this compose button to write a new email your email signature is going to pop up there so you don't have to write it out every single time and when you reply to people you can have this come up by default so anytime you send an email to everyone you know they have your name they have a link to your website they can have some call to action and they have links to your social media so the way you set up your email signature in Gmail is you got to go to settings so you got to click on this little gear icon here and you'll click on see all settings and then you scroll all the way down to a section called Signature and then you can create a bunch of email signatures here so I have a few on Deck so I got this master one I have one without my socials one with a few socials I have this test one which we can actually I'll go ahead and delete but what we can do to make a new one is just hit this create new button so we'll call this one example Sig as we saw with the email signature earlier you know is this picture and this formatted text and the social media icons so while it's easy to you know add text you know put your name here and it's easy to bold it it's easy to add in links here URLs you can add in images and then you can actually link these images with the URL one thing that you cannot do with the default settings in Gmail is have these like two columns or multiple columns in your email signature like what we saw before so if I hit compose again you have the headshot on the left and you have all the info on the right and so there's actually a trick in order to get like these two columns or if you want to add a third column or whatever and it's actually pretty simple so I'm going to go ahead and delete this and what we're going to do is head over to Google Sheets so if you're using Gmail you should also be familiar with Google Sheets what we'll do is we'll open up a new spreadsheet okay so what we're going to do here is we're going to basically create these two columns create the layout for our email signature in Google Sheets and then we'll be able to copy paste it over into our custom email signature the way to do this is you select the cell and then we can go over to insert and insert image and we're going to insert image into cell and so here you're are going to want to import your headshot so I already have mine ready to go so I'm just going to go and drag and drop this over and we can see the headshot is appearing here and so an important note here is that you need to make sure the headshot is a proper size so here my headshot is about 100 by 100 pixels and you really don't want it much bigger than that in fact if we go over here and we search Gmail signature size we see that the ideal email signature is 170 pixels high and 200 pixels wide so we have to be really cognizant about this when we're customizing our email signature and so here I just did a 100 by 100 headshot so just making sure I'm not too tall I think that's going to be the main concern here so I can clean this cell up a bit to make it nice and tight around the headshot if you want to add a bit more space you can always go over here and do the center of line and then this will kind of do the resizing automatically for you next we can add some text here so I don't recommend writing out your whole email signature here because when I was playing around with this and I was writing out my whole email signature in the cell here and then I copy pasted it over and then the formatting that got pasted over made the email signature too big and it wasn't able to be saved so I recommend just using Google Sheets to set up the formatting and then actually fill out all the content of your email signature in Gmail itself so here I'll just put test just so we can see that some content goes there and let's see I can now that's left to line it but let's make it in the center so I think that looks good and then one last thing is if I just copy paste this over it's gonna carry with it these like gray lines and it doesn't look very good so we don't want that what you want to do is highlight the cells and you want to add a border but you want to make sure the Border color is white and you do that and then you can click all borders and then you'll see the borders have disappeared from the cell and then when we copy paste it over we don't have those gray marks anymore Additionally you can add this little accent line in in between the headshot and the text so I'll just kind of put that in here as well it's just like a light gray have some thickness then you click right border and then it adds that little line there so that's optional you know you can really go to town on how you want to do the formatting of this okay but now we have the headshot we have the text we have this line in between and what we can do is just copy these two cells go back over to Gmail and paste that in and so now you'll notice that this headshot looks terrible we actually don't need to keep this headshot here we really just wanted to get the formatting get these two columns into this signature customizer here so we can actually go ahead and remove the headshot but now you see we have like this other column here and so with that what we can do is we can reinsert our headshot so I'm going to go to insert image and then I'll do upload and then I'm gonna drag over the same exact headshot I use so again you know I use 100 pixels by 100 pixels here so just make sure it's a reasonable size if the size is not little big you can go ahead and just hit small and then that'll make it look nice so now with the headshot in we can start adding text so what I had was something pretty simple I just had my name you know we can just have a title I like to have my website here and then some call to action so I'm going to put okay so here now we're running to an issue so going back you saw that the text was kind of jumping over to the next line that's because I didn't make this second cell wide enough so we're going to go ahead and make that cell a bit wider and then we're going to come back delete all this put this back in and just repeat those steps so yeah so I'd like to put my home page in there and then I'll put a call to action book a call so this is a good checkpoint here cool thing is just like with a lot of text editors these days we can make any text a URL so I'm going to go over to my website and I'm going to get this link as well I'm just going to copy this and you can highlight the text you want to turn into a link you just highlight it and then click this link button and then you're just going to paste in the link you wanted to go to and so now you should see it pop up as a link and then we'll just do that again for book a call there we go and so already this looks like a pretty nice email signature maybe I'll I'll add a space here just to give it a bit more distance from that line but this looks pretty professional so kind of going the extra mile we can import the social media icons like we saw so kind of going back adding in these social media icons and what's cool about these is that these are actually clickable links when you send the email and so to do that we'll do a similar thing as we did before and we'll insert an image and so if we click this insert image button here and there are actually two ways to do this so the first way is you can import an image as a URL and so you know an easy way to do this is just go to like any websites already have it loaded up in the search bar social media icons for email signature I found this one was pretty good and what you can do is just go to any website that these images exist and you can just right click on it and copy the image address and if you do that you can come back over here paste it in and you'll see the image pop up and there you go so now the image is in your email signature the other way to do it is to insert the image like we did before which is by uploading it from your computer so I have these like already on my desktop so I'll just kind of go drag and drop them over and so there we go and now the image is there and so again whenever we're inserting images into the email signature we have to be cognizant about the size of the image so the headshot I did is 100 pixels by 100 pixels and this LinkedIn icon I made sure it was 24 by 24 pixels and that's actually the same as this website over here I thought this was a good size 24 by 24 pixels but this website has more 32 64 128 so on also it's pretty easy to resize images if you're on Mac you can use preview and so it's pretty easy typically to scale down images just find it on the web somewhere download it and just make sure it's 24 by 24 30 by 30 whatever you want to make it okay so we'll just repeat this process so I'll go ahead and insert the other links and space and I'm going to hit insert image again I'm going to upload it again and then I'm going to bring in a few more so here's YouTube okay and you know obviously what social media icons you put here will depend on your industry and whatever platforms you're on but you can kind of customize it get the spacing right I just added white spaces in between the icons for the formatting and then what you can do to actually make these clickable links is you can highlight them just like we did with the text hit this link button and then paste in whatever link you want so I'll go back my home page and grab all these links so grab my LinkedIn gratitude that video highlight like and I finally sometimes it'll hide behind this thing but you can find that all right there you go and so now we have our custom email signature and when we're done we got to make sure we save changes so we'll click that I'll make a new email I'll send it to myself make the subject in line check out my email Sig and so this is actually a different one you can actually choose whichever email signature you want to use so I'll go to the one we just made which is called email Sig and it'll actually automatically change to whatever email signature you want to use so maybe you want to have different signatures for different clientele or different audiences and then I'll just hit send alright so that's it so I hope that was helpful if you were painstakingly trying to figure this out for hours like I was at one point hopefully this made your life a little easier if you enjoyed this video be sure to like subscribe and share it with others if you run into any issues or you have any questions please feel free to leave a comment down below I do read all the comments and try to answer all the questions that I receive and as always thank you for your time and thanks for watching
gp29_P3_lgo,2023-05-06T16:03:12.000000,"My $100,000+ Data Science Resume (what got me hired)",everyone I'm sure and in this video I'm going to walk through the data science resume that actually got me hired and so I've worked as a data scientist in three different Arenas in research in freelance and now in corporate and through this experience I've gotten a good sense of what the day-to-day of a typical data scientist looks like and what it takes to be successful in a data science role so the whole motivation for this video is that I know a lot of people these days are trying to get that first data science job that first job in any industry is always the hardest to get you know it's that old story of you need experience to get a job but you need a job to get experience and so the goal of this video is to talk through the resume that actually worked and got me hired and share the principles and tips that help me craft the resume as well as things that I did outside of the resume to help me land the job and so be sure to stick around to the end because I'm going to talk about five specific things that you can do to increase your value as a candidate Beyond on the essentials of a resume so I want to start by asking what is the goal of a resume and so it might sound obvious you might say the goal of our resumes to get the job or the goal of her resumes to describe my experience the goal of her resume is to say why I'm a great fit for this role so while there are many goals of a resume the one that I found the most clear and helpful is that the goal of a resume is to get the interview and the reason why I think this way of thinking about it is important is because people get so wrapped up in trying to jam-pack every little detail into their resume with the attention of trying to come across as a more attractive candidate but ironically when you put way too much information especially information that's not relevant to the role it actually makes you a less compelling candidate which brings me to my foundational principle of crafting a resume Which is less is more the way I see it the person reading your resume has a finite amount of attention that they're going to give to your resume so you've got about seven seconds to convey not so much who you are as a person and every little detail of your experience but rather your goal in that seven seconds is to be someone interested be someone that they want to reach out to be someone that they want to set up an interview with and so as we walk through my old resume we're going to see this principle of less is more come up through the design and the content of the resume so let's see what this looks like okay so here is the resume so this was for my current role which is a data scientist at Toyota financial services and so you can see this is a very simple resume no pictures no colors just to the point so I'm going to talk through four things that I think were good here so the first one isn't something I included but something I did not include which was this objectives section that you might see on other resumes and so just thinking back to this principle of less is more and that you have a finite amount of the reader's attention do you want to spend that fine finite attention for them to read a paragraph of objectives or do you want to save that attention for some impressive projects you may have done further down in your resume so for me I don't see any utility in having an objective section for data science roles the second thing I think was good here is I added this technical skills section and not only do I have this section I have it at the top of the resume and so I think one thing that's happening is if a human is reading your resume they are looking for specific skills especially in data science they want to know can this person code in Python do they have SQL experience do they have cloud computing experience you know they're looking for these types of things and so don't make them hunt for these keywords because they're going to be so focused on trying to find python in this wall of text here that they're gonna miss everything else so just give it to them at the beginning so they can relax their mind they're like okay great this person can code in Python that's all we use on our team so now with that sense of relief they can actually read your resume with an open mind the third thing is pretty standard which is these bullet points and how you craft these bullet points and so the format that I've been taught and that I found helpful is to start with a strong action verb and then there are resources online that give you like a strong action verb ideas for these types of bullet points you start with the strong action verb what you did and have some kind of quantifiable impact associated with that thing so here I put conducted data collection processing and Analysis for novel study evaluating the impact of over 300 biometric variables on human performance and Hyper realistic live fire training scenarios so that sounds really interesting but I will say one thing that's missing here is the quantifiable impact so what was the result of this work just stop and ask yourself that question of any bullet point you have on here it's like why is this important in a larger context and I would even say if you can't think of a wider impact of any bullet point consider just dropping it and picking up something else that has a higher impact and so if I was to do this over again I would think a bit more carefully about the impact of this bullet point here and here's a corollary to that anytime you can put numbers in these bullet points it just stands out like you might see clickbaity blogs out there like five ways to lose weight fast or seven secrets that will make you a more attractive mate or something like that there's a reason they put numbers in the titles is because our minds are attracted to that when you see a bunch of letters the numbers stand out and it gets your attention and so use numbers especially if you can have a quantifiable impact like here this is a better example analyze marketing and sales reports to inform an inventory acquisition which resulted in a 50 decrease in average inventory age that's a quantifiable impact and that's the type of bullet point you want to have for everything and then the fourth thing is a really small thing but I think it does make a difference in addition to listing your technical skills here bold them in each of these bullet points so this was actually something I picked up from a Google hiring manager when I was in the looking for a job phase of my life in grad school he was actually the one that also told me to put technical skills at the top and then he additionally said both these things because again if the person reading your resume is looking for something specific don't make it hard for them to find it those are the four things that I liked as far as things that I would do differently or improve upon one I already mentioned which is a lot of these bullet points don't have any kind of quantifiable impact they have the strong action verb they have the technical task that I'm trying to convey but there's no impact the second thing that I would improve on this resume is that notice that it's a two-pager and so there's debate on whether a resume should ever be two page should always be one page I'm not going to get into that but specifically here I might consider dropping the talks and Outreach maybe even the awards and honors here and then just have an Abridged version of the Publications just to bring it down to one page and just try to make it a bit more concise just to recap the four things that I thought were good here were no objectives section putting the technical skills at the top so the reader doesn't have to hunt for it having these strong action verb thing then quantifiable impact on the end and then finally building these key skills again so the reader doesn't have to hunt for them and then as far as the two things that I would improve upon is to reevaluate these two bullet points so they do have quantifiable impact and two try to remove some of these sections so this could just be a one-page resume instead of a two-pager another key point with the whole job application process and hiring process is to always ask for feedback no matter what even if you don't get the role ask for feedback in the interviews even if you don't get the interview ask for feedback if you can get in contact with a person and so toward that end when I had that first interview with the hiring manager like that initial interview before you get to the technical interviews and interviews and all that kind of stuff I asked a simple question to the hiring manager which was why me what stood out about my resume what made you think I was a good fit for this role and so I find that a very good question to ask in any kind of hiring process because now you're getting a little bit of insight into their world you're seeing what their problems are and how they see you fitting into that world and so the hiring manager mentioned two things that stood out about my resume that he liked the first was PhD and if you don't have a PhD or you're not pursuing a PhD you might think okay wow that's not helpful at all but I will say in my experience it seems that graduate degrees do open a lot of doors and opportunities so even if we're talking about a master's degree and not a PhD having these Advanced degrees definitely help differentiate you from other candidates and then the second thing that stood out wasn't so much the resume itself because again really all that seemed to stand out was the PHD okay he knows python let's talk to this person but the other thing that that stood out was I had this website here and so we can click on this and it takes you to an actual website having a website a home page an online portfolio is a great way to just stand out as a candidate and so this is being hosted completely for free using GitHub pages and actually in the last video on my channel I have a whole walkthrough guide on how you can spit up a website completely for free using GitHub pages so just having a portfolio website can help differentiate you from other candidates and it doesn't even have to be as sophisticated as this so this took me about a weekend to spend together using this open source drag and drop website builder called Moby rise so I use this Moby rise it's like a drag and drop interface to build the website it's completely free but you don't have to go through all this trouble so GitHub Pages gives you these great themes to spin up a website from a markdown file so still no coding involved whatsoever but the result in like 15 minutes of filling out a readme file you can spin up a good looking website that can also help differentiate you from other candidates and so that was the resume and as we saw based on the hiring manager feedback it wasn't just the resume that stood out it was also this portfolio so this is a great point because you have to realize the resumes and everything there's so much more beyond the resume that goes into the hiring process and everyone has the resume but when you're trying to stand out when you're trying to get the interview a lot of times it pays not so much to be a better candidate but to be a different candidate and what I mean by that is it pays to stand out to distinguish yourself as a candidate for a role and so we already talked about one way to do this which is a website portfolio but even beyond the resume and Beyond the portfolio there's still more that can be done and so what I'm talking about here is networking so a great way to do this is LinkedIn if you have a role that you're interested in go on LinkedIn and try to find people that work at that company try to find people that work on that team try to find people working in HR are at that company and just try to make contact with a human even though it's not going to work out every time that 20 of the time that you actually get connected with someone and start talking to someone that's in the company this can have a very significant impact on your probability of getting the job so if you're sitting there and you're like Shaw you're not telling me anything I don't know I have the resume I have the portfolio I reached out to 200 people in the last three months on LinkedIn and still I'm not getting anywhere so if that's the case a lot of times What's Happening Here is you need more experience you need more development as a data scientist which brings us back to the dilemma I mentioned at the beginning of this video I need experience to get a job but I need a job to get experience but I don't think that's the case necessarily I can think of five things you can do to get more experience as a data scientist and make yourself a more attractive candidate for these data science roles and so the first one that might sound obvious which is education and so this is a lot of go-to's for people you're applying to data science jobs with a bachelor's degree and you're not getting anywhere maybe get that master's degree maybe get that certification if you have the certification master's degree and it's still not working out for you maybe go for another certification or maybe go for a PhD but I'll just caveat this if you're getting a degree just to get a job I don't think that's going to be the right answer most of the time the second thing you can do is independent projects if you don't have enough experiences as data scientists get the experience by coming up with projects go find some data on the web build a web scraper and gather data and start building interesting data sets so you can do interesting data science projects and put that on your resume put that on your portfolio the third thing that you can do that I found very helpful is to make content write blogs on medium publish them in towards data science make YouTube videos like this where you're explaining data science topics where you're explaining your data science projects and you're displaying your competency and people have something tangible they can click on on your portfolio or you can refer them to with a link and Beyond just being something you shared with people sometimes these blogs and videos they take on a life of their own and people start coming to you asking you questions about this stuff and maybe even offering you jobs the fourth thing you can do that seems to be coming more and more popular these days isn't so much doing more data science projects or going back to school but just hosting on LinkedIn and so I've seen a lot of people working in data roles or still in school building a following and building an audience on LinkedIn and through that growth they're getting a lot of visibility and this leads them to getting job offers and so this is interesting because you don't necessarily need to be developing your skills further you just need to be showing off the Knowledge and Skills that you already have and then the fifth option is you can start freelancing and so I really like this option because if your goal is to get a full-time data science role when freelancing on a website like upwork you're getting a lot of interview reps and the time it takes you to apply to 10 full-time roles you can apply to a hundred upwork roles so you're getting 10 times the experience writing resumes writing cover letters doing interviews than you do otherwise and so you can really develop that skill set quickly and another benefit of freelancing you get experience by working on real world projects that aren't just for education but are making an impact for your clients and is putting money in your pocket and so I hope this video was helpful for you and trying to land that first data science role and break into the field if you enjoyed this content please consider liking subscribing and sharing with others if you have any questions about data science or career stuff please feel free to drop those in the comment section below I do read all the comments and try to answer to all the questions that I received and as always thank you so much for your time and thanks for watching
D9CLhQdLp8w,2023-04-24T21:30:03.000000,How to Make a Data Science Portfolio With GitHub Pages (2024),everyone I'm sure and in this video I'm going to walk through a super easy way to make a portfolio website without any coding using GitHub pages so it's about that time of year people are graduating graduating college maybe you're graduating grad school and now it's time to enter the workforce and if you're like a lot of people you've probably heard of data science and how data scientists are doing all these cool things you know they're building chat gpts they're building models they're using machine learning and AI to solve business problems and create impacts and this sounds like a really fun and exciting field to be a part of as someone that works in data science a big problem for a lot of data scientists is not so much knowing your stuff like the technical side of things but it's the ability to show your stuff and sell yourself and this is just something that you never really do When developing the skill set to become a data scientist but if you're trying to get a job whether that's full-time at a company or you want to go into freelancing or Consulting being able to sell yourself is a critical part of the process so one thing that I found super helpful in getting data science work is having a website portfolio that employers can go to to see my experience my projects and just get to know me but the problem is I'm a data scientist I can build you a machine learning model but don't ask me to build you a website because it's not going to be great so I'm totally incompetent with HTML CSS any kind of like web dev type of stuff and if you're a data scientist it's likely that you're in a similar boat but lucky for us our friend GitHub has this built-in functionality for generating and hosting websites completely for free without requiring any kind of web development experience whatsoever so GitHub Pages makes this spinning up a website super simple and I use this functionality a lot in grad school in spinning up websites for my own portfolio and for projects that I was working on quickly running through here what the steps are you have these two options you can build a website from scratch and just host it on GitHub or you can go a much easier route where GitHub will generate you a website automatically from your readme file using a package called Jekyll And so here all you gotta do is fill out your readme file with what you want in your portfolio and then Jekyll will take that text and generate a website based on the theme that you choose not too long ago like one two years ago this was like really really stupid simple just looking at this website it's a pages.github.com just a few steps so you go to the repository settings and then it used to have this Theme Chooser built in so you just say like which branch you have your readme file in and then you just like choose a theme and then you have this user interface this GUI where you could just click which theme you wanted and it would just automatically generate the website so when I first wanted to make this video I'm like okay yeah it's gonna be super simple show people how they can make a website completely for free free without any web development experience whatsoever but when I came to spin up my website this button was not there anymore this kind of let me down a rabbit hole trying to figure out what happened couldn't find anything anywhere no one's made a YouTube video about this no one's written a Blog about this github's documentation was not very helpful to me however after messing around with it for a couple hours I finally figured it out and now I'm gonna walk through step by step how you can build your website using GitHub pages and this built-in functionality so this is what the final product looks like so it's a really clean design cool looking website you have your picture here you think about your name like whatever job you're looking for I didn't do any kind of coding whatsoever I just typed up a markdown file and this was automatically generated so I was able to throw this together in less than 30 minutes because a lot of the stuff it's just like copy pasting from your LinkedIn or your resume or something similar and so this is the final product now I'll walk through the steps to build this so first step go to your GitHub Pro profile if you don't have a GitHub profile go ahead and make one it should be straightforward and a good idea if you're trying to get a job in data science here at our GitHub profile we're going to go ahead and click repositories and you can see this is the example portfolio I made this past week but we're not going to go there we're going to start over from scratch and click new and then we should get a screen like this so create a new repository repository name so we have two options here option one is if you want this portfolio website name to just be your GitHub username.github.io you just type that in right here so Shaheen T is my username GitHub dot IO this will appear in the search bar and this will be your website name I already created a website portfolio using this website name so it's not going to work for me but if you haven't done this already then it should work for you the second option is you can give it any repository name you like so it could just be portfolio well it could be data science it could be data scientist you know whatever you want and what's going to happen in the search bar for example if you make your repository name portfolio what's going to appear in the search bar is going to be your GitHub username.github dot IO slash whatever you make this repository name so if I make a portfolio it'll be Shaheen T github.io portfolio Okay so we'll go ahead and name it portfolio we don't need a description here and it's probably best that we don't have one but we're going to initialize our repository with a readme file so we'll click that this is gonna make it really quick and easy to spin up our website I'm not going to have a git ignore we're not going to add a license that's not necessary so just to recap we make our repository name either our GitHub username.github.io or whatever other name that we like so just go in portfolio here don't need a description and just be sure to click read me and then we're gonna create repository and there we go so now we have the super Bare Bones repository so we already have a readme file here but we're going to need to add one more file called a config file so to do that we can just go to add file create new file and we'll call it config.yml and so now that GitHub got rid of this choose theme button in the interface so let me go back now that they got rid of this choose theme button here and you can't have this super easy user interface for picking your theme you have to do it using this config.yaml file and so this is actually super simple and it's not much more difficult than this interface so I'm going to kind of jump ahead and this is the config file for the example that I've already put together we can just go ahead and like copy paste this so title would be at the top left corner of the portfolio website so you can just put your name here logo we can actually comment this out for now because this is going to be the relative path of the image you want on that left hand side so let me just go back so this is where the title appears this logo relative path is where this image is located in your GitHub repo but since we're starting fresh I'm going to comment that out if you want to add text below here so if we added a description to our repo it would appear here or we can overwrite that by manually putting a description here so show downloads true it's just giving you these options to download the zip the tar or go to the GitHub so I put true for my example portfolio if people want to steal the code but for yours I mean this is optional if you want people to be able to download your code you can make it false or just comment it out and then finally the key thing is this theme so instead of being able to click on the theme we have to manually kind of type it out using this syntax here so I went with this Jekyll theme minimal so that's what we're seeing here it's super Sleek super nice I think it makes a lot of sense for a portfolio but if you don't like that for every reason there are a bunch of other supported themes for example let's click on architect and we can see what that looks like so that looks like this a little different different design but again this will be generated automatically based on your readme file in the GitHub repository so I'll share this link it's pages.github.com themes and I'll share it below if you want to explore the other theme options okay so we have our title we're going to have show downloads we're going to just comment this out we don't really need that and then we're just left here so really right now all you need is the title and the theme that you want to choose you go ahead and hit commit changes and so now you have two files in your repository you have your readme file and your config file and now you can just start adding stuff in here so like data scientist education work experience uh data scientists full of points here we go big impact project one big impact project two let's see what else is good to have here education your work experience what else do I have projects yes that's important so projects see uh and then so on and so forth so you can start just building some stuff out I'm just doing this so when we spin up the website something appears so just through something really simple together here and so still we just have a readme file in this simple config.yaml file and then the last step is to go to settings here we're in settings then we're going to scroll down to pages and then we have this section here build and deployment so here we're going to leave the source as is we're going to deploy from a branch and then under Branch we're just going to hit Main and then we're going to keep it as root because we want GitHub pages to look at the readme file and the config file in our root directory and we're going to hit save so now notice that we didn't have that super simple GUI to select our theme and then if you hit add a jackal theme you get some instructions here but I got tripped up on theme theme name because this isn't the right syntax here so this uh typo here it's uh made this like really straightforward task like a two-hour task for me so you can put just Minima or minimal here you have to put Jekyll Dash theme Dash Minima so if you were in a similar boat and you were struggling for hours like me it's just a simple syntax issue and that's why the website's not working anyway going back so if we go here to actions we see it's already been built but if we came back earlier we would have seen this is like processing and something's happening but we built Pages was built and deployed so if we click over here to deployment we can hit this view deployment option here and it'll take us to our website so oh look at that then you can look in your search bar what the website name is so for me it's shaheent.github.io portfolio and then if we look over here we see our readme file built out we have data scientists which was in the title so I'll go back so we can compare so we have our readme file here so data scientist education work experience projects EEG band discovery that looks a little different but looks better here on the website and then we just build it out and so we can kind of do it on the cooking shows where I don't walk through the process here and just kind of jump to the final product and so going to this example portfolio repo here are a few simple things to do one I'll definitely add a assets slash image path and then dump all the images you want to use in your portfolio here going back next big thing is so we'll go ahead and edit this readme and so configuring a readme file is a lot easier than building a website with HTML you can just copy paste this or clone the repo and use this as a starting point you can customize the structure you can start completely from scratch whatever makes sense for you just a couple things you know you can add links here so this is the standard GitHub syntax you just say what you want the text to appear as and then this is what the link is that people click on if you want to add images you put the title of whatever image and then you put the relative path to the image so again it's in this assets image subdirectory and then that's the file name and then so on and so forth so really go to town on this you know you can spend probably hours just tweaking and fine-tuning it but this is hopefully a nice jump start and it'll get you something pretty close to a final product okay so I rambled on a bit at the end there but hopefully this is helpful you know it's hard to get that first job and bring to the data science field so I hope this kind of accelerates that process for you and if you enjoyed this video please consider liking subscribing and sharing the video with others if you have any questions about like building a portfolio or like getting a job data science you know feel free to drop those in the comment section below I do read all the comments and I try to respond to all the questions that I receive and as always thank you for your time and thanks for watching
4vvoIA0MalQ,2023-03-18T23:34:38.000000,Dimensionality Reduction & Segmentation with Decision Trees | Python Code,hey everyone welcome back I'm Shaw and in this video I'm going to continue the series on decision trees and talk about a couple of applications so in the last two videos of the series we talked about how we can train predictive models using decision trees so in the first video we just talked about models employing a single decision tree then in the second video we expanded this idea to tree ensembles so if you haven't already be sure to check those out because we're going to build upon those ideas in this video the whole point of the discussion today is that we can use machine learning models specifically decision trees for more than just making predictions so this is what I'll call Next Level uses of decision trees not because they're anything profound or groundbreaking but because they go beyond this obvious task of just using a machine learning model to make a prediction and I think for those who are just getting started in data science it may be easy to think that all there is to data science is getting some data training a model and making predictions and that somehow through this process there's going to be immediate real world impact in value and the reality is it's not so straightforward so what I really like about data science is the critical thinking and the creativity that's required to use these tools and techniques to solve real world problems and provide value and so that's all I mean by Next Level so toward that end in this video I'll talk about two ways we can use decision trees for more than just making predictions so the first is reducing predictor count and the second is called predictor segmentation starting with the first one reduce predictor count so this goes back to the previous video of the series where we talked about tree ensembles where we stitch together a bunch of decision trees which made our machine learning model more robust while decision tree ensembles give us so many great things like I was talking about in the previous video all these great things come at a cost which which is that tree ensembles are a bit of a black box so we know what we put into the tree Ensemble and we can see what comes out of it but what happens in between is a bit of a mystery and this is a problem for a lot of machine learning models we may get this great predictive performance but a lot of times it's not so easy to interpret what's happening under the hood however we don't have to use our tree Ensemble to make our final predictions rather what we can do is take the feature importance ranking from our tree Ensemble model and use that to inform a simpler set of predictor variables and so this is all motivated by Occam's razor which is a very popular idea and in this context what it implies is that simpler models are better so let's say that we have our true Ensemble model and it has 30 predictor variables to it but following the logic here what we want to do is take these 30 variables and just keep the handful of variables that are most important so not only does this help us in interpreting what the model is doing but in a lot of cases it can actually lead to an improvement in predictive performance so walking through what this might look like we take our tree Ensemble and we spit out our feature importance ranking then what we can do is take the top predictor train a machine learning model from it so this could be a decision tree could be a logistic regression model linear model neural network it really doesn't matter what kind of model we use it for we use the one predictor to develop a model and then we assess that model's performance then we can do the same exact thing for the top two predictors and now we have a model with two variables we grab its performance metrics for three models four models and so on and so forth so we just keep doing this until we have all the predictors in our original tree Ensemble model once we go through this process we can plot a chart that looks something like this we're on the x-axis we have the number of variables included in the model and then on the y-axis we have some performance measures so here I put AUC just as an example each of these points corresponds to a different model and we can see the gain and predictive performance here so for example we kind of see that we have pretty big gains until we hit about three variables so what this tells us is maybe we don't need all six variables and we can just get away with using three of them without a major loss in predictive performance and so the upside of that like I mentioned earlier is that a model with three input variables is a little easier to interpret than a model with 6 or 60 input variables so now I'm going to walk through some example code of doing this so here I'm going to use the same data set that I used in the previous video this series so there's a bit of overlap between the code here and from the previous video so I'm not going to spend too much time on any recurring details but if you want to learn more check out that video and also the code is available at the GitHub linked down here and Linked In the description below First Step as always is importing modules this is a lot of the same stuff as the example code from the previous video with the only addition of this import here where we're bringing in a logistic regression model from sklearn and then as always we're going to load in our data next we're doing some data prep this is very similar to what we saw in the previous video this is something new where all I'm doing here is since Y is a Boolean variable meaning it can only take values of zero or one all I'm doing is switching the meaning of zero and one so originally zero meant the tumor was malignant and one meant the tumor was benign and all I'm doing with this transformation is switching one means the tumor is malignant and zero means the tumor is benign and the reason I'm doing this is that later down the line when it comes to interpreting what the coefficients of our logistic regression model mean it's just a bit more intuitive to talk about things in terms of risk of breast cancer as opposed to the opposite which would be like safety from breast cancer and then this should also be a review where we're using smoke to balance our imbalance data set so we have way more benign cases than malignant cases so all smote is doing is synthetically over sampling the minority class and then we're using this train test split function to create our training and testing data sets Okay so now we can train our random Forest so this is one of the tree Ensemble methods we saw in the previous video so we can fit that model with just a couple lines of code so next we have something new everything up until this point we basically did in the previous video but now we're kind of going into some novelty so all we're doing here is pulling the feature importances from our random forest model and then we're sorting them in descending order so what this looks like is this where these are all the names of our features and these numbers here quantify their relative importance and so now what we can do is exactly the process I was describing before or retrain a model using the top predictor and assess its performance and we train another model using the top two predictors assess the performance top three sets performance so on and so forth so what this looks like in code could be something like this where we're initializing all these lists to store our classifiers and then to store our different performance measures we can ignore this I equals zero here this is just left over from an earlier version of the code what we're doing here is for I corresponding to the number of elements in this series we're going to go through one by one and do the following block of code so what we're doing here is listing the feature names up until I plus one we train our logistic regression model using this line of code and then we're just appending things to these lists from before so we're pending the classifier to the classifier list we're appending the AUC value for the training data set and then we're appending the AUC value for the testing data set if this is confusing and complicated don't worry what matters is this final result which is just like what we saw before which is our number of variables plotted on the x-axis and then the performance of the models on the y-axis so this this red dashed line is the AUC value for the random forest model we trained originally so this is a tree Ensemble model that uses all 30 predictor variables but what's really remarkable is that once we hit five variables the logistic regression model actually outperforms this more sophisticated model that uses six times as many variables and then you can see after five variables the logistic regression models just keep getting better and better but let's say for our purposes we really value being able to interpret what the model is doing as well as the model's accuracy so let's say once we beat our random force model we're satisfied so that's the model we're going to use and then since logistic regression is a linear model we can easily interpret the relationship between our predictor variables and the target variable by looking at the model coefficients and so the bars here are just showing the coefficient values looking at worst perimeter which is about 0.3 the way to interpret this is a unit increase and worst perimeter translates to a 0.3 increase in the log odds that the tumor is malignant so I know that was a mouthful making that a bit more qualitative as the worst perimeter increases the probability that the tumor is malignant also increases so now we kind of have the concrete quantification of the interaction between our predictor variables and the probability that the tumor is malignant and so there's a small technical detail here that I don't want to spend too much time on but I talk about more in the blog and it has to do with the resampling since we use mode to synthetically oversample the minority class we can't immediately translate our logistic regression model outputs to probabilities and that's just because the y-intercept for our logistic regression model is biased due to the over sampling and so there's a simple fix there we can just adjust our y-intercept to make it not biased and then everything works perfectly and so if you want to learn more about that check out the blog okay next we have predictor variable segmentation and this actually goes back to the first blog in this series where we used a decision tree model for sepsis survival prediction and there the final decision tree we had looked like this and what's interesting here is even though we had three predictor variables the vast majority of these splits are only using age so here the initial split is splitting on ages less than or equal to 58.5 years and then 44.5 78.5 56.5 67 86 so this is really interesting what this is indicating is that when it comes to sepsis survival age is the most important risk factor we have in our data set and so the other ones we had were the sex of the patient and also the number of previous sepsis episodes and so sometimes in cases like this where there's one predictor variable that has this outsized impact on our Target it can make sense to do segmentation on that predictor variable and what that means is all we're doing is taking this continuous variable age and we're going to partition it into discrete sections so kind of looking at this visually let's say we have ages on our data set ranging from zero to a hundred all segmentation does is split these ages into some number of subcategories so let's say we want to split it into five subcategories and then the result looks like this and so what you can do now is instead of training a decision Tree on all of your data you can train separate decision trees for each age group and what this can translate to is better model performance especially if there are systematic differences between these age groups which requires separate model development so the question is how do we come up with these segments so we can definitely do it manually so we just kind of look at the data and say okay let's do this age group and that age group or use some kind of subject matter expertise but another way we can do it is using a decision tree so this picture here is showing how we can come up with these segments using a decision tree you notice that age is actually being split into these different sections based on the sepsis outcome of dead or alive but maybe we wouldn't want to use this decision tree directly because it has this other variable involved in the splits so what we can do is train another decision tree model but now instead of using the three predictors of age sex and number of sepsis episodes we can just use the one variable we care about which is age so now I'm going to walk through what that looks like I'm going to use the same data set from the first video of this series and then as always we're going to start by importing our modules so these shouldn't be anything new next we're going to load in our data just like we did in the first video and then we're going to do some some data prep so here all we're doing is keeping the variables of age and the sepsis survival flag and now we're going to do a little bit of data prep so what we're doing here is we're grouping the data based on age so you can imagine that we have all these different patients and there can be multiple patients of the same age so all we're doing here is reshuffling the data to have only unique age values but then for each of these unique age values we're going to have a percent of patients that are alive and we're going to name this column percent alive and then on the flip side we can take 1 minus the percent alive and create a new column called percent not alive and so the result of that is a data frame that looks like this so now we only have unique age values starting from zero going all the way up to a hundred and then for each age value we have the percentage of them that are live and the percentage of them that are dead next all we're doing here is grabbing the variable names and creating separate data frames for our input and Target variable so here the predicted variable is age the target variable is going to be percent not alive and as a first pass to the relationship we can just plot them against each other it's on the x-axis we have age and on the y-axis we have percent not alive so as percent not alive goes up that's the indication that the risk of sepsis increases so you can see around midlife there's this clear uptrend of the percent of patients that are not surviving their sepsis episode but before that this risk is relatively low and stable and so just looking at this plot we could probably chop up this data into any number of segments based on this risk so maybe we would do like zero to 40 40 to 60 60 80 100 like whatever but this is just us eyeballing it and it'll be interesting to compare this intuition to what the decision tree is going to spit out okay moving forward we can now train in our decision tree model so here we can Define our number of bins by controlling the maximum number of leaf nodes in our decision tree regressor and the reason this works is that as we saw in the first video a fully grown decision Tree on this data is just massive so you can virtually have any number of bins that you like and it'll work and then finally we just fit our data to the decision tree and now with the decision Tree in hand we can go in and grab all the split values in an automated way so this code is a bit involved so I won't spend too much time on it but for those who are curious you can take a look at it here and it's also available at the GitHub repository linked here but the final result looks something like this so here we have the same plot from before where we have age and years plotted against the percent not alive and so this is qualitatively pretty similar to what we were talking about before like maybe we would have put one here and then adjusted the rest but this is kind of a tricky problem because if I shift this border from here to here to make this first bin look a little better now this bin may not look as good because now you're mixing together these lower risk patients with these higher risk patients and then from a treatment standpoint that may not make a whole lot of sense that's one of the upsides of using a decision tree and leveraging that greedy search to Define these bins because it already is doing that tricky optimization for us and then as a final note I'll just say to take all these with a grain of salt just because the decision tree spits out these optimal age buckets this may not translate well to treatment strategies and so as opposed to just taking this as gospel this is more of a starting place and may just serve better than just arbitrarily drawing lines for these different age groups okay that's it so if you want to learn more be sure to check out the blog published on medium and Linked In the description below feel free to steal the code from the GitHub repository and apply it to use cases or projects that you're working on if you if you enjoyed this content please consider liking subscribing and sharing your thoughts in the comments section below I do read all the comments and I find all the questions and feedback that I receive very valuable and as always thank you for your time and thanks for watching
ZaXpMou55lw,2023-03-03T14:22:24.000000,10 Decision Trees are Better Than 1 | Random Forest & AdaBoost,hey everyone I'm Shah and in this video I'm going to continue the series on decision trees and talk about decision tree ensembles so instead of just using one decision tree a tree Ensemble combines a collection of decision trees into a single model so with that let's get into the video so like I just said a decision tree Ensemble is a collection of decision trees which are combined into a single model so if you recall from the previous video we saw decision trees where a way we can make predictions through a series of yes or no questions and they look something like this you start at the top note here and just follow the arrowheads based on predictor variable values which eventually lead you to your final prediction on the other hand a decision tree Ensemble will look something more like this so now instead of a single decision tree we have multiple decision trees each giving a prediction and then we can combine these predictions together to give us our final as estimation and the key benefit of a decision tree Ensemble is that it generally performs better than any single decision tree alone and we'll kind of touch on why this is the case a little later in the video but first i'm going to talk about two different types of decision tree ensembles the first one is bagging which is short for bootstrap aggregation or bootstrap aggregation bootstrapped bootstrapped aggregation first short for bootstrapped aggregation and then the second one is called boosting which isn't sure for anything starting with bagging here the idea is to train a set of decision trees one at a time by randomly sampling with replacement so what the heck does this mean so we'll just walk through this one step at a time say we start with our training data set T naught and each of these blocks here represents a different record or example in our training data set so we have record one we have record two three four five what we do here is we create another training data set by randomly sampling T naught with replacement what this might look like is this where we randomly pick five records from the original training data set and so notice record number three actually shows up twice in this training data set this just follows from sampling with replacement so basically all that means is every time we pick a record from T naught for T1 we will replace it before making a second pick so we have this new training data set T1 we can just do the same thing and we get T2 so let's say this time we really got a lot of twos through this random sample and then so on and so forth and then let's say the nth training data set looks something like this so now notice instead of just a single training data set we have a collection of training data sets which allows us to train a collection of decision trees so that could look something like this and then just like we saw on that first slide we can combine the predictions from each of these decision trees to produce our final prediction so one of the most popular machine learning algorithms that uses bagging is called random forest and I'm not going to get into all the details of that in this video but for those who are interested be sure to check out the blog published in towards data science where I give a few more details about random Forest additionally there's a really nice paper by the creator of the random Forest algorithm Freeman who is very well known for his work on decision trees and decision tree ensembles so I definitely recommend reading that paper if you're into that kind of stuff the second type of decision tree Ensemble we're going to talk about uses something called boosting so boosting is completely different than bagging so here we will recursively train decision trees using an error-based re-weighting scheme no worries if this doesn't make any sense we're gonna walk through what I mean by this one step at a time so again imagine we start with this training data set T naught but now we're going to introduce this concept of weight and this is exactly what it sounds like essentially we can give different records in our data set more weight or more importance when it comes to developing our model so we'll just start with T naught in such a way that all the weights are equal so every record every example is equally important and then we can use this training data set to create a decision tree we'll call it h naught but now we can create another training data set based on the performance of this decision tree and so that might look something like this so notice the different colors here all this is showing is that records one and four were correctly classified in this binary classification problem we're trying to solve while records two three and five were incorrectly classified and so what we can do now is we can decrease the weight of Records one and four and increase the weights of two three and five and then with that we have this new training data set T1 and we can train a new decision tree we'll call it H1 and then we can repeat this process we evaluate the predictions of H1 we see which records were correctly predicted which records were incorrectly predicted update their weights accordingly create another decision tree and so on and so forth and we can do this for however long that we want now notice again we have a collection of decision trees and we can just aggregate the predictions of these decision trees into a single estimate the first technique that really introduced this idea of boosting is called add a boost or adaptive boosting and so when people are talking about boosting they're typically talking about a process similar to what we see in Ada boost which is essentially what I walk through here just explaining some of the details a bit more here so basically all that added boost does is it combines each of these decision trees into a linear model and weights each of the decision tree predictions based on this Alpha value and then the alpha value is just proportional to the decision trees performance and here what I've written out is the specific a re-weighting scheme used in add a boost so notice that incorrectly classified records will get a weight update proportional to this value while correctly predicted records will have their weight updated proportional to this Factor over here ever since ataboost was introduced in the mid 90s there have been two major Innovations around this idea of boosting the first of which is called gradient boosting so instead of talking about the specific re-weighting scheme and details of add a boost gradient boosting just provides a more generalized framework where you can take any differentiable loss function and Define this gradient and Define some boosting strategy from it so the second major Innovation comes from a library called xgboost which basically makes the gradient boosting idea much more scalable and computationally efficient through a set of different heuristics and so that's all I'm going to say about those I talk a little bit more about gradient boosting and XG boost in the blog associated with this video and I also have the original references is for those ideas in the blog as well so now coming back to this question of why are decision tree ensembles better than just single decision trees and if I were to summarize everything into a single picture it would be something like this we're essentially going away from point estimates toward population estimates so what I mean by this is instead of just having a single number as our prediction from our decision tree we now have a population of predictions from our decision tree Ensemble which has these three main benefits that I'm going to talk about now so the first key benefit is that decision tree ensembles are much more robust to the overfitting problem than single decision trees so if you saw the previous video of this series we saw that overfitting is when your machine learning model essentially over optimizes to a single training data set in such a way that when you try to apply it to new data it doesn't work as well this turns out to be a pretty big problem for just single decision trees but this problem for a lot of cases tends to go away when you start aggregating groups of decision trees together the second key benefit of decision tree ensembles are more robust feature importance rankings so importance rankings are a critical output of any decision tree based method and so these can be based on things like Information Gain or out-of-bag error if we're talking about random forest or any number of different ways that we want to Define importance some of these quantities that we can use to define importance are only possible through tree Ensemble based approaches so if one example is out of bag error defined in the random Forest algorithm so I won't get into all the details of that if you're interested I talk a little bit about it in the blog but all that to say is that tree Ensemble based approaches not only open up more ways of defining importance but now kind of going back to this idea of population estimates we're not just relying on the importance of our features from one view of the data essentially from one decision tree but through having a wide collection of decision trees our importance rankings can become robust and then finally the last key benefit of decision tree ensembles is through population estimates we now have a pretty straightforward way to quantify our confidence or uncertainty in our model's predictions and anytime you want to use your model in the real world or there are physical consequences for your model it's good to have some measure of confidence or uncertainty just so you know your exposure while if you just have a point estimate you don't really know the confidence of your prediction it could be zero uncertainty or it can be infinite uncertainty so that's another case where population estimates are very beneficial okay now we're going to jump into some example code so here we're going to do breast cancer prediction using decision tree ensembles like always we're going to use the sklearn python Library which is one of the most popular machine learning python libraries there is and then the data that we're going to use for this example comes from the UCI machine learning repository so the first step is we import our python libraries so just kind of running through quickly we have pandas to help Wrangle our data numpy to do some math matplotlib will help us make some nice visualizations sklearn data sets so this is the data set while it's originally from the UCI machine learning repository sklearn has this data set readily available for us and this is my short apology for using a toy data set and not wrangling a data set from The Real World but the point of this is to focus on the tree ensembles and not the data preparation step so I hope you'll forgive me next I imported smote so this is optional I haven't commented out for the results we're going to see here but if you're interested head over to the GitHub uncomment this code block and you'll be able to see what the results are doing here and if you recall we use smote in the previous example to balance our imbalance data set and then finally we import a whole bunch of other things from sklearn this handy function to create a training and testing data set decision tree classifier and then we import all the different tree Ensemble approaches that we've talked about it's a random Forest add a boost and gradient boosting and then finally we import three different evaluation metrics for our decision trees basically what we're going to do in this example is we're going to train four different models using these four different approaches and we're just going to compare their performance sklearn makes it super easy to import this toy data set just one line of code we have it in a pandas data frame and then it's always a good practice to plot the histograms of your data here are all the predictor variables we have at our disposal and then here is our Target variable so this kind of goes back to the imbalance data set idea we see that there are a lot more cases where the breast tumor is benign as opposed to malignant and so while we could apply smote here to synthetically over sample the minority class we're not going to do anything here and see how the four different models hold up okay next we Define our predictor and Target variables so this is basically grabbing everything but the last variable name in our data frame this is grabbing the very last variable name in the data frame and then this is just creating two data frames based on the variable names and then with that we can easily create our training and testing data sets here we use the 80 20 split sklearn makes this super easy a bit of a warning with this next block of code because I just inherently refuse to copy and paste code over and over again I use what I've heard referenced as automatic code generation and basically all that's going to happen here is instead of explicitly writing the python command out and then copy pasting changing one thing copy pasting changing one thing and so on you can Define your python command as a string and then use this handy execute function to execute the command so while this might conceal kind of like what's going on here this is is just a much cleaner way and convenient way that I found to write code and I'm sure there are going to be some programmers out there that are going to yell at me for doing this but I haven't run into any major issues writing code this way so be curious to hear other people's thoughts on this while it might conceal kind of what's going on here I have everything printed out so essentially what's being dynamically written here is this single line of code so all that's happening is four different models are being created using the four different things we imported from sklearn so decision tree classifier is our loan decision tree the random Forest classifier uses random Forest add a boost and then gradient boosting so all these different models are initialized in this clf data structure and then each of these are stored in a list so now we have a list of models as you can see here the automatic code generation gets even worse here because we have a lot of combinatorics happening so we have four different models we have two different data sets and we have three different performance metrics we want to Define for each of these cases so this code block may not make a whole lot of sense but I printed everything that's being dynamically written here so let's just look at these first three lines so all that's happening is we're going one model at a time for the models in our list and we're going to apply it to the training data set and get a prediction and then we're going to compute the Precision recall in F1 for this model applied to the training data set so that's what these three lines of code do here and then we do the same exact thing with the same exact model but now for the testing data set so we get a prediction compute the Precision recall F1 score and we just append everything to the same list and so on for each model the results get stored in this performance dict it's just a dictionary that we initialized here where the keys are the different model names and the values are all the performance metrics relevant to that model and then after all that this dictionary gets all fill build up for all four models for all three evaluation metrics and for both data sets and we can just convert it all to a pandas data frame and so if this is all confusing and doesn't make any sense don't worry about it it doesn't really matter what matters is this final output here where we can just simply look at all four of our models and all the different performance metrics that we have and just compare them together so we can see that all four models performed perfectly on the training data set so we can see the Precision recall F1 score the training data set is one but the real test is looking at the performance metrics for the testing data set and so in this context we can use as a rule of thumb the difference in performance between the training and testing data sets is indicative of overfitting put that more simply the smaller these values are the more overfitting that model is showing based on that heuristic we can see that the decision tree classifier seems to be overfitting most because it has the worst performance when applied to the testing data that so on the other side of it random forest and gradient boostings seem to have the best performance when looking at the F1 score a close second is at a boost which has F1 score of zero nine six three and so these results make sense they agree with this story and intuition that tree ensembles are more robust to overfitting than single decision trees alone so two things I did not explore in this example which are obvious next steps are looking at the feature importance rankings for all four models and then additionally doing some kind of uncertainty estimate for each of these models okay so that's basically it if you enjoyed this video and you want to learn more be sure to check out the blog published in towards data science Linked In the description below again feel free to steal the code from the GitHub repository referenced here also please consider liking subscribing and sharing your thoughts in the comments section below and as always thank you for your time and thanks for watching
B6a64wdD7Zs,2023-02-18T17:30:14.000000,An Introduction to Decision Trees | Gini Impurity & Python Code,hey everyone I'm Shaw and in this video I'm going to be giving a brief introduction to decision trees so we might ask ourselves what are decision trees and put very simply a decision tree is something we can use to make predictions via a series of yes or no questions so let's look at a concrete example so let's say we want to predict whether I'm going to drink tea or coffee and to make that prediction we can use a decision tree like the one shown here and so the way this works is we start at the top of the decision tree here and we just answered the following yes or no questions so first we ask is it after 4 pm and if yes we follow this Arrow here and if no we follow this Arrow here so let's say yes it is after 4 pm then our answer is I will drink tea but if the answer was no we would follow this arrow and end up at another yes or no question so this one asks if I got more than 6 hours of sleep last night if the answer is yes we go with t once again but if the answer is no we're going to go with coffee so this is a very simple and straightforward way we can make predictions using just two pieces of information namely the time of day and the amount of sleep I got last night and so just to talk a little terminology because it will come up later these rectangles that we're seeing throughout the decision tree are called nodes and we have different types of nodes so for example the node that sits at the top of the decision tree is called the root node over here in green we have what is called a leaf node and these are nodes that I don't ask any yes or no questions and where we can assign our prediction and then we have splitting nodes which ask yes or no questions but are not the root node and so another way to think about decision trees is graphically and I personally find this a much more intuitive way to think about things so here on the right we have the example from before where we have two pieces of information namely time of day which we're plotting on this x-axis and the amount of sleep I got last night which we're plotting on the y-axis so another way we can represent what the decision tree is doing is by partitioning this predictor space meaning the space defined by R2 predictor variables time of day and hours of sleep partitioning this predictor space into different sections and assigning a label to each section so what this will look like for splitting on 4 pm and 6 hours of sleep is something like this so here's 4 pm we draw a line here and then here's six hours of sleep we draw another line here and now we just look at the leaf nodes for each of these splits and assign a label to each section so intuitively this is all a decision tree is doing it's taking the predictor space splitting it into the different sections and then assigning a label to each section so now that we have a basic understanding of what decision trees are and and an intuition for how they work a natural question is how can we bring this into practice namely how can I use a decision tree in the real world and so this is a great question and as it turns out we can use decision trees in practice by developing them from data so put another way we can learn decision tree structure from data so I'm going to walk through an example here just to give you a qualitative sense of how this works and I'll just kind of start with the disclaimer that there are many ways to grow decision trees from data but what I'm going to describe here is a widely used methodology all right so before getting into it I need to introduce the concept of Genie impurity and so I'm just throwing the equation up here for completeness and for those that I think in terms of math and just describing what this is it's saying that the genie impurity of a sample s like this sample right here is equal to 1 minus the sum over p i squared and so Pi corresponds to the probability of the I class looking at this example here we have two possible classes tea or coffee so the genie impurity of this sample here would simply be 1 minus the probability of t squared minus the probability of coffee squared and if that doesn't make any sense no worries we can think of the genie impurity in terms of its extremes namely its minimum and maximum value so visualizing this we have minimum impurity whenever every class in our sample is identical so either every class in the sample is T or every class in the sample is coffee on the other end of the spectrum we have maximum impurity when each class is equally likely and so for those of you familiar with information Theory or the concept of entropy you'll notice that this quantity Genie impurity is actually proportional with information entropy okay so you might be saying Xiao why are you talking about this Genie impurity what does this have to do with decision trees and I'm glad you asked that because we can use the genie impurity to learn decision tree structure from data and so the goal when growing decision trees is to use our predictor variables to split our data such that the overall Genie impurity is minimized so essentially growing a decision tree is an optimization problem and in the following slides I'm going to walk through a popular way of doing this so just putting aside our minimum and maximum impurity just as a reference and then consider this data set on the right where each of the rows or records of this data set are represented in this sample over here so each icon in this box corresponds to a different record in this table so let's say like this icon corresponds to the first row with this T this coffee icon corresponds to the second row and so on and so forth so again our goal is to use our predictor variables time and amount of sleep to split our data such that we minimize the overall Genie impurity a sort of Brute Force way of doing this is evaluating every possible split option that we have in our data set for example consider time we can just go through one record at a time and split based on each value we observe in this table so for example we take this first value of 721 am and we can split our data based on time being less than or equal to 721 am and the resulting split would look like this so here we have a sample with just one record this first one here and then we have everything else in this other sample here and then we can evaluate this split option by Computing its Genie impurity so basically what I mean by that is we calculate the genie and purity of this sample and the genie impurity of this sample and then we take their weighted average so here we just have one class so that is actually minimum impurity of zero and then this one is a bit of a mix so it's going to be pretty close to maximum impurity and then we will wait the average by the number of Records in each sample so this one will have a very low weight because it only consists of one record and then this one will have a very high weight because there are a lot of Records in this box over here so this split will give us a number corresponding to its genium Purity and then we can just continue this process and so now we split on 8 47 am calculate the average genium Purity we split on 9 30 am calculate average Genie impurity and so on and so forth for every possible value of time in this data set and then we do the same thing for amount of sleep we look at our first option which is 5.5 we get something like this 5.9 595 and so on and so forth for every single value we observe in our data set and so let's say after doing this and calculating all these average genium Purity values we discovered that the split option of sleep less than or equal to 6.75 hours is the optimal value this gives us the smallest Genie and purity of all the different split options that we observe in our data set and so now notice this node over here is pure it has minimum impurity so it doesn't really make sense to split this sample further but on the left hand side we still have some impurity in this node and we can do additional splits so let's do that here so now we have a smaller data set so instead of starting with all the data in our table we just have a subset shown by this smaller table over here and then we just repeat the same exact process as before we evaluate every split option let's say that after evaluating all the split options we discover that splitting on time less than or equal to 145 PM gives us the smallest Genie impurity and now notice again we have a pure node here but we still have some impurity here so we can just keep splitting the data until every single node is pure so meaning every single node just has a single class in it and it has Genie impurity equal to zero so at first while this might sound great you might think oh we can have a perfect classifier we can have a decision tree that is absolutely perfect however this is not such a great idea because this brings up a very well-known problem in machine learning known as the overfitting problem and overfitting is when you learn a machine learning model based on some data set but your model becomes over optimized on the data set it was trained on and when you try to apply that model to new data that it's never seen before you'll find that your model is actually very inaccurate and so instead of allowing our decision tree to grow without end and become hyper optimized to our data set we can control the growth of our decision tree using what are called hyper parameters put simply hyper parameters are values that constrain the growth of our decision tree and so just to look at a few examples let's say we have this original decision Tree on the left hand side but we find that it doesn't generalize well to different data sets and we actually want to tune it to this simpler decision Tree on the right hand side so in order to do this we can actually use hyper parameters and so here I'm going to show three different hyper parameters we can use to go from this original decision Tree on the left to the tuned decision Tree on the right so first we have the maximum number of splits so in the original decision tree you see that we have two splits happening but we could have easily constrained the size of this decision Tree by setting the max number of splits equal to one another hyper parameter we could have used was the minimum Leaf size so in the original decision tree we have a minimum Leaf size of two but if we would have set the minimum Leaf size to something like five this additional split could have never happened and then finally we could have controlled the number of splitting variables so in the decision tree from the previous slide we split on both hours of sleep and time of day but if we set the number of splitting variables to 1 decorative constrained our decision tree to the sex and so the key point is hyper parameter tuning can help avoid this overfitting problem and improve your decision trees generalizability so its ability to perform well on new data and then as a final note although this is a very widely used way to develop decision trees this is not the only way to develop decision trees and I talk a little bit more about alternative strategies for developing decision trees in the blog associated with this video so if you're interested in that be sure to check that out okay so with the theoretical Foundation set let's dive into a concrete example with code and data from The Real World so here we're going to do sepsis survival prediction using decision trees and so here we're going to use the scikit-learn python Library which is a very popular machine learning library in Python and then we're going to use a data set from the UCI machine learning Repository all this code that I'm going to walk through here is available in the GitHub repository which I will also Link in the description below the first step is we're going to import some helpful python libraries so pandas is going to help us with formatting our data numpy is helpful for doing some math and calculations we use matplotlib to make visualizations we import several things from sklearn and then finally we're going to import this smote function to help balance our data set which we will talk about here soon okay so with our libraries imported we can read in our data set so with pandas this is just one line of code and the CSV file used here is available at the GitHub repo as well as two additional CSV files that can be used for validating our decision tree okay so with our data read in we can plot the histograms for every variable in our data set so here we just have four variables the age of the patient whether the patient is male or female the number of sepsis episodes that the patient has experienced and then finally the outcome variable which is an indicator of whether the patient survived or died and so the first thing that we should notice here is that we have a imbalanced data set so what that means is we have a lot more patients that survive than that diet well this is a good thing from a human perspective this is not a good thing from a model development perspective because if we train our decision Tree on this data directly basically what will happen is that our decision tree will overestimate the a live class and underestimate the dead class and so one way we can correct this is using smote which stands for synthetic minority class over sampling technique I think I got that right and it's basically a way to over sample the minority class to make it more Equitable with the majority class and ultimately reduce bias in our decision tree so this is pretty straightforward so here we're just grabbing the predictor variable names and the outcome some variable name here we store the predictor and outcome variables into two pandas data frames namely X and Y and then finally with just one line of code we can use smote to over sample the minority class and then we can plot the results using matplotlib and look at that we have a more balanced data set all right so now that we have balanced our data set we can create our training in testing data sets and so basically the point of this is our training data set will be used to grow our decision tree and then the testing data set will be used to evaluate its performance and so here we use the 80 20 split so 80 of the data is used for training 20 is used for testing and then with that growing the decision tree is very straightforward we can do it with just two lines of code as we do here so first step is we initialize the decision tree classifier and then the second step is we fit our decision tree to our data and then that's it so we have our decision tree we can take a look at it using this built-in functionality inside like it learn and this is what it looks like needless to say this is a very big decision tree and it's hard to think that a doctor or any medical staff will be able to interpret this decision tree to extract anything meaningful but let's just put that point aside for now and evaluate our decision trees performance and so the way we could do this is using a confusion Matrix and so just looking at this one on the left what this is showing is the number of troop negatives true positives false positives and false negatives so in other words this is just comparing the decision trees predictions to the ground truth and so I don't want to get into too many details of interpreting confusion matrices and whatnot for this discussion I'll say when it comes to confusion matrices you generally want to maximize the diagonal elements and minimize the off diagonal element what that means is we want our predictions and the ground truth to agree as much as possible and we want them to disagree as little as possible possible so we see for both the training and testing data sets the performance seems reasonably well and then another way to evaluate performance is using three different metrics namely Precision recall and the F1 score which are defined by these equations on the left here so Precision is basically the number of true positives scaled by the sum of the true positives and false positives recall is a similar kind of thing but it is scaled by the number of true positives and false negatives and then the F1 score is the harmonic mean of the two and so in this case I'd say the Precision is something we care more about than recall because in this context we probably care more about false positives than false negatives and so the reason being is a false positive corresponds to the case where the decision tree predicted that the patient would survive and they did not and so for using this decision tree to quantify patient risk then there's a lot more downside to predicting that a patient would survive five that didn't then predicting that a patient would die who doesn't and so clearly which one of these metrics you want to look at and care about is highly context dependent sometimes you care equally about false positives and false negatives sometimes you care more about false negatives sometimes you might care more about false positives like the case here and so which metric you use to evaluate your model depends on the problem and the context you're looking at and then here's a handy function available in the GitHub that generates all this okay so coming back to this massive decision tree from a couple slides ago this brings up once again the overfitting problem so while this decision tree might work reasonably well on the data set here a decision tree that looks like this is prone to overfitting meaning it may not generalize well to new data sets and So to avoid this problem we can use hyper parameters and so here we're just going to use one hyper parameter which is the maximum depth and so here we're just going to set that equal to three so we ensure that the decision tree doesn't get too many branches setting this is super easy with sklearn we just pass this input argument into our decision tree classifier and then we fit our model just like we did before and then outcomes our tuned decision tree and so plotting out the decision tree it looks like this and already we can see this is much more interpretable we can actually read the text here and so just kind of looking at all these different splitting nodes we're seeing the age predictor is appearing a lot so this is indicating that age seems to be a very important risk factor when it comes to sepsis survival prediction and then also right here we're seeing that sex is playing a role which is a little surprising that we're not seeing number of episodes and surely if we were to increase the max depth or do some other hyper parameter tuning we would see that variable appear in additional splits okay so our hyper parameter tuned decision tree seems more comprehensible but how does it perform and so we can once again look at the confusion Matrix and those three performance match tricks and surprisingly the Precision is actually a little better for this hyper perimeter tune decision tree than the fully grown decision tree but notice that the recall in F1 scores are significantly lower than what we saw before but I would say in this case we may not care about that because Precision like I was saying before might be the metric that we're really trying to optimize in this context because we likely will want to wait false positives more than false negatives so that was a tremendous amount of information if you still want to read more check out the blog published in towards data science on medium be sure to check out the GitHub repo and steal the code and train your own decision tree model with your own hyper parameter optimizations and if you enjoyed this content please consider liking subscribing and sharing your thoughts in the comment section below and as always thank you for your time and thanks for watching
CTu8JNLq5ZU,2023-01-28T15:06:57.000000,5 Reasons Why Every Data Scientist Should Consider Freelancing,are you feeling stuck you got the degree you got the job you're working in data science you're killing it but for some reason you don't feel like you're progressing in life you're not growing you're not developing the skills that you want to develop or maybe you haven't even gotten the job yet you're desperately applying jobs you're getting scared all the tech companies are laying people off what am I supposed to do about this how am I supposed to get a data science child but just a bachelor's degree with my Master's Degree with my PhD even if you resonated with any of these random points I just spat out at you then this video is for you hey everyone I'm Shaw and as someone who has worked in data science as both a freelancer and a full-timer I just wanted to share a little bit of my experience for those pondering and reflecting on their data science journey and trying to figure out where they want to go with it toward that end in this video I'm going to break down five reasons why every data scientist should at least consider freelancing so for my view these points are beneficial to anyone in data science but perhaps especially so for those just getting started in my personal experience freelancing accelerated my development as a data scientist and it played a big part in helping me get my current full-time role as a data scientist and even if your goal isn't to get a full-time gig at a company freelancing can serve as a main source of income or even give you insights into different industries that might help you develop a new business or a minimum viable product so if you like this content and you want to see more about data science productivity entrepreneurship go ahead and hit that subscribe button right now so the YouTube algorithm will know that you want to see this face on your computer screen and with that let's get into the Five Points okay so the first reason why every data scientist should at least consider freelancing is to work on new problems one of the greatest powers of data science that I personally just love so much is that data science is so often context agnostic so what do I mean by that basically all I mean is you can take one method one Technique One Piece of code and apply it to multiple different use cases so like a very simple one is logistic regression you can use logistic regression to solve binary classification problems which comes up in credit risk modeling will the person I'm giving a loan to pay me back or analyzing customer retention what's the probability that our customer will continue our services next month or even marketing analytics what's the probability that a user will buy our product after they watch our ad and so these are completely different contexts we're talking about credit risk and and financial services lifetime customer value with retention analysis and then we're talking about ads and marketing with that last Point completely different context but you can use a single data science approach to solve all of these problems and these are just three off the top of my head there are countless use cases and applications for logistic regression or any common data science approach and so all that to say when it comes to freelancing you have the opportunity to use this basic toolkit in a wide range of contexts so when I was freelancing I was working as a graduate research assistant in the physics department but through my freelance work I gained exposure to different fields so one example was trying to classify sepsis sub-phenotype so basically subtypes of sepsis using unsupervised machine learning techniques that I've used in countless other contexts so that's one thing to consider when thinking about freelancing and data science you have the opportunity to work on different problems leveraging your experience and the skills that you've acquired in new contexts and it kind of enriches your understanding of those tools so every time you use a technique in a different context it helps you build an intuition of what else you can use that same method for so reason number two is developing soft skills so when you're freelancing you're really on your own you have to figure out how to put yourself out there how to get clients and how to communicate with a diverse set of people and so when you're freelancing you're trying to find gigs you often have to network you have to talk to people you have to connect with people and this really forces you to develop your soft skills you know sending out cold messages email etiquette talking to people at networking events reaching out to people on LinkedIn responding to potential clients reaching out to you because they see some work that you've done or they came across your profile on some freelancing website like upwork or Fiverr and so all these interactions all these reps really allow you to develop these soft skills that you may just not have as much opportunity to develop in a full-time role where the work is more delegated to you and is more stable and you're typically interacting with the same handful of people constantly as opposed to in a freelance role you're constantly interacting with new people and brushing up on those skills so the third reason is fine-tuning your pitch and this has some overlap with the second reason it comes down to your ability to communicate and connect with people but fine-tuning your pitch is really about selling yourself and what I mean by this is fine-tuning your resumes your cover letter or proposals and your interview skills so this is another area where full-time roles and freelance gigs have a big difference and It ultimately just comes down to timeliness so for the full-time role you know you submit your resume and cover letter you spend all this time on it but it's not uncommon to not hear back from that application for weeks or even months sometimes so it's really hard to kind of gauge how effective your resume and cover letter were and conveying your skill set and experience but on the flip side in freelancing the time scale is just much faster if you apply to a gig let's say on a site like upwork the feedback is typically much quicker if someone wants to work with you you will a lot of times hear from them within a few days and if you don't hear back from them in a few days that probably means they're moving forward without the candidates or the jobs no longer relevant or something like that and so ultimately what this means is in freelancing as opposed to full-time roles you really can get a lot of reps in on your resume and cover letter and get much faster feedback and so what this allows you to do is fine tune your resume and cover letter to convey your skills and experience more effectively and this is something I definitely benefited from so I was freelancing in grad school so constantly fine-tuning my resume and my cover letter and then eventually when I graduated and decided to apply for a full-time role my resume and cover letter were in a pretty good spot and I could just leverage what I learned from freelancing to apply to the full-time gig so I would say to anyone trying to break into data science you know you just graduated or you're about to graduate I would recommend freelancing even if you don't get any gigs and don't get any work through it at least you get these reps you get to fine-tune your resume and your cover letter and hopefully your interview skills through chatting with potential clients and you can leverage this experience in these reps and the feedback for applying to a full-time gig but overall the skill set of selling yourself being able to communicate your skill set and how your experience and skills are relevant to solving other people's problems is a very valuable skill set to have and essentially being able to sell yourself and your ideas is something that will be valuable in whatever context you find yourself in reason number four is flexibility and autonomy and so this is one of the greatest benefits of freelancing and I feel one of the main reasons why people are so attracted to it and freelancing you essentially choose what you work on because you choose the clients that you work with and moreover freelancing gigs are typically on a much shorter time scale than full-time roles so you could be working with a client on a month by month basis and it could be going great for six months but then at a certain point the work May no longer be relevant or getting overloaded on contracts with a handful of other clients and then you have the option and opportunity to reduce your workload or refocus your efforts toward a specific type of work and also you don't just get to choose what you work on but you typically get to choose where you work how you work when you work and this is something that a lot of people have value in people who greatly value their autonomy their flexibility their freedom you know maybe they don't want to be bound to a certain city they want to be able to travel and you know this was something that got big during covid you know people were getting gigs online or during remote work they were living for months in different countries you know living in Europe or South America or Asia or something like that and so for people that that that lifestyle is appealing to them freelancing is a great option for that okay so the fifth reason is networking and so I kind of touched on this before but here when I'm specifically talking about is building new relationships and new connections and so through my freelancing I've met a wide range of people that has given me insight into Worlds that I didn't even know existed so I've worked with medical doctors clinicians with people working in Special Forces military police officers business people you know so many different walks of life and backgrounds and has really enriched my own experience and my understanding of the world which I find a lot of value in relationships and learning from people is something I give a lot of weight to so this is one aspect of freelancing that I really enjoy okay and if those five reasons were not enough to make you consider freelancing in data science I've got two bonus tips to share so the first bonus tip is money so even if you don't really care about developing your technical skills expanding your experience and Horizons developing your soft skills building new relationships what else did I talk about fine-tuning your pitches your ability to sell yourself all these different reasons you could always just do it for the money and most of the time freelancing gigs are much more lucrative than full-time roles so just speaking from my personal experience before I entered into my current full-time role I had two offers on the table I had the my current role and I had a essentially a contractor role which could have been full-time and just comparing the pay of the two roles the contract rule paid almost twice as much as my full-time gig I would say 80 percent yeah paid about 80 percent more than my full-time gig which is a lot of money I'm just saying that to give you an idea of how much more you could get as a freelancer as opposed to a full-time role and to those who are saying like oh if freelancing is so great why didn't you take that why don't you take the money and that was just a personal decision for me when I graduated I'd done the freelance stuff I'd worked in research but I never worked at a large company as part of like a big data science team the biggest team I had worked with was my research team which was about 12 people I work on a data science and analytics team that's I want to say like 100 people if not more and so for me the reason I went with the full-time role is because it was a new experience for me it was also the opportunity to learn from other data scientists and data analysts that have been working in the field much longer than I have and then the last thing I'll say about the money is that you know the great thing about freelance is that you can customize the freelance workload so you can definitely be a full-time freelancer and reap all the benefits there but if you're just trying to make some extra cash on the side and you have a full-time role you can probably just pick up one or so contracts every so often if you just want to make some extra cash on the side and the second bonus tip is that freelancing gives you options if you have freelancing experience or you've done it in the past you always have that option on the table so say you're working full time and you want to make some extra cash you can always just go to freelancing or kind of given all the res recent Tech layoffs it's kind of a scary thing you could just wake up one day and your full-time employer says we don't need you anymore or we don't see the value in data science anymore and they lay you off now what are you going to do well if you're freelancing on the side or a freelancer in the past you have an immediate thing you can fall back on until you can either work up your client base or find another full-time role okay so that's basically it so in this video I give you five reasons why every data scientist should at least consider freelancing with two additional bonus tips and so if you enjoyed this content you want to read more take a look at the blog associated with this video published in towards data science on medium if you enjoyed this content please consider liking subscribing and sharing your thoughts in the comments section below and as always thank you for your time and thanks for watching
O72uByJlnMw,2023-01-14T15:08:23.000000,Causal Effects via Regression w/ Python Code,hey everyone welcome back my name is Shaheen and today we're going to continue the series on causal effects and talk about causal effects via regression this will actually be the fifth video in a larger Series so if you're just joining in now check out the other videos to get a larger context of what we're talking about here so the title of this talk is causal effects via regression so we might ask ourselves what is regression and regression is very simple it's a very widely used technique in science and business economics progression is simply a way to learn relationships between variables using data and one of the most popular regression techniques is linear regression which you may have even seen back in high school in linear regression we use data to develop a linear model between two or more variables so a super simple example is if we have a variable X and we want to find its linear relationship with another variable y we can just Express their relationship through this equation so y equals Theta X plus b we know X and Y so through the regression process we actually learn values for Theta and B that best fits our data and so the result of a regression technique is what we call a model so a model is essentially something that we can use to make predictions so this is an example of a linear model so if you give me a value for x I can make a prediction about the value of y so where the causal effects come in is if we have a linear model like this and we have a variable X and a variable y we can interpret this coefficient Theta as the causal effect of X on Y and so this is an important note here because if you've been watching previous videos of this series you might notice that this is a fundamentally different way to define causal effects as we've seen in past videos so namely before we we defined the causal effect through the average treatment effect which was essentially the difference in mean outcomes for two different groups a treatment group and a control group here where x equals one is representing the situation where someone receives a treatment like taking a pill and x equals zero represents the situation where someone does not receive a treatment or does not take a pill for example so that's an important thing to know so in this video I'm going to talk about three regression based techniques for estimating causal effects so the first is linear regression which we've kind of already touched on but I'm going to talk more about in the next slide we have a more sophisticated technique called double machine learning and then finally we have another popular technique called meta Learners so for linear regression as we've seen here is where we train a linear model to predict the outcome variable Y in terms of the treatment variable X so again we have a linear model y equals Theta X plus b and so just defining some of these things Y is our outcome variable and X is our treatment variable so X could be whether someone takes a pill or not and Y can be an outcome such as headache status or something like that and then like I mentioned earlier we interpret Theta to be the causal effect of X on Y and then here we can interpret B to be an error term and so this is a very simplified view of the causal effect of X on y but we can go a step further and include confounders in our linear model so what do I mean by that so in this context a confounder let's call it Z is something that influences both X and Y so the way we can handle this situation is we can include our confounder Z in our linear model for y and then we can additionally write out a linear model for Z in terms of X or equivalently a linear model of X in terms of Z we can just move some terms around and get that equivalently but what this allows us to do is plug in our equation for Z into our equation for y and get a new equation for y that does not include our confounder and this gives us a different equation for our causal effect so no longer is our causal effect simply the coefficient of x in our linear model for y but it's got this extra term here which is representing the impact of the confounder and so this even still is a simplified example and for those who are interested there's a really nice book chapter by Andrew Gelman and Jennifer Hill called causal inference using regression on treatment variables which I will link in the comments and you can find the link in the blog associated with this video and I highly recommend this chapter it was a really helpful resource for me okay so linear regression is a very simple and easy to understand technique when which is helpful when using it in practice however the downside of using it is that since it is such a simplified technique it may not accurately capture the relationships between our variables of Interest so for example if X and Y are related by a non-linear relationship even a quadratic relationship or a cubic or sinusoidal or whatever it might be and so for these situations we can turn to more sophisticated techniques and one such technique is called double machine learning and so the idea here is we're going to estimate the treatment effect using a three-step process in which we develop two separate machine learning models and so double machine learning is a bit of a daunting technique at least it was for me when first hearing about the idea I came across the paper it's like 70 pages and just kind of reading through it's a lot of math a lot of abstract ideas but upon reading through the paper a couple times and then watching a really nice talk given by the first author Victor uh I'm not even gonna I'm gonna butcher this a really nice talk by Victor on YouTube which I can also link down below this whole process that may seem very daunting and complicated on the surface can be broken down into a simple three-step process so the first step is we train two regression models the first we can call the outcome model and the second we can call the treatment model so what this notation is representing f is a machine learning model that takes in covariate values and estimates the outcome value associated with those covariates and similarly G hat takes in covariate values and estimates the treatment value associated with those covariates so we train two machine learning models f-hat and G hat and we can use any machine learning model we like we can use linear regression we can use neural networks we can use decision trees you know whatever we like which is one of the big powers of this approach okay so the second step is we compute residuals so essentially what that is is the difference between the True Value Y and the estimated value from F hat and similarly the True Value X and the estimated value G hat and then this gives us our residual values u and v for each model respectively and then using these residuals we can compute the treatment effect directly and so we use this equation here which at first may seem like it's a lot going on but here we have essentially just three things we have our residual value for the treatment model V here and here we have our residual value for the outcome model U which is this thing and then we have our ground truth treatment values that we observe and so I is just indexing our samples and then n is the number of Records in our main sample so one important detail which I kind of left out of this three-step process but is critical to doing double machine learning is a process called crossfitting so what we do in crossfitting is essentially we split our data into two subsets we have our main sample an auxiliary sample and then we use the main sample to train one of our models and then we use the auxiliary sample to train our other model and then once we do that we switch so if we used our main sample to train f-hat and our auxiliary sample to train G hat we switch we then use our main sample to train G hat and the auxiliary sample to train f-hat and so through each of these steps like step two and step three these will give us a treatment effect and then so we can aggregate these two treatment effects to give us a final causal estimate this may seem like a unnecessary extra step but it's actually critical because some of the assumptions that it allows us to make it actually helps make this equation simpler than it would be otherwise and so if you want to learn more details about that I talk a little more about it in the blog and then the authors give a more detailed explanation in the paper paper that you can find on the archive here so the final regression based approach that I'm going to talk about are the so-called meta Learners so what these do essentially is use regression models to simulate unobserved outcomes and estimate causal effects so there are several types of meta learners but in this video I'll just talk about three which are the T learner or two learner the S learner or single learner and the X learner so what do these things mean so we'll start with the T learner so the process of using T Learners to estimate causal effects can be broken down into a two-step process so we start with our treatment group data so y x equals one is representing outcome values for people who received treatment and z x equals one are the covariates for people that received treatment and then similarly we have our control group which are the outcomes for people who do not receive treatment and covariates for people who did not receive treatment okay so once we have our two groups our treatment and control groups the first step is to train two machine learning models that is estimating the outcome Y for the treatment group in terms of their covariates and then we similarly estimate the outcome value for the control group in terms of covariate so this gives us two machine learning models f-hat and G hat and then the last step is we compute the causal effects so here we Define the individual treatment effect which we actually saw in the first video of the series titled causal effects but defining it in this meta learner framework we have the individual treatment effect of the ith record is equal to the outcome value estimated by the treatment group model for the ith record minus the estimated outcome value from the control group model for the ith record saying it another way this is the estimated outcome for the ith individual if they receive treatment minus the estimated outcome value of the iPhone individual if they do not receive treatment and so this difference gives us the individual treatment effect which we can then aggregate in this way to get the average treatment effect and then going a step further we can Define the conditional average treatment effect which is not defined for any particular individual but for a particular subpopulation defined by the covariate values that we plug into this equation okay so next we have the S learner so with s Learners we don't need to split our data into a treatment and control group we can just use all data from all units and then we use this data to train a single model we'll call it f hat which estimates the outcome variable y using both the treatment variable and our covariance so notice we're not restricting the values of the treatment variable here it can take all different levels so one upside of this is that we're not restricted to Binary treatment variables zero one we can actually have multi-level treatment variables here okay and then in a very similar way we can compute the effects so we Define the individual treatment effect in a similar way as we did with the T learner so the individual treatment effect for the ith record is defined as the estimated outcome when the ith unit receives treatment minus the estimated outcome when the ith unit does not receive treatment and so instead of having two separate models one for the treatment group and one for the control group we can modulate our outcome value predictions by manually setting our treatment input to be one or zero or whatever treatment value we like and then we can aggregate individual treatment effects using the average human fact exactly how we did before and then we can Define the conditional average treatment effect in a very similar way as before but now notice we're manually setting our first input value as one for the first term and zero for the second term and finally we have the x-learner and so this has overlap with with the T learner process we saw before but it takes things a step further so first we train two models one for the treatment group and one for the control group just like we did with the T learner then we impute outcomes and compute the individual treatment effects and so we have like this Crossing term here we use the control group model to impute outcomes so these are unobserved outcomes for the treatment group and so since the treatment group got the treatment we know their outcomes given treatment but we didn't observe their outcomes in the reality where they didn't take treatment and so that's what this model G allows us to do it allows us to simulate these unobserved outcomes and compute the individual treatment effect which we're calling D i1 here and then we do a similar thing for the control group so since we observed what the outcome values are for the control group given that they don't take treatment we simulate what they would have been and if they did receive treatment using our treatment group Model F hat and then the next step is we train models to specifically predict our individual treatment effects and so we'll call these models Tau hat one and Tau hat zero and so this will use covariate values Z to predict our individual treatment effects D1 and d0 and then finally we can Define our conditional average treatment effect estimator by combining Tau hat one and Tau hat zero using a weight function W so in the paper by kunzel at all they suggest using a propensity score for this weight function W and we talked about propensity scores in a earlier video of this series which I will link down below for those who are interested and then here the conditional average treatment effect is equivalent to Tau so that's just how we Define it and so if you want to learn more about meta learners I found this resource very helpful the paper titled metal learners for estimating heterogeneous treatment effects using machine learning available on the archive I also talk more about this in the blog post associated with this video okay so now we're going to dive into a concrete example with code so this is a example that I've probably overused at this point but we're going to estimate the treatment effect of grad school on income so here we're again going to use the python do y Library which you can find at this link here and then the data source is the UCI machine learning repository so here's the data source it's a open access data source so you should be able to grab the data from here or additionally I have a kind of pickled version that is a pandas data frame available on the GitHub okay so first step is we import our modules and load in our data and so importing pickles so we can bring in our data econ ml to do the double machine learning stuff and I think the metal learner stuff but we'll find out soon and then do wide to do a lot of the heavy lifting for the causal effect calculation and then we import things from sklearn to help us train our machine learning models okay so this is a critical step specific to the DUI Library we have to Define our causal model which essentially here is defining our treatment variable our outcome variable and our confounders or common causes and then we Define an S demand which is a recipe that tells us how to define our causal effects with this ground set we can go through each of the three techniques that I discussed earlier so we start with linear regression and the do I Library makes it super easy we essentially can get the linear regression causal estimate with one line of code so we input our s demand and then we just say the method that we want to use so we use linear regression and then backdoor and if you want to learn more about the backdoor Criterion I talk about that in a previous video of this series but through that the result of this is the average treatment effect based on linear regression is 0.2976 which is kind of in the ballpark what we've seen in example analyzes using this data set next we have double machine learning the very sophisticated and complicated approach relative to others but here we just again can get the double machine learning estimate through one line of code so we have our s demand then we're defining our method name which is double machine learning which we're grabbing from the econ ml library and then we're defining our method parameters so here I just use linear regression for everything because the data here are so simple we only have three variables two of them are binary so linear regression seem to give the best results here I would say double machine learning is a bit of overkill for this simple example but I think it would have a lot more utility and more complicated contexts and then here we see the average treatment effect is very similar to what we thought the linear regression approach which isn't surprising since we use linear regression for all the sub models here and then finally we have the X learner so this is one of the meta Learners we talked about and again it's the same type of thing we put in our s demand we Define our method name which is from the econ ml library next learner here and then we use a decision tree for the two sub-models in our xlearner and then that spits out an average treatment effect of about 0.20 okay so there's a bit of variation and I think that's one of the upsides of libraries like do y is that we can very easily and programmatically try a suite of different causal effect estimates for our data set and then we can have a wider distribution of average treatment effects or causal effects that we can use to inform our analyzes so that's it for now if you enjoyed this video please consider liking subscribing and sharing with others if you have any questions comments or suggestions for future videos please share those in the comments section below if you want to read more about this subject check out the associated blog on medium additionally feel free to steal the code from the GitHub repository link below and as always thank you for your time and thanks for watching
-5c1KO-JF_s,2022-12-15T13:33:49.000000,Smoothing Crypto Time Series with Wavelets | Real-world Data Project,hey everyone welcome back my name is Shaheen and in this video I'm going to be talking about how we can smooth Financial time series data using something called wavelets so this is a little different than past videos that you may have seen on my YouTube channel what I usually do is videos in more of like a lecture type format where I start talking about Theory and Concepts and then usually finish up with a concrete example with code but here I wanted to do something a little different a real life use case came up when an old friend reached out for some help on a project that he's working on and I thought it was such a wonderful example of what data science looks like in the real world so in this video I want to walk through the background and the problem that my friend was running into and walk through my thought process and how I arrived to the final solution for this problem so I'm going to go through this Jupiter notebook which kind of has step by step the the whole story of what happened in the this specific project and you can find this Jupiter notebook along with the data used in this example in the GitHub repo that is linked Below in the description so if you want to follow along or use these solutions for a problem that you're facing feel free to download the code there okay so let's get into it so like I mentioned the whole setup for this problem was my friend approached me because he was running into an issue in a project that he's working on so the project was actually a tool that he's developing to automatically trade cryptocurrencies and he has some strategy for executing these Buy sell decisions that I don't fully know the details of but he reached out to me because his strategy was all based on time series data that looks something like this and so the problem that he was facing is that his strategy was starting to give undesirable results due to the volatility of this signal so I mean just looking at this we we see like these crazy fast oscillations in the signal so if your goal is to develop a automated trading system you want it to be robust in a wide range of situations and so I can see just looking at how noisy this time series is how a strategy might break down so this brings up a really good point it highlights a mistake that a lot of data scientists make early on in that they're almost too eager to jump into the the coding jump into the technical details jump into the math the techniques the algorithms and don't take the time to take a step back and really ask themselves what is the problem we're trying to solve why are we interested in this problem what is it that we are truly after or what is it that the client is truly after so kind of in line with that my initial thoughts from this ask is what is this data it's very noisy and it turns out to be you know quite a specific signal that's being extracted from cryptocurrency prices it's something like the time average z-score over the past 45 minutes of a currency's trading price so if that doesn't make any sense don't worry it's not going to be critical to the larger point of this use case okay so my second thought was why do you need to use this particular time series so my initial thought is if you're relying on a signal for your strategy and that signal is fundamentally unreliable or noisy are there other data that we can look at and use for our strategy and so in this case there probably are however my friend had already developed his strategy that had worked well for him in the past based on this data so that's the reason he wanted to stick with this particular signal so even if these questions like in this case didn't change the direction of the analysis it's good to make sure you have an understanding of the context and the bigger picture and where your collab operator is coming from and asking for your help with a data science project after some conversation understanding what this data is getting a better idea of why he wanted to use this particular time series and at least in the short term kind of ruling out alternative strategies my first thought was well why don't you just apply a moving average so that's a very simple way to take a noisy signal like this and make it more smooth and so that brings us to the first solution and as it turned out this was a solution that my friend had already tried and it was still not giving him the results that he was looking for and so you know without knowing too many details about his technique there are just three possible issues that kind of jump out to me with this solution so looking at this orange signal here we still see that there are rapid changes in particular sections so while this section looks pretty smooth we can look back a little bit and we can see we still have of time ranges where there's rapid oscillation and that is conceivably something that'll make a automated Buy sell strategy very unstable the second thing is there are time shifting artifacts when we do moving averages so for example we can see that we have a peak in the original signal right here however in the moving averaged in the smoothed version of the signal using the moving average that Peak has actually shifted slightly to the right and so we have these time shifting artifacts anytime we do a moving average and it's kind of like this trade-off you know you can make your moving average window very small and you'll have less of these time shifting artifacts but the signal is going to stay noisy so we can actually look at that a little bit so if I made the window size 10 and run this we can see that the orange signal sticks pretty closely to the blue signal we don't have the time shifting artifact as much then on the other end we can make the window size very large and now we have a really smooth signal but now the time shifting artifacts are very large so this is likely not desirable especially if you have a program that's executing Buy sell decisions automatically it might be a little late to the party and then the third issue is the optimal window size for the moving average is likely Dynamic so just as we saw like this window size dramatically changes how our smooth signal looks and anytime you have to fix a parameter like this it's easy to let's say over fit to a example time series that you use in development so in other words what that means is we might pick a window size of 25 and agree that it looks good on this data but then let's say tomorrow we get new data and then 25 doesn't give us the desired outcome for these reasons the moving average is likely not going to be a optimal solution for this specific use case and so the first thing that came to mind which was a complete failure which is funny because I was so confident that this would solve the problem that my friend offered to he offered to pay me for my time and I was like oh no no this is going to take like five minutes like don't even like it's not even worth it's not even worth it like the coffee you just bought me is uh is equivalent to the amount of effort I'm gonna spend on this problem and I was wrong and thinking through it a little more it kind of becomes obvious why this would fail so this is a great example of when expectations and intuitions can fail like the the gut reaction the gut instinct can a lot of times mislead you when you're in kind of new territory and so uh let's kind of walk through the solution so at a high level the strategy is we have this signal let's fit it to a polynomial so we're talking about like x x squared x cubed x to the fourth so on and so forth and we're just gonna fit it to enough polynomials that we capture the properties we care about of the signal and don't capture the noise essentially and so this was a complete failure so here we fit it to 15 polynomials so what that means is we had a constant term x to the first power x squared x cubed x to the 4 all the way up to x to the 15. we just do a polynomial fit and then we plot the fit and compare it to the original signal and so while this well the polynomial fit is a lot smoother than the original signal it doesn't capture any useful information from the underlying signal so this was a complete failure so in the face of this failure I had another thought I thought of course polynomials aren't going to work here polynomials want to shoot up to infinity or shoot down to negative infinity and here we clearly have a signal that is oscillating around zero so when I think of something oscillating around zero I naturally think of Sines and cosines which brings up the concept of a Fourier transform if you're not familiar with the Fourier transform is one of the most powerful and widely used techniques in Signal processing Math and Science and so basically what we're doing conceptually is we're taking a signal so our blue signal here and we're going to decompose it into a set of Sines and cosines of different amplitudes and frequencies and so notice this is very similar to what we did with the polynomial fit so in the polynomial fit we take the signal fill it fit it to a set of polynomials you can conceptually think of what we're doing here as taking the signal and fitting it to a set of Sines and cosines the result of that captures a lot more of the underlying signal but we still have a lot of noise this actually has even more noise qualitatively than the moving average solution so if I were to pick between the moving average solution and this solution I would pick the moving average because it is both simple and qualitatively better and so solution number two attempt number two was yet another failure and so now I'm starting to think more polynomials didn't work they smoothed the signal but they kind of smoothed it so much that we lost all the useful information the Fourier Transformer may have captured the signal but there's still a lot of noise is there something that's in between these things and that's what inspired using the third technique which is the wavelet transform what we're doing here is the same idea as with the polynomial fit and the Fourier decomposition but instead of fitting our signal in terms of polynomials or in terms of Sines and cosines we're going to fit our signal to a set of wavelets which are essentially these wave-like things that are localized in time and if you want to learn more about the wavelet transform and the 40 transform I have some videos on that I have some blog posts on that so that should be a good primer if you're completely lately unfamiliar with the technique and so here we do the same concept we kind of fit our signal to a family of wavelets and then we just drop these higher order these more detailed coefficients of the wavelet transform and something remarkable happens in that we have a kind of Goldilocks smoothing situation you know it's not oversoothed like the polynomial fit but it's not too noisy like with the Fourier transform fit and so I shared the solution to my friend he seemed really excited about it really happy with it so he's testing it out in his platform now and hopefully it works out well and so there's one key Point here that is one aspect of data science that comes up a lot that gets me really excited which is that so often data science techniques and solution are application or context agnostic so for those of you who did see the video I posted on the wavelet transform you might notice that the steps that we ran through to smooth this financial time series is essentially the same exact process we used to detect our peaks in ECG data it's a great feeling when you can spend a lot of time working on a project and then somewhere down the line like two years later you come across a different project and you can basically copy and paste code from a project you did in the past even if the context is completely different like with analyzing ECG data versus Financial time series data okay so that's basically it I've also created a user-defined function to implement this strategy and that is also available at the GitHub repo and then here's a zoomed in version of the time series so how would you approach this problem leave your ideas and suggestions in the comments section below if you enjoyed this video please consider liking subscribing and sharing with others and as always thank you so much for your time and thanks for watching
ASU5HG5EqTM,2022-12-02T13:54:12.000000,Causal Effects via DAGs | How to Handle Unobserved Confounders,everyone welcome back my name is Shaheen and in this video I will be talking about causal effects via dags or in other words directed acyclic graphs so this video is part of a larger series on causal effects in the previous video of this series we defined causal effects using something called the do operator and then this led us to the concept of identifiability so that's where we're going to start our discussion today so identifiability is all about answering this question can the causal effect be evaluated from the given data so one way we can Define causal effects is via the average treatment effect or ate for short and in the previous video we defined the ate using this equation and so I don't want to spend too much time on it because I broke this down in the previous video but here we have an expectation value for an outcome variable y given an intervention in a treatment so that's what this you 2 operator is representing subtracted by the expectation value of the same outcome but for a different valued intervention so from this view the question of identifiability reduces to the following can the Interventional distribution be expressed in terms of observational distribution so on the left here we have an Interventional distribution which we discussed in the previous video and this is what we're seeing in our definition of the average treatment effect and on the right here we have a observational distribution so a key point is in the real world the data we collect is usually an observational distribution so expressing these Interventional distributions in terms of the things we actually measure is what identifiability is all about and so we actually saw an example of this in the previous video where we had this dag on the left directed acyclic graph representing the causal connections between different variables in our problem and we saw that we could express this in Interventional distribution in terms of observational data alone and this is the result that we saw what allowed us to do this in the previous video is that this is a special type of causal model it is a so-called markovian causal model so these are models that satisfy two conditions one they have no cycles and two there are no unmeasured noise terms that simultaneously cause two or more variables so we can make this a bit more concrete with some examples so the first one here this is an example of a causal model or dag that is not markovian because it has a cycle as we can see here we have another example of a non-markovian causal model so even though there are no Cycles we have this unmeasured variable X which is simultaneously causing two variables A and B but then on the far right here we have a markovian model because there are no cycles and we have unmeasured variables but they are not simultaneously causing two or more other variables and so a key point about markovian cause models and why we care about them is that if our causal model is markovian then the causal effect is always identifiable and we saw this in the previous video via the truncated factorization formula however Things become much more interesting when we have these unmeasured unobserved confounders so let's take a look at a concrete example of this suppose we have this markovian model but then suppose Jimmy forgot to turn on the Z2 sensor so actually the Z2 variable is unobserved and so now this model is not markovian because we have an unobserved variable that simultaneously is causing two other variables we can equivalently express this model by removing Z2 and connecting Z3 and Y with a bi-directed edge and so we started with a markovian model but now with just missing information about one variable the model is no longer markovian and we can see that in two ways the middle one has this unobserved variable that causes two or more other their variables and then this far right model actually has a cycle in it because we have this Loop here but as we saw in the previous video Even though this model is not markovian the causal effect is still identifiable in this case and so identifying causal effects of non-markovian models brings up two graphical criteria the backdoor Criterion and the front door Criterion basically these are graphical tests that we can apply to a directed acyclic graph either markovian or not to answer the question of identifiability or in other words evaluate whether the causal effect can be evaluated from the data that we are given so I'm going to Define each of these criteria and give a concrete example for each but first we need to Define two concepts the first concept is that of a backdoor path so a backdoor path between two nodes say X and Y is any path that starts with an arrow pointing into X and terminates at y so looking at this example on the left here all the back door paths in this dag are listed here so we have X to Z1 to Z3 to Y X to Z1 to Z3 to Z2 to Y X to Z3 to Y and then X to Z3 Z2 to Y notice that when talking about backdoor paths the only arrowheads would pay any attention to are the ones going into X the second concept is that of blocking and so a path p is said to be blocked by a set of nodes zi if and only if two conditions are satisfied the first is p contains a chain and this is an example of a chain here or a fork and then this is an example of a fork here such that b this middle variable is an element in the set z i and so this is what we might intuitively think of as blocking we have some flow of information so to speak from a to c via the chain or the fork and if we remove b or adjust for it we essentially block the flow of information between a and C the second point is that P contains a collider or in other words an inverted Fork such that b in any of its descendants are not in z i this point is a little less intuitive at least it was for me when first learning about this so the way to think about this is that a and C in a inverted Fork are statistically independent however if we condition on B we generate a statistical dependence between a and c and so this is also known as berkson's Paradox so basically all the second point is saying is that we are not creating a statistical dependence between a and C by conditioning on it or including it in our set z i and so here are some examples using the dag on the left here so Z3 blocks the path X z3y so this one because it is the center node in this Fork here another more interesting example is that Z1 blocks this path here because again we have a fork but notice that Z3 does not block this path because Z3 is an inverted fork or in other words a collider so if we were to include it in the blocking set it would essentially open up a statistical dependence between X and Y okay so with those two concepts of a back door path and blocking we can finally Define the back door criteria so a set of nodes zi satisfied the back door Criterion relative to X and Y if no node in x i is a descendant of X so essentially no node in this set zi that we're considering comes after X and two zi blocks every backdoor path between X and Y so the way we can evaluate this for this example tag on the left here is we first write down all the back door paths between X and Y so we have X the Z1 Z3 to y z E1 Z3 Z2 to Y and so on and we choose a set of nodes that block each of these four backdoor paths and so in this example we have two sets that satisfy this condition we have Z1 and Z3 so we see Z1 blocks this path as does Z3 Z1 blocks this path but Z3 does not block this path Z3 blocks this path and Z3 blocks this path and then almost trivially the set Z1 Z2 and Z3 also satisfies the backdoor Criterion and so a set of nodes that satisfies the backdoor Criterion is called a sufficient set and once we've identified our sufficient set we can immediately write down the Interventional distribution using this equation here where I is indexing our variables in our sufficient set so here we can either use Z1 or Z3 or Z1 Z2 and Z3 and this is exactly what we saw in the example from the previous video we initially had all three variables in our X expression for the Interventional distribution but then magically we were able to remove Z2 so hopefully now you can see how that trick was done and it's because Z1 and Z3 satisfy the backdoor Criterion so there's really no need to include Z2 in this expression so next we have the front door Criterion a set of nodes say Z I satisfy the front door Criterion relative to X and Y if these three conditions are satisfied one zi intercepts all directed paths from X to Y to all back door paths from X and each element in zi are blocked by the empty set and three all back door paths from z i to Y are blocked by X so there's a lot to chew on here so we're just going to go through each condition one by one for this example on the left here the first condition is zi intercepts all directed paths from X to Y so the way we can evaluate this is by writing down every single directed path from X to Y and just inspecting which sets of variables inner intercept each of these paths as it turns out for this example it's really simple we only have two directed paths from X to Y so we have X to Z1 to Y here and then we have X to Z2 to Y so really there's only one set that satisfies this first condition which is the set Z1 and Z2 but we're not done yet even though this set satisfies condition one we still need to check it against the other two conditions so looking at condition two we need to evaluate whether all the backdoor paths from X to z i are candidate set of variables are blocked by the empty set so the way to do this is we write down all the backdoor paths from X to each variable in z i so here we have the back door paths between x and z i and here and then on the right hand side we have all the back door paths between X and Z2 and so just looking at this backdoor path we have X to Z3 to Y to Z1 we see indeed that this Factor path is blocked by the empty set because we have a collider in y and and then actually looking at every other of the back door pads the collider at Y is present in all of them so indeed all the backdoor paths between X and Z1 and x and Z2 are blocked by the empty set that's a check mark for condition two and then finally condition three we need to evaluate whether all the back door paths from z i to Y are blocked by X so again we just write out all the backdoor paths between each variable in z i and y so just looking at this path our way of z i to X to Z3 to Y and then we can immediately see that X indeed does block this path because we have a chain where X is the center and there are no colliders and this is the same exact story for actually all of these backdoor paths we see X is either a chain or a fork and so this satisfies our definition of blocking from before okay so all three of these conditions are satisfied by our candidate set Z1 and Z2 which means we can write down the Interventional distribution directly using this equation and so this equation has a bit more going on than what we saw for the backdoor Criterion because we have two different summations so to make things a little more concrete I'll write out this first summation for the example we have here so since this set satisfies the front door Criterion we can write the Interventional distribution like this so notice we write out this sum the first term is for Z1 and the second term is for Z2 and this x here will correspond to the X on the left hand side so this will be a fixed value based on the intervention that we are looking at but notice we have this other X this x Prime and what this is denoting is the summation over all possible values of X so for example if we had a binary treatment variable and X could take values of zero or one then we would do this summation over both possible values of X so we would write this out with X Prime equals zero and then X Prime equals one and so this equation would go from two terms to four terms okay so that's pretty much it there is a Blog associated with this material so if you're interested in learning more check that out there are some more details there that I didn't include in the video and if you enjoyed this content please consider liking subscribing and sharing so if you have any comments or questions please share those in the comment section below I find those very helpful for my own personal development and Learning Journey and as always thanks for your time and thanks for watching
dejZzJIZdow,2022-10-22T13:30:09.000000,Causal Effects via the Do-operator | Overview & Example,hey folks welcome back this is the third video in the series on causal effects in the last video we learned how to estimate causal effects with observational data via something called a propensity score while these approaches help us in the face of measured confounders there's still the problem of unmeasured confounders in other words variables that bias our estimate that we do not observe in our data in this video we will see how we can resolve this problem of unmeasured confounders to do this we will need to re-evaluate how we think about causal effects so with that let's get into it everything I'm going to talk about in this video concerns connecting observational data to Interventional data in other words connecting data that we might passively observe to data that we might measure more carefully through a randomized controlled trial or something similar and so this distinction between observational studies and Interventional studies was made in the previous video but just as a quick recap of what we're talking about here say we're trying to quantify the causal effect of a pill on headache status what a observational study would look like to investigate this is we passively observe a population of people with headaches some of them take the pill some of them don't take the pill and we just observe their natural outcomes with no intervention or influence on who takes the pill and who doesn't on the other side of this we have an Interventional study which is more akin to a randomized controlled trial where we carefully pick a population of people at random and then randomly split them into two groups an experimental group and a control group would give the experimental group the pill and we don't give the pill to the control group and then we can compare their outcomes so clearly observational data take less effort to measure while Interventional data take more effort to measure but as has been said in the past there's no such thing as a free lunch so even though I observational data are easier to capture takes more effort to estimate causal effects from them and even still it may not be possible to estimate causal effects from them we saw one way we can estimate causal effects from observational data in the previous video where we talked about propensity scores and in this video we're going to talk about an alternative strategy to doing this and so first we're going to take a step back and re-evaluate our definition of the average treatment effect which is what we've been using to quantify causal effects in this series formula one here is how we Define the average treatment effect in the context of a randomized controlled trial so what we have is this expression here and just quickly running through this e is denoting the expectation value y denotes the outcome variable X denotes the binary treatment variable x equals 1 means the subject took the pill x equals zero means the subject didn't take the pill so this first term is the expected outcome of the treated group because all the these people took the pill the second term is the expected outcome of the untreated group because they didn't take the pill so this is a very simple expression and if we have a randomized control trial we can obtain the average treatment effect in a very straightforward way but there's an implicit assumption here that people may not realize if just seeing this equation on paper or on a website or something so the assumption is that the treatment status is statistically independent of all other factors so that's why this equation is true for a randomized controlled trial that's the whole point of randomization so that who is in the experimental group and who's in the control group has no statistical relationship with any other factors but now we're going to look at an alternative way to formulate the average treatment effect and to do this we're going to use the do operator which we first saw in the previous video on causal inference so here we have a very similar thing we have this expected outcome of the treated group and we have the expected outcome of the control group but the only difference here is we now now have this do operator in each expression with the do operator represents is a physical intervention as opposed to on this side in Formula One this notation does not distinguish between a intervention and just a passive observation and so what we gain by expressing the average treatment effect in this way is that we are explicitly denoting that the treatment status is statistically independent of all other factors and so in the context of a randomized controlled trial these two formulas are equivalent but this isn't true in other situations in other words in an observational study if you use this equation without being careful about statistical dependencies you're likely going to get biased estimates of causal effects because there are likely systematic differences between the people that received the treatment and people that didn't receive the treatment okay and then as a final note in the previous video we talked about these propensity score based methods and there we were using this equation and we were using observational data so there the implicit assumption is that the propensity score based methods approximately remove the statistical dependence between treatment variables and measured covariates so that's what allows the propensity score based methods to work is that we're taking care of these other statistical dependencies okay so more on this do operator so the do operator again is a way to denote interventions and so this brings up another observational Interventional Distinction on the left here we have an observational distribution so this is what we saw on the left hand side of the previous slide in other words this is what we saw in Formula One it was the probability of Y given the variable X is observed to be X naught while in Formula 2 we had an Interventional distribution so that was the probability of Y given the variable X is artificially set to X naught I talk more about the do operator in the video and causal inference so check that out also this is a very good reference this is an introduction by Judea Pearl who invented the do operator and it's a big figure in the space of causal inference so often we don't measure Interventional distributions directly so what this would look like is doing the Interventional study doing the randomized control trial in other words doing the physical intervention and capturing the statistics however Formula 2 this alternative definition of the average treatment effect relies on the Interventional distribution so if we want to use this more general formula for the average treatment Effect one that's not just true in randomized controlled trials but is true everywhere it would be good if we can translate any Interventional distribution into observational distribution so translating the thing that we want to estimate in terms of things that we actually measure and that brings up the question of identifiability and so identifiability is all about answering the following question can the Interventional distributions be obtained from the given data so in the context of a randomized controlled trial we likely already have the Interventional distribution because that's what we painstakingly went through the process of measuring but in all other cases where we just have observational data what identifiability is all about is expressing the Interventional distribution in terms of observational distributions and so Pearl and colleagues developed a systematic three-step process for answering this question of identifiability okay so the first step in this three-step process is to write down the causal model and so taking the example from the previous video where we were estimating the causal effect of going to grad school on income this was the causal model that we had assumed age causes grad school and income then grad school causes income then the second step is to evaluate something called the Markov condition and so this consists of two parts the first is asking is the graph a cyclic so basically do any Cycles exist in this graph so if we start at Z and we follow the arrowheads there's no way we can get back to Z if we start at X we follow arrowheads there's no way to get back to X and if we start at y there's no way to get back to Y so indeed this graph is a cyclic the second part is that all noise terms are independent so basically the noise terms for age would be all factors that cause age that are not captured in our causal model similarly the noise terms for grad school would be all other factors other than age that drive someone's probability of going to grad school and then the noise terms of income would be all other factors other than age and grad school that have a causal effect on someone's income and so assuming that these noise terms are independent means that there's no cross connections between these external factors is this true in this case that is questionable but for the sake of this example we'll just assume that it is okay and finally step three we express the Interventional distribution in terms of observational distributions and so we can do this using the wonderful truncated factorization formula which expresses any Interventional distribution in terms of observational distribution so notice we have a do operator on the left hand side and there's no do operator on the right hand side okay but still on the left hand side we have Z and to estimate the causal effect of X on y we just want to isolate y in this conditional probability here so the way we can get rid of Z is by summing over it so basically setting Z equal to every value in our data set so Z equals one two three four five all the way up to you know 100 whatever we might have and then just evaluating this summation okay so now that we have the Interventional distribution in terms of observational ones we can compute the average treatment effect using our formula two from before and so all we do is just plug in one here stick it over here plug in 0 here stick it over here and then just compute the average treatment effect and so the truncated factorization model I showed in the previous slide was just for that specific example but it's a much more general formula as you can see here so I'm not going to spend too much time on this but this is from the introduction by Pearl and I also talk a bit more about this in the blog associated with this video so now the whole motivation of this video from the propensity score video was that propensity score based methods don't account for unmeasured confounders they only handle confounders or covariates that are included in our data so what can we do about unmeasured confounders so using everything that we've talked about in this video we can compute the average treatment effect even when we have data missing from our data set so we're going to run through this systematic three-step process again but now for a more complicated example so this is taken from the introduction by Pearl so suppose we have this causal model we have our treatment X our outcome Y and we have three covariates okay so step two is we evaluate the Markov condition we can see that this graph is acyclic there are no cycles and we will just assume that the noise terms are independent and then step three we'll Express the Interventional distribution so again we do this using the truncated factorization formula which is a lot longer now because we have five variables instead of three and then to isolate y on the left hand side we can just sum over variants but now suppose that Z isn't measured it's missing from our data set we can run through the same process again we write down the causal model we evaluate the Markov condition we express the Interventional distribution first with the truncated factorization formula then summing over covariates but now the problem is we don't measure Z2 we don't know these probabilities so what are we supposed to do so now we can just apply a little bit of magic and out comes the Interventional distribution without Z2 on the right hand side so what just happened here so what we just did there was the result of this expression here so this is the Interventional distribution via the parents of the treatment so on the left hand side we have the Interventional distribution of Y given the intervention in X and on the right hand side we just have observational distributions in terms of x y and the parents of X so the key inside here is we only need the parents of X to estimate its causal effect and so again this is from the introduction by Pearl referenced down here so that was a lot of information so I'm just going to try to recap some key points the first is given a markovian causal model with all the variables measured we can use the truncated factorization formula to express any Interventional distribution in terms of observational ones and thus we can express any causal effect using our new formulation the average treatment effect the second key point is that if we have unmeasured confounders we can always simplify the truncated factorization formula given a markovian causal model to include only the parents of X but now what if we can't measure the parents of X and so what that brings up is this notion of alternative covariate selection that's going to be the topic of the next video in this series so there we're going to further explore this notion of simplifying the trunk the factorization formula to only include variables that we measure or in other words to exclude the variables that we don't measure and then we'll get further insight to the magic that we saw a couple slides ago so there's a Blog associated with this video there's some details in there that I may have left out in this video and I'll drop a friend Link in the description so you can access the blog even if you're not a medium member and if you enjoyed this video please consider liking subscribing and sharing this content if you have questions please share those in the comments section below a big reason why I make these videos and write these blogs is for my own learning process and development and the questions that I receive is a huge help in this whole learning process and as always thanks for your time and thanks for watching
dm-BWjyYQpw,2022-10-14T13:37:55.000000,Causal Effects via Propensity Scores | Introduction & Python Code,hey folks welcome back this is the second video in a series on causal effects in the last video we learned some theoretical Concepts that underlie causal effects however there were questions surrounding how to translate this Theory into practice in this video we will resolve these questions with a set of practical techniques for estimating causal effects these techniques are all based on something called a propensity score we will conclude the discussion with a concrete example with python code and real world data so with that let's get into the video so in the last video of this series we were talking about estimating causal effects and to estimate causal effects we need data but not all data are equal and so here I'm going to distinguish two types of ways we can obtain data so the first is data from what I'll call an observational study so an observational study insists of passively measuring data without intervention in the data generating process so as an example the causal effect of taking a pill on headache status would an observational study might look like is we passively observe a population of people with headaches some of them taking pills some of them not taking pills and just doing an analysis trying to quantify the causal effect of that pill on headache status the second type of way we can obtain data is from what I'll call an Interventional study and what this consists of is an intentional manipulation of a data generating process for a particular goal so an example of this is a randomized control trial which we talked about in the previous video so if we were interested in studying the same thing the effect of a pill on headache status what a randomized controlled trial would look like is we pick a group of people from a larger population of people with headaches we split that group into two subgroups the first of which we give all the people a pill and then the other subgroup we don't give them a pill and then we can evaluate the causal effect so like I mentioned in the previous video Interventional studies or randomized control trials or something like it is one of the most common ways to quantify causal effects but this comes at a cost it takes a lot more effort and care to collect data through a randomized control trial than it is to just passively observe natural behaviors of people so it would be very advantageous if we could compute causal effects using observational data because it is a lot easier to obtain but there's a problem here in observational studies there could be systematic differences between people that take the pill and don't take the pill that can bias your estimate so for this example where we're just passively observing people with headaches not controlling who takes the pill and who doesn't take the pill there could be a variable that we might not be measuring such as age that could be a confounder because someone's age could drive their behavior to take a pill like kids probably won't be taking pills because their parents won't let them adults might be more likely to take pills than kids because they don't need permission from anyone to take a Tylenol and then that could also affect headache status kids might be less prone to get headaches than adults and so this introduces a systematic difference that can bias our causal effect estimate so one solution to this problem is the propensity score and so a propensity score aims to solve this problem of systematic differences by estimating the probability a subject receives a treatment based on other characteristics so essentially what we do is we include additional variables to our treatment and outcomes so our treatment variable could be takes pill or not our outcome could be headache status and then we can collect other variables that might influence treatment status or headache status such as age income sex or some other variables and so a common way of computing the propensities score is a two-step process first we train a logistic regression model so what that consists of is taking your covariates basically any variable that's not your treatment or outcome and so let's say here we have age income and sex and then we set as our Target variable the treatment status so in other words we're using the covariates to predict treatment status and logistic regression is just a way to connect a set of predictors to a binary Target once we get the logistic regression model the next step is to use it to generate the propensity score so what this might look like is we take a subject with a set of covariates here we have age income and sex we pass them into the logistic regression model that we just developed and then out comes a probability of treatment or in other words a propensity score and so now we can do this for all our subjects and we have a propensity score for every single subject in our data set with a propensity score for every subject in our observational data set we can use the propensity score in different ways to help estimate unbiased causal effects and here I'll be talking about three different propensity score based methods for doing this and so these methods are matching stratification and inverse probability of treatment waiting okay so starting with matching in the simplest case what matching consists of is creating treated untreated pairs with similar propensity scores what this might look like is we have an observational data set so we've just passively observed People's Natural behaviors when they have a headache let's say five people took a pill seven people didn't take the pill and we have propensity scores for each of these subjects and notice that there are people that actually took the pill that have a relatively low probability of treatment and conversely there are people that didn't take the pill that have a relatively high probability of treatment so this is good because it helps us in doing the matching process so one way we can do this is called one to one unmatching without replacement say we pick this subject with a 93 percent probability of treatment and what we do is match this subject to a subject in the untreated population with the most similar propensity score so we can just look at all these subjects we see 73 is the closest to 93 out of all these participants and we just match these two together and then we can take this subject with the 80 propensity score and then we do the same process excluding this participant here with 73 and then we pick this participant with a 57 percent pick the closest one in the untreated population so on and so forth so now we have a so-called matched sample and this is reminiscent of what we might see in a randomized control trial we have two groups of equal size one group took a treatment the other group didn't take the treatment so with this match sample we can compute the average treatment effect just like we did in the previous video so conceive of probably there are two ways we could go about this the first way ate stands for average treatment effect defined in the previous video e is the expectation value which is essentially an average Y is denoting the outcome variable one is indicating a treatment status of one meaning they took the pill zero is indicating a treatment status of zero meaning they didn't take the pill and I is just indexing these pairs in the Matched sample so kind of walking through this I equals one let's say it corresponds to this matched pair so this person corresponds to this term and then this person corresponds to this term we take their difference then we look at this pair we take the difference in their outcomes then we look at the next pair look at the difference in their outcomes and so on and so forth and now we'll have five values corresponding to the difference in outcomes for each of these pairs and then we can just take their average and then that'll give us an average treatment effect alternatively we can use the Expression we had for a randomized control trial so that's what RCT means here so this is is the average treatment effect in a randomized controlled trial so it's just slightly different instead of taking five differences and Computing the average here we will look at all the participants in the treated population look at their outcome and compute the average then we look at the outcome for the untreated population and then we take their difference so these are two alternative ways we can compute an average treatment effect with a matched sample so here there are a lot of details that I glossed over and I don't want to spend too much time on it I'll just refer you to this nice paper by Austin where he dives into details on optimizing the Matched sample basically how we match the took pill population with the didn't take pill population and then also Alternatives and matching so here we did one-to-one matching but you can also do one-to-many matching so essentially there what you're doing is instead of pairing each individual in the treated population with one subject in the untreated population you can match many untreated subjects to a single treated subject and this is all in the paper for anyone who is interested so we'll move on to stratification so in stratification we split subjects into groups with similar propensity scores so again let's say we have our observational data set five people took the pill seven people didn't take the pill and then what we do is what is called rank ordering so basically we order the subjects from lowest to highest propensity score so this is like what you do in elementary school math take the subjects from smallest propensity score to largest propensity score and then what we can do is split them into groups so here we split the subjects into four equal sized groups and then what we can do is compute the average treatment effect in each of these groups don't worry too much about the notation here so G is just indexing each group so we can have group one group two group three group four and then p is just indexing the people that took the pill and is indexing the people that didn't take the pill and we can compute the average treatment effect for each group like this so looking at this term this is the expectation value the average outcome for the people that took the pill let's say in group one so let's say G is equal to one and then this for g equals one we look at the average outcome for the people that didn't take the pill and since it's just one person we just look at this one person's outcome and then we take the difference and then we have the average treatment effect for group one we can do the same thing for group two then for group three group four so we have four average treatment effects and then we can go a step further and compute the average of these four averages and get an overall average treatment effect all right the last method I'll talk about is inverse probability of treatment waiting or iptw for short and the first two methods were kind of similar where we were clumping together people with similar propensity scores and comparing average outcomes between people that took the pill and didn't but in iptw we do things a little differently so here instead we use the propensity score to derive five weights from which the average treatment effect can be computed directly so what this might look like is we have our same observational data set five people take the pill seven people didn't take the pill we have propensity scores for all 12 of these individuals then we can convert these propensity scores into weights and so how we Define the weight will depend on whether the subject hooked the pill or didn't take the pill if they took the pill we just do one divided by the propensity score so 1 divided by 0.23 is about 4.35 1 over 0.93 is about 1.08 so on and so forth but if they didn't take the pill we do one divided by 1 minus their propensity score and then we can get weights for each of the subjects okay and then the next step is we use these weights to aggregate the outcomes for the treated and untreated populations so we weight each outcome so p is indexing again the people that took the pill we do this for every subject in the treated population we multi put these together we add them all up and then we divide by n which is the total number of subjects so in this case it would be 12 and then that gives us the aggregated outcome for the treated population then we do the same exact thing for the untreated population which will give us the aggregated outcome for the untreated and then we can use these values to estimate our average treatment effect so here we can just simply take the difference between the aggregated outcome of the treatment with the aggregated outcome of the untreated now we're going to do a concrete example in Python so here we'll compute the average treatment effect of going to grad school on income and for those of you keeping up with the causality videos I've been putting up this example will be very similar to what we saw in the causal inference video this code and much more is available at the GitHub which I will link in the description below so first we import our libraries so we have pickled just to import our data do y is going to help us do the causal effect estimation and then we have numpy to just do some math then we load in the data we have this pickle file which is a pandas data frame also available at the GitHub repository and then this data comes from the UCI machine learning data repository which I will link in the description as well okay next we Define the causal model so this isn't really necessary for the propensity score based techniques but this is a standard procedure in the do y Library we'll get a better sense of why the library is set up in this way in future videos of this series but for now we can just view this as picking out or labeling which variables in our data set are the treatments the outcomes and the covariance so our treatment is the variable has graduate degree which is a Boolean variable indicating whether the subject went to grad school our outcome variable is greater than 50k indicating whether the participant makes more than fifty thousand dollars or not and then we have the common causes which here we can just view as the covariates and we just have H as the single covariant and so what this causal model looks like is this this is identical to what we saw in the causal inference video and then we can compute the causal effects using these three different propensity score based methods so here we're just creating the S demand which isn't so important here but we will see why this is very important in future videos of this series and then I create a list of the names of each of the propensity score based methods so we have matching stratification and waiting then I initialize two data structures a dictionary and a list to store the causal estimates for each of these approaches and then we just compute each causal effect in a for Loop so just one by one for each of these elements in this list we're just going through and Computing the causal estimate and then storing those in both the dictionary and the list that I created earlier so the result of all this is for matching we had an average treatment effect of 0.136 for stratification we had 0.25 and for inverse probability of treatment weighting we had an average treatment effect of 0.331 so it's interesting to see that all three methods even though they're all using the same data give different causal estimates so one thing we can do is just aggregate these and take their average and so the average of all three methods is 0.24 for those of you who recalled the causal inference video where we did an identical analysis but there instead of using propensity score based methods we use something called a meta learner and we got a similar result so they're the average treatment effect was 0.2 and here we have 0.24 so very similar so how we can interpret this result is going to grad school will increase the probability someone makes more than fifty thousand dollars a year by 24 okay so before we jump with joy and say that we can compute causal effects from any observational data set I would share a word of caution so the whole point that people go through the trouble of randomized control trial is that they can handle both measured and unmeasured confounders through the randomization process you can mitigate systematic differences between your treated and untreated populations but with these propensity score based methods we can only hope to handle measured confounders in the example that we just ran through we had only a single measured confounder which was age but there are conceivably other variables that could impact both someone's probability of going to grad school and someone's probability of making more than fifty thousand dollars a year things like parental income or field of study or work ethic and so on and so forth so these propensity score based methods won't be able to account for other confounders that are not included in the propensity score model and so this may not be such a big deal when you know what you need to measure and you have the ability to measure it but the situations where you know what you need to measure but it's very tricky key to measure so for example let's say we're considering work ethic as a confounder to both someone's probability of going to grad school and someone's probability of making more than fifty thousand dollars a year it could be challenging to quantify work ethic and measuring that for each of your subjects so that brings up the problem of unmeasured confounders and so in the next video of this series we will see what we can do about unmeasured confounders okay and lastly there is a Blog associated with this video linked here and I'll also link it in the description there are some details in the blog that I didn't discuss here so for those interested feel free to check that out and if you enjoyed this video please consider liking subscribing or sharing the content if you have any questions please share those in the comments section below I definitely learned a lot from the feedback I get in the comments section and as always thanks for your time and thanks for watching
BOPOX_mTS0g,2022-10-07T12:23:26.000000,Causal Effects | An introduction,hey folks welcome back this is the first video in a new series on causal effects causal effects go beyond statements like a causes B and quantifies how much a change in a will change B in this video we will explore a few Core Concepts that underlie causal effects which will serve as a solid foundation for future videos and with that let's get into it one way we can think about causal effects is going Beyond answering questions of why to answering questions of how much so for example we might ask why did my headache go away put another way what is the reason my headache went away what caused that and maybe what happened was I took a pill so the pill caused my headache to go away but asking how much would be what if I took two pills how much does a unit of treatment so how does an additional pill affect my headache and so that's what causal effects are all about answering this question of how much and so before diving into the discussion on causal effects I'm going to touch on two pieces of background information the first being the types of variables that we see in estimating causal effects and so the first type of variable we have is the outcome which is the variable we're ultimately interested in so using the same example from the previous slide headache status another type of variable we have are treatment variables and so these are the variables that we change in order to influence the outcome so from the same example the pill was the treatment and so we took a pill to try to influence our outcome variable of headache status and then we basically have everything else which we call the covariates and then this could be age weight exercise level how often someone takes headache pills or how adapted they might be to them so on and so forth and so the other piece of background information that we're going to talk about is this potential outcomes framework this is a big idea in the space of causality and it's an approach to estimating causal effects through what if questions so what this might look like is we have scenario one which is what actually happens so let's say I have a headache and I take a pill and then my headache goes away that's scenario one and then we can ask what if I didn't take the pill would I still have a headache what situation would I be in so that's scenario two so we can use this way of thinking this scenario one scenario two way of thinking to formulate causal effects and so in this video I'll talk about three different types of Castle effects listed here and kind of embedded in all these different types of causal effects is the potential outcomes framework okay so first we have the individual treatment effect and so this basically quantifies the impact of treatment for a particular individual so just like we were talking about before scenario one let's say me personally specifically I have a headache and I take a pill and then my headache goes away but then we can imagine this what if scenario scenario two where I have headache but I didn't take the pill I kind of turned back time to the moment just before I took the pill and just didn't and then let's say in that scenario my headache got a little better and so then we can formulate this individual treatment effect or ite for short as the difference between these two outcomes putting this into an equation Y is representing our outcome variable headache status X is representing our treatment status so x equals one represents taking a pill scenario one and then x equals zero represents not taking the pill so scenario two and then what this is representing is the outcome variable when I took the pill versus the outcome variable when I didn't take the pill so we just take the difference and that's the individual treatment effect but if we're trying to investigate the efficacy of this pill we're probably not just interested in how effective it is on me but how effective it is on the larger popular population and so that's where we get into the average treatment effect which is the expected impact of treatment for a population so here we're going to do the same exact thing as before but instead of just for one person we're going to look at a bunch of people so scenario one we have a bunch of people with headaches we give them pills and we measure their outcomes scenario two take those same people and we do the what if scenario and then we measure their outcomes and then just like we did for the individual treatment effects we take a specific person's scenario one look at the outcome subtracted by their outcome in scenario two and then we just take the average for this Rod population we can represent this in an equation form where e is representing the expectation value and then I is indexing the subjects in this study so what this means is we look at the I participant we see what their outcome was in the scenario one and we subtracted by their outcome in scenario two and then we compute the expectation value which is just a weighted average is what we're seeing here okay so we have this way of formulating causal effects through the individual treatment effect and average treatment effect but you know a big question here is how are we supposed to compute this when we can only really observe one of these two scenarios in practice we don't have all the information that we need to compute the individual treatment effect or the average treatment effect and so that brings us to the individual treatment effect in rcts or randomized controlled trials and so this is how we can compute an average treatment effect experimentally so imagine that we pick out a population of people at random and then we randomly split them into two groups so we'll call one the experimental group and we'll call the other the control group and then we can do a similar kind of thing as we did before we give half the people the experimental group a pill when they have a headache and we measure their outcomes then we have the other half of people in the control group and we don't give them a pill and we measure their outcomes but now what we do slightly differently is we compute the average outcome for each of these two groups so with the average outcome for the experimental group and then we have the average outcome for the control group and then we can compute the average treatment effect in the context of this randomized control trial as the difference between these two average outcomes so this is probably the most common way that the average treatment effect is computed in practice through randomized controlled trials or something similar and so how we can represent this in an equation is instead of having just a single expectation value now we have two expectation values and then J is indexing the experimental group and K is indexing the control group so this is much easier to measure in practice than the previous equation because here we're just comparing two numbers as opposed to doing a bunch of subtractions and then Computing the average and then finally we have the average treatment effect for the treated or similarly for the controls so what this is is the expected impact of treatment for the treated population or the control population and so how we can express this mathematically the average treatment effect for the trade-in also called ATT we can write it down like this so here we have again the expectation value which is just a weighted average we have the outcome variable y i is indexing our participants so this is the outcome variable for the ith participant when they take the treatment minus the outcome variable for the ith participant when they don't take the treatment so we can take that difference but now we have this conditional statement so basically what this is saying is what is the average treatment effect but conditional that the participant actually received the treatment and so in the textbook by Morgan Winship they have a nice discussion about this and an example where this might come up the ATT the average treatment effect for the treated is let's say you're trying to measure the causal effect of Private School versus public school on SAT scores or something so what average treatment effect for the treated and what that study might look like is you take students that are already in private school so the treatment variable is in private school you split them into two groups where one of them you keep them in private school and the other group you send them to public school and then you can measure their SAT scores compute the average and then compute the difference conversely for the same example you can do the average treatment effect for the controls you do a very similar study where you split your population into two groups the treatment group and the control group but the only difference here is instead of taking a population of private school students for your study you take a population of public school students for your study and so generally these values these average treatment effects will be different for the two different populations so if your kid already goes to private school school you're probably more interested in the average treatment effect for the treated as opposed to the overall average treatment effect so in this video we talked about three different types of causal effects which are all based on this potential outcomes framework which essentially works by comparing the outcomes of what you actually measure to a what if scenario that you don't measure but there are some practical concerns with this way of doing things so how are we supposed to deal with all these counter factual terms how are we supposed to deal with all these what-if scenarios in practice so we saw that we could do a randomized control trial but are we limited to just doing those kinds of studies what about observational data randomized controlled trials and things like it are expensive not just in cost but in effort you need researchers you need protocols you need irbs it takes a lot of effort to manage these kinds of studies however observational data this is just data that we can passively observe this kind of data is much more prevalent than data from randomized controlled trials it would be very advantageous if we can use this data to compute causal effects and then finally what kind of software tools are available for this stuff so in the next video this series I'm going to touch on all these different questions and discuss three different kinds of practical techniques for computing causal effects from observational data and so if you enjoyed this video please consider liking subscribing or sharing this content if you have questions please share those in the comments section below I learn a lot from the questions I've received in the comments section and as always thanks for your time and thanks for watching foreign
5zeqo-R12vk,2022-10-01T02:01:57.000000,How to STAY dumb,a lot of situations we might feel the need to look smart while this may work out in the short run there's a big problem if you're so busy trying to look smart you kill your chances of actually getting smarter it all comes down to this when someone trying to look smart comes across something that doesn't make sense they act like it does on the other hand someone trying to get smart will ask a question so you have to decide do you want to look smart or Get Smart
WgmMK5fS0X0,2022-09-27T19:09:20.000000,How to do MORE with LESS - multikills,multitasking isn't a good strategy how do we solve this problem trying to get more done in less time one idea is we can seek guidance from our bird hating ancestors and try to kill two birds with one stone at first glance this might sound like multitasking but it's not multitasking is like when you do two things around the same time while killing two birds with one stone is getting two things done with a single action
gazeatME3dI,2022-09-26T22:07:08.000000,How to be more ANTI-FRAGILE,only certainty in life is uncertainty and unfortunately for us uncertainty doesn't seem like something that's beneficial but what if there was a way we could gain from uncertainty in his book anti-fragile author Nasim Nicholas taleb defines something anti-fragile as something that benefits from things like uncertainty harm and disorder
aD9ereUJBII,2022-09-24T20:12:19.000000,The Achievement TRAP,be careful with achievements it probably sounds silly since achievements are usually thought of as good things but they can be dangerous the more we focus on what we've achieved the less we focus on what we need to do now when we think we're so cool for hitting a goal or winning an award or being the best at something it's easy to get lazy about the habits that helped us achieve that thing in the first place don't fall into the achievement trap no matter what you've achieved there's always room for improvement
6A4qwVOkPF0,2022-09-23T19:42:12.000000,"It’s not DANGER behind the fear of conflict, but uncertainty",so now let's talk about fear the fear of conflict like many other fears doesn't usually stem from any kind of actual danger what it really comes from is the unknown consider the example of driving to work driving is probably one of the riskiest and most dangerous things we do on a daily basis but no one's afraid to commute to work that's because we do it every day it's familiar and things that are familiar aren't scary so in a similar way just like how every time you drive and you survive it becomes less scary every time a team goes through a conflict and survives conflict becomes less scary
W6TkOTsI7vM,2022-08-26T13:30:31.000000,What Is Data Science & How To Start? | A Beginner's Guide,i get a lot of data science questions naturally through my blogs and youtube videos but i've also been getting questions that are a bit more meta the questions are more career oriented people ask me how can i get into data science what do i need to become a good data scientist so on and so forth so the fact that people are reaching out to me with these kinds of questions is probably a good indication that there are a lot of other people out there with similar kinds of questions so that's the inspiration for this video here i'll give my take on what is data science and what is the best way to get into it so i prefaced this whole discussion with this is just my impression as someone that's worked in data science for the past few years in both an academic and now in a business setting and data science being the huge field that it is there are certainly a wide range of different manifestations of what it might look like across different organizations businesses etc however this is the kind of mental model that i've arrived at about data science and the different associated roles through my experience so we'll start with this first question of what is data science and to me data science fits into a bigger picture which includes a variety of different roles and responsibilities that all center around this core of data well again there might be different labels and details for these types of roles the way i see it there are generally five distinct roles in this data space to list them the first is data engineering second is data analytics third is data science which is what i do and the main topic of this video the fourth is machine learning operations and engineering and then the fifth is data management all these archetypal roles and responsibilities in the data space all serve this bigger data pipeline so data engineering is the starting point and then that goes all the way to ml operations engineering and then data management kind of sits on top of this whole thing okay so the first role data engineering is mainly focused on taking data information from the real world and bringing it to a place where data scientists data analysts can do something with that information and as a data scientist i love data engineers because they make my life a lot easier so every data analyst and every data scientist's dream is to just have a curated data set where everything is good to go the data is clean the data is pre-processed and all they have to do is make a visualization or develop a model so on and so forth so good data engineers are very valuable asset probably a big reason why it's such a big job these days okay so the second role i mentioned was data analyst and typically what a data analyst does is take data that is made available to them through whatever means and typically will generate interactive dashboards or visualizations graphs plots they will typically tell stories with data they'll make slides they'll give presentations to decision makers and business partners and stakeholders and a data analyst will typically use tools like microsoft excel tableau power bi and some kind of interactive sql application so a lot of these tools don't require much programming experience so in that regard a data analyst role is typically less technical for lack of a better word than a data scientist rule and probably the biggest difference between a data analyst and a data scientist is that a data scientist will do a lot more programming than a data analyst and so that brings us to the third role which is the data scientist what i do a data scientist will do a lot of the same things as a data analyst we will create visualizations we'll tell stories we'll communicate our findings but like i mentioned what i spend most of my time on is programming so i use python and that seems the most popular these days in the data science space but there's also r which is pretty big in the statistics community there's matlab which is big in the academic or scientific community and then there's also julia which is a kind of new up-and-coming programming language so it's not just a matter of data scientists use different tools than data analysts to do the same thing data scientists also will typically focus on developing models so what's a model a model is basically something that lets you do predictions you give it some information that you can measure and it will give you a prediction about something you care about that you can't necessarily measure right now so this could be something like given someone's past credit history what's the probability that they won't make a credit card payment in the future models are definitely a powerful tool in science in business in finance and sales and basically any industry that has data and has things that they would like to predict models are going to be helpful to that industry which brings us to the fourth role in this data space which is machine learning operations and what this role typically does is takes the model that the data scientist develops and deploys it into production so it doesn't matter how amazing and insightful your model is if it can't be used and deployed in the real world it's not going to have any value what this might look like using the same example from before what's the probability that someone will make their credit card payment based on past credit history so it's great if i develop a model and it's sitting on some server somewhere or sitting on my local machine or whatever it might be but how does that help the people making the decisions of whether to increase someone's credit card limit or to approve a new credit application so that's where the machine learning operations will be valuable the fifth and final rule sits on top of this whole pipeline so we start with the data engineers who take the data and information from the real world and bring it to a place where analysts and data scientists can use it the data analyst will typically take the data that is there and visualize it look at it in different ways and create stories and slides from it the data scientists will do a lot of the same things but we'll take it a step further in taking that data and developing models with it then finally the machine learning engineer will take that model and deploy it into production so this fifth role which i labeled as data management kind of sits on top and what this role focuses on is handling the metadata the meta information so not so much what the values in a specific data table might be but where's the data table located what variables are in this data table what do these variable names mean what is the description of each variable what data types are they using where is it coming from where is this data being pushed to eventually kind of keeping track of all the data about the data which is very important so this ecosystem like all these different types of roles are critical to make sure the whole process is working effectively like you can't just have data engineers you can't just have data scientists you can't just have machine learning engineers you really need every kind of piece of the puzzle to have data make an impact in your business or your research or whatever it might be as a final caveat even though i've presented these five different roles as beautifully distinct and specialized responsibilities in reality it's not like that at all there's a lot of bleed over there's a lot of overlap so just speaking from my experience as a data scientist i do the dating engineering work i do the data analytics work i'll do the machine learning development i'll do the data management stuff and that's just the reality of things so while it may be the case at some organizations that these roles are very specialized so a data scientist will only focus on doing data science type things or a data engineer will only focus on data engineering type things for the most part there is bleed over so if you want to get into any one of these roles it's good to have some knowledge and experience doing the other roles in this space okay so now we'll talk about the second question which is how can i get into data science so first preface this again that this advice is definitely biased by my experience and my data science journey so there are certainly other paths to get into a data science role but these are the tips that i would give to anyone that comes up to me and says that they want to get into data science i have three tips for anyone trying to get into data science so the first thing to do is get a technical graduate degree so probably 80 to 90 of the people that i work with in data science they either have a master's degree or a phd in some technical field so what do i mean by technical there's a lot of flexibility here so i have a phd in physics i've seen phds in math phds in statistics masters in statistics master's in data science and data analytics any training that has a focus on math science programming will be helpful to getting a job and while you don't necessarily learn all that you need to know to do data science in these different programs there's typically a good amount of overlap so in physics you learn a lot of math and science and you do a lot of modeling and so that's a pretty natural transition into data science but second and probably the most practical is that it's just something to have on your resume so the reality is when people are looking at resumes for data scientists they're getting flooded with applications and there just has to be a way to just narrow down the applications to help make a decision and typically the first filter that people use in picking data science applications is do they have a graduate degree and in what and so again i'll highlight even though this is the background of the bulk of people i see in data science they have a technical graduate degree there's certainly other people working in data science without them maybe they'll just have an undergraduate degree or maybe they got an undergraduate degree and did like a data science boot camp so it's certainly not the only way into data science but it is the bulk of what i've seen okay so the second tip i would give to anyone trying to get into data science is to learn the basics and what are the basics so i would say there are three kind of core things that you need as a data scientist one you absolutely need to know how to program i would definitely recommend to learn python the second basic basic thing you need as a data scientist is the solid mathematical foundation and so specifically what this includes is linear algebra statistics and calculus those are the three core mathematical topics that you need to be comfortable with and then the third basic thing you need as a data scientist which i personally think is the most important but is often the one that's focused on the least is communication if you want to be a good data scientist you need to be able to present technical information to a non-technical audience you need to be able to translate your insights into something practical into something that has implications for the real world and the reason i think it's the most important is it doesn't matter how profound or impressive your findings are if it doesn't make sense to people if no one understands what you're talking about it's going to have absolutely zero impact it's going to provide absolutely no value to anyone so again second tip is learn the basics learn how to program probably python's your best bet get the solid mathematical foundation and work on your technical communication okay so the third and final tip that i would give to any aspiring data scientist is to do it i think the best way to learn is by doing so the best way to learn data science is by doing data science and so what this might look like in the early days is just finding some data and doing a project ideally using real world data so not like curated and ready to go data sets that you might find on kaggle for instance but go online try to find data sets try to find data that is not clean use a web scraper to pull information off a website reach out to your local network do you know anyone doing research that is working with real world do you know anyone that owns or works in a business that is working with real world data try to get access to that data and come up with some projects that might be helpful to that person so if you want to get good at data science you got to do data science so try to come up with at least one or two data science projects and do it from a to z do the data engineering bit do the data analytics do the data science bit and do the machine learning engineering in other words take the data from the raw source and bring it into a place where you can analyze it create visualizations look at the basic statistics try to tell a story with the data identify a problem identify a project then develop a model do something that has real world impact that provides value and then try to deploy it maybe make a website maybe make an api so people can actually use your project and then i'll also say a lot of times it's not just about what you know but about what you show like let's say you do the whole shebang you do it all perfectly you get real world data you look at the data you get the basic statistics you make visualizations you develop a model and then you deploy that model and it has tremendous real world impact and it provides value but if you don't present that information no one's going to know about it so if you're trying to work in data science you want to get hired by someone to do data science you need to not just know your stuff but show your stuff show people what you've done what you can do and it's going to increase your chances of success so don't just do a data science project and keep it a secret make a portfolio put it out there and show people what you can do okay so that's basically it so if you watch this and you still have questions please feel free to drop additional questions in the comments section below the questions are really helpful for me i feel like i learn a lot about the people that watch these videos through the questions that i see in the comments section so i greatly appreciate those i hope you found some value in this video and after watching it you know what to do next to start your data science journey or you realize you don't want to be a data scientist at all and you're going to open a coffee shop or something either way i hope it was helpful and as always thanks for watching you
5ezFcy9CIWE,2022-06-16T13:50:15.000000,Persistent Homology | Introduction & Python Example Code,hey folks welcome back this is the final video in a three-part series on topological data analysis or tda for short in this video i'll be talking about another specific technique under the umbrella of tda called persistent homology the big idea behind persistent homology is finding the core topological features of your data that are hopefully robust to noise i'll start with a brief discussion of key points surrounding persistent homology and then dive into a concrete example with code of how to use it and with that let's get into the video there are many layers to persistent homology so i'll try to start super simple and build things up in a way that hopefully makes some sense like i mentioned throughout this series tda is all about looking at the shape of data so let's go back to preschool and talk about shapes or more precisely polygons like the ones shown here but not all polygons are equal there's one that is special and the reason it is special is because it is the simplest polygon we can construct the triangle and one neat thing about triangles is that we can use them to make any other polygon for example a square is really just two triangles stuck together a pentagon can be made from four triangles like this and the star is just the same pentagon but with five triangles coming out of it so one thought is if we want to analyze the shape of our data maybe we can break it down into a bunch of triangles well as it turns out this is essentially what we do in persistent homology but with one technical detail since most data sets live in more than just two dimensions that's to say we have more than just two variables flat two-dimensional triangles may not capture the full richness of our data's shape don't worry like most things mathematicians have generalized the notion of a triangle to any number of dimensions and they call these generalized triangles simplexes so the triangle that we know and love is called a too simplex since it lives in two dimensions a line segment is the simplest shape we can construct in one dimension and it's called a one simplex similarly a tetrahedron is called a three simplex and a point is a zero simplex and so on for all the other dimensions so just like a collection of triangles can make any two-dimensional polygon a collection of simplexes can approximate just about any complicated high-dimensional shape that may underlie our data and so since you'll probably see it elsewhere the technical name for a collection of simplexes is called a simplicial complex and this is a key concept in persistent homology okay so this gives us a clue as to how we can take unstructured point clouds in other words data sets and translate them into shapes so now let's talk about how we might compare shapes together no matter how different or complicated they may seem so one way to do this is by looking at poles for example these three objects shown here we have a taurus a loop and a coffee mug so while these may appear to be very different shapes they have something fundamental in common they all have a hole and this is like the joke that a topologist looks at a coffee mug and a donut and sees the same thing the reason being that one can continuously transform one into the other for the aficionados out there this is called a homeomorphism but the fundamental thing here is the number of holes so one way we can characterize and group shapes together is by counting holes and just like before when we generalize triangles into simplexes we can generalize holes as well we can think of cavities as holes in 3d and we can think of singly connected components as holes in 1d and so these generalized holes form the basis of what are called homology groups and these give us a formal way to characterize different shapes so when we talk about homology we are essentially just talking about holes okay so now that we've talked about constructing shapes with the generalized triangles and characterizing those shapes via generalized holes we can finally talk about persistent homology and the first step in persistent homology is to convert data into a simplicial complex to see this consider a data set i.e a point cloud like this and one way we can construct a simplicial complex out of this is by drawing n-dimensional balls around each point and since our data here is two-dimensional we just draw circles around each point which might look something like this so at the center of each of these gray circles we have a point we can form one simplexes ie line segments by connecting the data points whose corresponding circles overlap which might look something like this and so now we have two shapes we have our original point cloud which is indeed a simplicial complex where each point is a zero simplex and the shape we just constructed made up of both zero and one simplexes and then we can compare these two shapes by looking at their homology more specifically by counting the number of connected components which corresponds to the h0 homology group that we talked about in the previous slide and there we go so we can see that in our first shape on the left we have 20 separate connected components while on the right here we have 13 singly connected components but there's nothing special about this radius epsilon sub 1. so let's do this again but with bigger circles now we can start to see two simplexes appear in other words triangles and the number of connected components decreases but still there's nothing special about epsilon2 so let's go even bigger and now we see three simplexes appear i.e tetrahedrons and so on and so forth however there is a special radius value here which is when every circle overlaps with every other circle and we are just left with one big connected component and this is a natural limit to this process as we can see with each of these simplicial complexes the shape of our data is evolving and its evolution is captured and quantified by the number of connected components in other words by the change in its homology so although only four different choices of radii are shown here corresponding to the four different shapes on screen we can do this for every choice of radius between zero and the limit i mentioned earlier so this gives us a way to suss out which topological features of our data are significant based on how long they persist during this circle growing process in other words the holes that persist over a large increase in radii are more significant than the ones that persist over just a short period okay so how can we track the persistence of these holes so one good way to do this is by using a persistence diagram these look something like the plot on the left here which is showing the persistence diagram of a hollow sphere and looking at the plot each of these blue orange and green points corresponds to a topological feature or in other words a hole in blue we have the h sub zero homology group which are the singly connected components in orange we have the h1 homology group which are closed loops and in green we have the h2 homology group in other words cavities the x-axis of this plot indicates the radius at which a hole appeared in the evolution of the datus shape in other words in this circle growing process that we showed in this previous slide and on the y-axis we have the radius at which that hole disappeared so therefore a point that sits near this black dashed line this y equals x line corresponds to a hole that disappeared soon after it appeared conversely points that sit far away from this line represent holes that disappeared long after they appeared therefore two key points of a persistence diagram are the points close to this y equals x line are noise while the points relatively far from this line are significant so in this example we have two points that are far from this line the blue one in the top left here and the green one right here so we can ignore this blue one here because this corresponds to when every n-dimensional ball overlaps with every other ball so the significant topological feature of this data is captured by this green point here which is telling us that the data is characterized by one cavity and this makes sense since the data for this example are organized on the surface of a sphere okay so up until this point i've discussed only toy examples and meant to give you an idea of what's going on with persistent homology so now we'll switch gears to an example with real world data so in this example we'll walk through how one could use persistent homology to analyze market data and i suppose it's worth mentioning that this example is not meant as financial advice i'm a physicist not a trader never taken a finance class in my life however i hope this example gives you an idea of what an analysis using persistent homology might look like and inspire ideas for analyses using data that you might be working with okay so similar to the last video we start by importing python libraries the notable libraries here ry finance which gives us an api to grab market data and the ripster and persim modules which are part of the same scikit tda ecosystem from the last video next we load in market data over a four year period using why finance here we are grabbing four major market indexes namely the s p 500 dow jones nasdaq and russell 2000 we have daily prices for these indexes organized in a pandas data frame so you can imagine four columns for each market index and many rows corresponding to each day that the markets were open over this four year period then we convert this pandas data frame into a numpy array and compute the long daily returns of each index and this choice of data prep follows the procedure used in the paper by gideon cats which was the inspiration for this example and you can find it at the archive reference here okay so now we get into the tda stuff so in this analysis we want to track changes in the shape of the markets by looking at how the homology of the market changes over time so to do this we start by initializing this object that constructs simplicial complexes from data next we define a time window size which will allow us to grab a chunk of data to analyze the homology of so here we're sending this window size to 20 days next we define the total number of these chunks we will have and finally we create a numpy array to keep track of a number that quantifies changes in homology okay next we go down to this for loop and we do some persistent homology so first we take the first 20 rows of data to do persistent homology and create a persistence diagram that is we grow four-dimensional balls around each point where each choice of radius creates a simplest complex and we track the holes that appear and disappear using a persistence diagram so we do all that with just one line of code and we do the same thing but now for another set of 20 rows specifically the second row all the way down to the 21st row so now we have two persistence diagrams corresponding to two overlapping 20-day windows in which the market was open so next we can quantify the change in the overall homology between these two persistence diagrams using something called the washerstein distance which is essentially a distance measure between two persistence diagrams so at the end of this whole process we get a single number and store it in the numpy array we created earlier then we repeat this whole process for all the rows in our data set okay so after this whole process we have a set of values which quantify the changes in homology between consecutive days that the market was open and so we can just plot this as a time series which is what's happening in this block of code here and the plot will look like this blue line here which we can see there's this clear peak near the middle of the time series and then for some context we also have scaled s p 500 close prices plotted in orange just above and this vertical red line here is indicating when the crash of 2020 occurred and then as it turns out the peak in this washer sign distance time series seems to correspond very closely with when this crash occurred so did homology changes predict the crash of 2020 well i wouldn't go that far but this is indeed interesting one idea to investigate this further is one could try to use these washer sign distances to predict future market index prices so if past distance values predict future index prices then maybe there's something here so as you may be able to see from this example there is a lot of room for creativity when using persistent homology in practice and in some sense this is more art than science so that brings us to the end of our three-part series on topological data analysis a tda is a young field with a lot of untapped potential so i hope this series was helpful in getting a better idea of what it's all about if you'd like to learn more check out the other videos in this series linked in the description below there's also a corresponding medium article to this video and the others in this series which you can find in the description if you enjoyed this content please consider liking subscribing or sharing this video like many of you i am indeed still learning so if you have thoughts questions or concerns please feel free to share those in the comments section below and as always thanks for watching
NlMrvCYlOOQ,2022-06-03T10:02:25.000000,The Mapper Algorithm | Overview & Python Example Code,hey folks welcome back this is the second video in a three-part series on topological data analysis or tda for short in this video i'll be talking about a specific technique under the umbrella of tda called the mapper algorithm what this approach allows you to do is translate your data into an interactive graphical representation which enables things like exploratory data analysis and finding new patterns in your data i'll start with a discussion of how the algorithm works before diving into a concrete example with code and with that let's get into the video so in the previous video i discussed a famous problem in math called the seven bridges of koenigsberg so i won't go into all the details of the problem but it was eventually solved by famous mathematician leonard euler and the way he solved it was by drawing a picture and this picture is what we now call a graph and so a graph consists of dots connected by lines more technical term for these things the dots are called vertices and the lines are called edges another equivalent terminology is instead of calling this thing a graph we can call it a network and we can call the dots nodes and we can call the lines links so these are all equivalent terminology that i'll probably use interchangeably for this video and so graphs or networks they typically represent something from the real world so in this case euler drew a graph representing konigsberg where each of the nodes represented a land mass and each of the lines connecting two nodes represented a bridge and so what this does is it boils down the problem to its essential elements and this is what allowed euler to famously solve this problem and as i mentioned in the previous video this is essentially what we're doing when we do topological data analysis we are translating data from the real world into its essential elements or in other words into its underlying shape and so one way of doing this is via the mapper algorithm and the main topic of this video so the mapper algorithm allows us to translate data into a graph so key applications of the map or algorithm one is exploratory data analysis it allows us to take a data set and generate a visually engaging and interactive visualization another application is that it allows you to compress and visualize very high dimensional data so imagine trying to visualize a 500 dimensional data set with mapper algorithm we can take our data set compress it into a two-dimensional graph and then visualize it and try to highlight some key insights and we saw some examples of this in the previous video with discovery of cancer subtypes defining new roles in basketball and characterizing the evolution of the two political parties in the u.s so at a super high level the mapper algorithm takes data and translates it into a graph but how exactly does it work i've broken down the algorithm into five steps and i apologize in advance because it's a bit sophisticated but i will do my best to explain it in plain english so the first step is we start with our data set so here we have a two-dimensional data set because we have two variables x1 and x2 then the second step is we project our data into a lower dimensional space so here we're going from two dimensions and we're projecting down to one dimension and we can do this with any dimensionality reduction strategy like we do something standard like pca we can do something more sophisticated as we will see in the example later another popular strategy is to take just basic statistics to project out into one dimension in other words you could consider two variables x1 and x2 so each point will have a corresponding x1 and x2 value you could take the average of those two and organize them onto a one-dimensional axis you could take the max you could take them in so they're all these different strategies so we've gone from two dimensions down to one dimension so nothing too fancy yet so the next step is we define something called a cover so basically what this means is we're going to define two subsets indicated by this red circle and this green circle and we will have these two subsets have some overlap so we can see here that the red subset and the green subset indeed have some overlap and these are indicated by the yellow points in the center of this picture and so that's what we mean by cover we just define a collection of subsets that have some overlap which include the entirety of the data set another thing is we could do more than just two subsets we could have three subsets four subsets so on but just for this toy example i chose two because it's easy to see what's going on here okay and then the fourth step is we cluster the pre-image there's a lot of jargon here so i'm just gonna break it down so if we look at step three we have these red points green points and yellow points but if we remember that each of these points has a corresponding point in our original data set so what's being shown in this picture in step four is our original data set but the points are colored based on which subset they appear in from step three and so the next step is we're going to iteratively go through all our subsets so we only have two subsets we have a red subset at green subset and we're going to apply our favorite clustering algorithm we'll start with this red subset so in other words we're going to look at the red and yellow points only and we're going to do a clustering algorithm and let's say it looks something like this then we will go to our next subset which is the green and yellow points here and we will cluster those and let's say we get something like this so now we have these four clusters defined with some overlap between them now we're set up to create a graph so we can create a graph where the nodes are these clusters so four nodes corresponding to four clusters and then two nodes are connected by an edge if the clusters have shared members so this middle cluster shares members with the other three and so that's what's being shown here okay so this is just a toy example i hope that was somewhat clear of what's going on here but i'm going to try to make things more concrete with an example with code so in this example we're going to do exploratory data analysis of s p 500 data so our first step is to import some modules so we have the yahoo finance module which allows us to get the stock data we have this k-mapper module which allows us to do the mapper algorithm stuff so we're importing this umap module sklearn and then something from sklearn and we're using these for our dimensionality reduction and then we have numpy and matplotlib to do some standard math and visualization stuff okay so the first step as with any data science project is you're gonna get your data so this is pretty straightforward you just define your ticker names and you define the date range for which you want your data and then with one line of code you can pull all that data and so this code is available on the github everything should just work out of the box and once we have our data we can do some more preparation to make it ready to go to do our analysis so the first step is we're just going to look at adjusted close prices and so now what you can imagine is we have columns corresponding to ticker names and then we have rows which are corresponding to days that the markets open and then what we're going to do is convert this pandas data frame into a numpy array and then we're going to standardize each of the columns so basically what that means is we're going to consider a column compute its mean and standard deviation and then we're going to subtract the mean from each value in this column and we're going to divide it by the standard deviation and then the last step here is we do a transpose just because later this will allow us to compare takers together as opposed to days so we could also not do a transpose and then the analysis wouldn't so much be comparing different tickers together but comparing days that the market was open and then the last step here is we're going to compute the percent return of each of the tickers because later when we generate this interactive network we can color the nodes in the network based on the percent return value of each of the tickers okay so all this talking and explaining and we still haven't really gotten into any topological data analysis so if we think back to that visual overview from earlier this is all still step one we're still getting our data so now we can finally get into the mapper algorithm stuff first we will initialize this object next we're gonna do step two in the process which is project our data into a lower dimensional space so we actually have 495 tickers here and what we're going to do is project that down into two dimensions and the way we do this is a two-step process first we use iso map from this manifold library in sklearn so that'll take us from 495 dimensions down to 100 then we'll use umap which will take us further from 100 down to two dimensions so the nice thing about this syntax is we can define a custom data pipeline to do our dimensionality reduction so we can see this projection keyword is being set to a list and this list is actually a list of functions element in this list is manifold.isomap with all the input arguments there and then the second element of the list is umap with all its input arguments but we could have gone further we could have added a third element and made that pca which took us from two components down to one component or we could have done a completely different data processing pipeline so you can already start to see that you have a lot of flexibility in using the mapper algorithm in practice so we essentially will combine steps three four and five from the overview earlier into one line of code so defining a cover clustering the pre-image and generating an output graph is all compressed down to a single function call in that we pass in the projected data from the previous step the original data set and we defined the clustering strategy that we want to use here we use the db scan with a cosine similarity metric yeah you can also customize the details of the cover but here we're just using the default values and in less than a second it generates the graph and so the next step here i define a file id which isn't really necessary i just like to do it because every time i've used the mapper algorithm i'll try different choices of cover i'll try different projection strategies i'll try different clustering algorithms and so on and i'll typically have these going in a for loop and i don't want the output graphs to get overwritten so i'll define this file id which will automatically generate a unique file name for each output graph and then the last step is we visualize the network you just pass in the graph you define a file name you can give the graph a title you can have these custom tool tips which is the label for each of the members so basically this is our ticker names we can define color values which we will define as the log percent returns we can give a name to the color function and then we can also have multiple options of how these color values are aggregated though we could just do a simple average we could compute the standard deviation the sum the max the min and so on and so what the output of the mapper algorithm looks like is something like this it actually generates a web page which allows you to interact with the graph and blends itself very well to explore to our data analysis which we're doing right now okay so the code that we just walked through will actually generate an html file which we can go ahead and open so first look this does not look like the network i showed earlier but if we go to this help menu we will find different viewing options so we can click e on our keyboard to do a tight layout and already it's starting to look a bit nicer and then we can click p for print mode which will just give the graph a white background and then next we can click on any node we like and it'll start to kind of radiate this glow and then we can go over to this cluster details click on this plus sign and so remember that the nodes in this network are actually clusters of data points and then the way we do the analysis here is we actually have clusters of tickers or in other words stocks so down here you'll see the names of the members of this cluster listed and so this was generated from that custom tool tip option in that last function call we made and then here we also have a histogram which is showing distribution of the log percent returns of the members of this selected cluster so right now the weighted average of the log percent returns of each cluster is what generates each node's color but we could use other statistics so if we go over here to this node color function we can click this drop down menu we could do the standard deviation which doesn't look too exciting we could also do the sum which also looks pretty uniform but then we could also do max so now we're starting to see some variation we then might be curious about the clusters that contain members with high returns so we can click on this yellow node here and then we can look at these ticker names and maybe do some further analysis so i'm no financial expert so i don't have much intuition to offer here but when working with data that you are familiar with you may immediately start to see interesting patterns just by jumping around and you can really do this all day you can click on a particular node see what members are in that cluster and then you can click on adjacent nodes and see what members are in those clusters and then you can go back try out different projection strategies try out different clustering algorithms generate new graphs and then repeat this whole process all right so that's basically it again the code for this example is freely available at the github if you want to learn more check out other videos in the series in the next video i will discuss another specific tda technique called persistent homology if you enjoyed this content please consider liking subscribing and sharing this video like many of you i am still learning so i would also appreciate your questions concerns and feedback in the comments section below and as always thanks for watching you
fpL5fMmJHqk,2022-05-21T19:15:28.000000,Topological Data Analysis (TDA) | An introduction,hey folks welcome back this is the first video in a three-part series on topological data analysis or tda for short tda is an up-and-coming approach to data analysis that focuses on looking at the shape of data this first video will serve as a brief introduction to the topic setting the stage for the next two videos which talk about two specific techniques the mapper algorithm and persistent homology so without any further ado let's get into the video so starting place for all this is the observation that data volumes across different domains seem to be increasing at an accelerating rate and in many cases this is a good thing for example it served as the fuel for so many of the technological innovations in the past decade or so like image search or recommendation systems natural language processing and so on but there's a problem mo data often means mo problems and any practitioner will tell you data from the real world is often noisy and high dimensional and so that's where topological data analysis or tda for short can help and the catch phrase for tda often goes something like data has shape and shape matters so the basic basic idea of tda is to take data and translate it into some kind of shape so the reason this is helpful is that the underlying shape of our data is often much more robust to noise and perturbations that our data generating process may undergo and additionally all the tda algorithms that extract shape from data at least the ones that we will discuss in the series are readily applicable to very high dimensional data sets so all in one package we can both handle the problem of noisy data and high dimensional data with this single framework of topological data analysis so the first key word in the acronym tda topological comes from the mathematical field of topology and the story of topology goes back to a famous problem called the seven bridges of konigsberg and so koenigsberg is an old city which looks something like the image we have here on the left so koenigsberg can be described as these four land masses sitting to the north south east and in the center connected by seven bridges and that's what's being shown in this image here and so the famous problem goes something like this what path crosses all seven bridges without visiting a single bridge more than once so i'll try to say that again in different words so is there a path on this map of konigsberg where we can cross all seven bridges but only visit each bridge a single time so if you want you can take a moment to try to find such a path but it would be futile because no such path exists it's one thing to say it but to demonstrate this mathematically took the work of a very famous mathematician leonard euler and so the way euler solved this problem is by drawing a picture of konigsberg but not like the picture we see on the left here the picture that euler drew was much more simple and it looks something like this it's just a collection of circles connected by lines and today we call this type of picture a graph and we call these circles vertices or nodes and then we call these connections between nodes edges or links and so what this picture what this graph is representing is actually konigsberg where each circle corresponds to a land mass in konigsberg and every line represents a bridge and we have two circles are connected by a line if the corresponding land masses are connected by a bridge so what euler did in drawing this picture drawing this graph is he boiled down this map of konigsberg to the essential elements which eventually allowed him to solve this problem so what euler did all those years ago is basically what we're doing when we do topological data analysis so we take data we can consider this map of konigsberg as data and we want to extract the fundamental shape of this data so as you can imagine there are countless ways in which we can take data and translate it into its underlying shape or structure so in other words there are a lot of ways in which we can do topological data analysis so there's this very nice review article uh by chazal and michelle i probably butchered those names so i apologize but the reference is given here at the bottom so in this article they describe a generalized pipeline for topological data analysis it goes something like this you start with your data you translate that data into some shape some underlying structure it could be a graph just like euler did or it could be something else and from that shape you extract top logical features then you can use those features to inform analysis which can range from just basic statistics all the way to very sophisticated machine learning models so the motivation for this series is that when i was first exposed to topological data analysis it was really exciting i would see these really cool and engaging visualizations applied to very diverse set of problems such as discovering new subtypes of cancer or defining new roles based on basketball statistics or even describing the evolution of politics in the united states but that the problem came when i started to dive a bit more into the details of topological data analysis and i was met with this a wall of terminology in jargon which isn't a bad thing mathematical rigor associated with topological analysis is a good thing a lot of the techniques under the umbrella of topological data analysis are very well founded mathematically but as a non-mathematician as an outsider so to speak trying to figure out what's going on here there was just a barrier to entry to use this stuff in practice and so my goal in this series is to put the terminology aside to a large extent and focus on two main goals one i'd like to highlight key ideas from topological data analysis for non-mathematicians and two i want to share ready to use example codes that showcase what topological data analysis can do so in the next two videos i will be talking about two specific techniques respectively the first technique is called the mapper algorithm what this algorithm provides is something very similar to what euler did we will take any generic data set and we will translate it into a graph then in the following video we will discuss another specific technique under the umbrella of topological data analysis called persistent homology so i hope you've enjoyed this video if you'd like to learn more check out the resources provided in the video description if you found this video helpful please consider liking subscribing or sharing your thoughts in the comments section below and with that thanks for watching
poGxnBR3hEU,2022-02-23T20:52:15.000000,"Multi-kills: How to Do More With Less (no, not by multi-tasking)",in life it can often feel like we have too much to do with too little time and this puts us in a tough spot because we need to figure out how to get more done in less time and this drives people to do crazy things like multitask we all know multitasking it's when you try to do two or more things at the same ish time but really all this is is rapidly switching between tasks and it might work for small things like listening to music on a jog or drive but this can easily get out of hand like trying to do your taxes while cooking dinner there's a lot of stuff out there about the evils of multitasking but to me multitasking isn't so much evil as it is just not an optimal strategy the biggest deal breaker for multitasking is that it splits your attention put another way it doesn't allow you to focus your attention and performance and learning tend to be best when we can really hone in on the task at hand it's like that old saying it's better than a whole ass one thing than to half-ass two things so multitasking isn't a good strategy how do we solve this problem of trying to get more done in less time one idea is we can seek guidance from our bird bird-hating ancestors and try to kill two birds with one stone at first glance this might sound like multitasking but it's not multitasking is like when you do two things around the same time while killing two birds with one stone is getting two things done with a single action the key benefit of this is that we can get more done with less and we're not just talking about saving time here which some might argue multitasking does since the proverbial stone can be anything it can be time money energy attention social capital whatever and on top of all that since you don't have to split your attention in half you can really focus on one thing at a time with this strategy so referring to this idea as killing two birds with one stone is tedious and limited so let's generalize it and define a multi-kill as a single action that results in multiple desired outcomes this word multi-kill might sound foreign but people do this all the time and here i'll share three concrete examples of multi-kills that will hopefully inspire more ideas that you can implement in your life right now the first example is one of my favorite multi-kills which i'll just call a cleaning break the power of breaks is something that took me far too long to figure out and a cleaning break is when you take a break from something mentally tasked take a break from something mentally tax taxing mentally taxing like saying this word a few moments later a cleaning break is when you take a break from something mentally taxing and doing something that's not mentally taxing like tidying up around the house so this could be doing the dishes doing your laundry brushing your teeth you know whatever something that is automatic something that you don't really have to think too much about and notice this isn't multitasking because if you take a break from your computer screen to go wash the dishes you're only doing one thing but you're getting two desired outcomes you're one taking a break and two you're getting clean dishes the second multi-kill boils down to this get paid to do things you do anyway this is like what every adult tells you when you're younger and older too find your passion they'll say things like love what you do and you won't work a day in your life what they also meant to say is love what you do and you'll be multi-killing life if you enjoy your work and get paid for it that's one of the best multi-kills there is and not to mention other things you can get out of your work like developing skills learning new things social interaction like that's like a five thing multi-kill right there this third and last example relaxes the multi-kill definition instead of killing two birds with one stone will use one stone to kill one bird and just slightly injure another so this is like a single action that results in completing one task and then getting started on another so i'll make this a bit more concrete with an example so you may have noticed that each of my youtube videos has a blog associated with them and this isn't just because i like writing and talking about stuff but these videos usually start out as blogs so when writing these blogs i one get a blog out of it and two i get an outline and a starting place for each of these videos so multi kills can be less strict than one might think at first so just to summarize i'll mention three key takeaways one multitasking is not an effective way to get more out of your time two a more effective strategy is multi-kills which is a single action that results in multiple desired outcomes and three we can incorporate multi kills into our lives to get more out of our time and i gave three concrete examples that might inspire ideas of multi kills that you can implement in your life so if you enjoyed this video and you want to learn more check out the blog on medium where i talk about this idea of multi kills more and give a few more concrete examples if you have multi kills that you do already in your life or just some ideas please drop them in the comments section below that would be very helpful to me and others who are trying to get more out of our time and if you enjoyed this content please consider liking subscribing and sharing this video and as always thanks for watching
6m82mLNDCyg,2021-12-29T21:11:41.000000,How to Be Antifragile | 7 Practical Tips,it's been said that the only certainty in life is uncertainty and unfortunately for us uncertainty doesn't seem like something that's beneficial but what if there was a way we could gain from uncertainty in his book anti-fragile author nasim nicholas tele defines something anti-fragile as something that benefits from things like uncertainty harm and disorder and the story goes something like this what's the opposite of fragile i'll give you a second a few moments later most people will say something like robust resilient tough tenacious etc but think of it like this suppose you're sending some very expensive champagne glasses to a good friend in siberia get all the packing material you put the glasses in there and you make sure the contents are securely packed and the box is labeled fragile handle with care the key thing here is this label of handle with care what does this imply this implies things that are fragile don't like don't benefit from variability or disorder or being shaken up so if things that are fragile don't like change don't like variability it's opposite should love it things like robust resilient tough tenacious they don't quite fit under this category something robust doesn't care if there's variability or stability something robust is apathetic toward its environment in the book anti-fragile celeb defines the opposite of fragile as anti-fragile and this is something that benefits from variability and volatility and all these things that don't typically sound like good things so these three concepts of fragile robust or something like it and anti-fragile form what naseem calls a trinity and there are three stories from greek mythology which highlight these three concepts the first story is the sword of democracy which is the story about a servant who sees the life of a king with all his gold all his power and wants nothing more to be in his position and so one day the servant trades places with the king and what he soon realizes is that it's not so great being king because when you have everything everyone wants to take your stuff everyone wants to take your power and the sword sitting above the king's throne is a symbol for the constant threat you are under when you are in a position of power so being king is an example of being fragile because when you have everything you have nothing to gain and everything to lose any changes in the environment and situation will probably not benefit you the second story is the story of the phoenix which is a bird who when it dies it rises again from its ashes and this highlights the concept of robustness the phoenix doesn't care if it lives in a stable tranquil environment or an erratic constantly changing environment because worst case scenario if the phoenix dies it'll rise again from its ashes the phoenix is apathetic toward its situation and the final story is the story of the hydra monster which is the monster that hercules fought and whenever he would chop off one of its heads three more would grow in its place the hydra is anti-fragile because the more you harm it the stronger it gets so when you hear this idea of anti-fragility and you've spent any time in the real world which is constantly changing a natural question is how can i be anti-fragile i'll talk about seven points toward this idea of anti-fragility so i'll just briefly talk about these seven points and i'll leave further exploration and discussion for future blog posts and videos so the first point is having more upside than downside another way to think about this is limiting your losses and leaving your gains unbounded and really this is the core idea behind anti-fragility naseem gives a more technical definition of anti-fragility and fragility in terms of something called convexity which i talk a little bit more about in the blog the second point is having options as opposed to not having options or in other words having a specific and detailed plan in life unexpected things inevitably happen if you sit down and chart out your whole life your five-year plan like this is exactly what my life's gonna be and this is exactly what i'm gonna get out of it you're setting yourself up for disappointment because i hate to break it to you it's probably not going to happen exactly the way you think right now so detailed plans and having no options is fragile while having options helps you be anti-fragile the third point is bottom-up traditions as opposed to top-down rules this isn't so much a specific protocol but is a potentially useful observation bottom-up tradition is evolution at work things get passed down through generations based on usefulness or some kind of appeal and then each generation adds their little twist and nuance to it and updates it based on their situation in other words traditions can adapt and evolve over time based on the needs of the people that practice them as opposed to top-down rules which are rigid it's kind of like having a detailed plan there's only one way in which the top-down rules typically work and if things deviate from how they were when the rules were written they're probably not going to give you the desired outcome the fourth point will sound kind of silly but it's failure failure makes you anti-fragile and success makes you fragile it's just like the king from the sword of democracy story the king is fragile because he's successful he has nothing to gain and everything to lose while failure and mistakes help you be anti-fragile because when you're a failure a loser when you have nothing no one expects anything from you you don't expect anything from yourself so you have a lot more to gain than to lose just like when you hit rock bottom when you hit rock bottom there's literally nothing to lose and everything to gain point number five is in a similar vein and that is staying small as opposed to being big when you're small your mistakes are small but when you're big your mistakes are big it's like that old piece of wisdom mo money mo problems the bigger you get the bigger your problems get what would have just been a small inconvenience when you're small becomes a significant thing when you're big so the sixth point is diversification as opposed to specialization diversification helps you be anti-fragile and this is just like having options when you diversify you're essentially giving yourself more options naseem gives a specific protocol in the book called barbelling which is kind of like this 80 20 rule people typically hear it in the context of business like 20 of your clients earns you 80 of revenue or just something like that when naseem calls bar billing is when you put eighty percent of your resources in a low risk situation while twenty percent of your resources in a high risk situation so a concrete example could be working a nine to five job that pays you a steady salary which you spend 80 percent of your time on but 20 percent of your time you spend on something more risky like a new business venture or side hustle the problem with specialization is the same problem as having a detailed plan if something happens your specialization is no longer needed and in some ways you are no longer needed or if you put all your eggs in one basket you just specialize all your assets you specialize all your resources into one venture you put all your money into that high risk side hustle business venture there's a chance that you're gonna lose everything the seventh point is this idea of independence as opposed to dependence on others when you're independent either financially emotionally or however you want to look at it you're setting yourself up to be anti-fragile while if you're dependent on others for a house a car money a job a hobby whatever it is you're setting yourself up to be fragile so what this ultimately comes down to is if you lose access to these external things to other people or other things you're going to find yourself in an undesirable situation well if you're self-reliant you're not dependent on external things you're independent you're setting yourself up to be anti-fragile because you're not going to lose there's no downside to when your environment changes so as usual i probably rambled on way too much so i want to leave you with three key takeaways the first is something anti-fragile is something that benefits from things like uncertainty harm and disorder the second takeaway is we can classify things into these three categories as fragile robust and anti-fragile and by classifying things into these categories we can study them and hopefully learn what we should do and should not do based on our goals the third and final takeaway is we can do things that make us more anti-fragile and i highlighted seven points that can help us on this journey so there's no way i could have gone into all the details of such a big idea in one video that's why this video is just part of a larger series of videos and blogs exploring this idea of anti-fragility and trying to answer this question how can i be more anti-fragile so if you enjoyed this video please consider liking sharing subscribing and commenting your thoughts if you have ideas or answers to this question how can i be more anti-fragile please leave that in the comment section i think others and myself would love and benefit from hearing that and as always thanks for watching
tufdEUSjmNI,2021-10-26T13:23:44.000000,Causal Discovery | Inferring causality from observational data,hey folks welcome back this is the final video in the three-part series on causality in this video i'll be talking about causal discovery which aims at inferring causal structure from data start by introducing what causal discovery is sketching some big ideas and then finishing with a concrete example with code in python so with that let's get into the video in the previous video i was talking about causal inference which aims at answering questions about cause and effect we talked about a lot of great things we talked about the do operator which stimulates interventions we talked about confounding which talked about estimating causal effects and all these things were great however there was one key assumption that was necessary in order to do causal inference which was a causal mod and obviously a lot of times in the real world we don't have a causal model in hand when we're starting our analysis and it's not always clear which variables cause which causal discovery is one thing that might help with obtaining a causal model and the goal of causal discovery is to find causal structure in data so basically given data inferring the underlying causal model so causal discovery is an example of a so-called inverse problem and inverse problems can be understood in contrast to forward problems for example imagine you have an ice cube sitting on your kitchen counter you know the shape of the ice cube you know the volume and if you were to let that ice cube sit there for a few hours you could probably predict with some reasonable degree of accuracy what the resulting puddle of water would look like the inverse problem is like the opposite of this in other words the inverse problem would be given a puddle of water on the kitchen counter predicting the shape of the ice cube that made that puddle and clearly this is a hard problem because there are several different shapes of ice that could create the same puddle of water connecting this to causal discovery the shape of the ice cube is like our causal model and the puddle of water is like the statistics that we observe in our data so following this analogy there are several causal models that could potentially generate the same statistics we observe in a given data set the approach to solving inverse problems is to make assumptions basically we narrow down the possible number of solutions through assumptions and although assumptions help they often do not fully solve the problem this is where we need to use some tricks to go a little further here i'll talk about three different tricks for causal discovery the first trick is conditional independence testing i start here with a definition of statistical independence which is shown here in other words two variables x and y are said to be independent if their joint distribution is equal to the product of their individual distributions from this we can get a definition of conditional independence which is basically the same thing however now we look at distributions of each variable when conditioned on a particular variable say z we can use this idea of conditional independence testing to do causal discovery and this is actually the main idea behind one of the first causal discovery algorithms called the pc algorithm which is named after its authors clark glymore and peter spears i probably butchered that so i apologize but there's a reference to a review paper by them at the bottom here so i'll just briefly go through the main idea of the pc algorithm more details can be found in the blog linked in the description the first step is to form a fully connected undirected graph so we have a node for each variable in our data set and we connect undirected edges between each of these nodes in step two we do pairwise independence tests so we do an independence test between every possible pair of variables and if two variables are independent we delete the undirected edge between them the third step are conditional independence tests so basically we do the same thing however we pick a variable to condition on then if two variables are found to be conditionally independent we delete the edge between them and we add that conditioned node to the separation set and we continue these conditional independence tests until there are no more candidates for conditional independence testing then in step four we orient colliders so if we have three variables say i j and k we form a collider out of them meaning we make directed edges pointing from i to k and j to k given k is not in the separation set of i and j then in step 5 we add more directed edges to the graph following two constraints namely we do not create any new v structures in our graph nor do we create any directed cycles and hopefully after all that we output a directed acyclic graph which represents the causal connections of our system again more details on the blog and the two references at the bottom here have a great description of the pc algorithm so trick two is a greedy search of the dag space so there are three key concepts here first is a dag which should be familiar since they've been discussed in the previous two videos next is a dag space or in other words the space of all possible dags for example consider the space of dags with two nodes in one edge which is shown here there are only two possibilities x could point to y or y could point to x then finally we have the notion of a greedy search which is a widely used idea in optimization in short a greedy search is an optimization strategy that picks what's best in the short run as opposed to the long run and this is usually done using a heuristic or rule of thumb for example suppose you're trying to get out of a forest you may think i'm trapped in a forest forests have trees so to get out of the forest i should go where there aren't any trees in other words every step you take should be in the direction with the least number of trees so you repeat this strategy and go all the way along this black line until you finally get out of the forest which we can call the greedy path because it is the result of a greedy search however if at the start you were to go in the exact opposite direction of the greedy path you would make it back to civilization much faster so you might say why would we ever want to use a greedy search from the look of it they just seem to give some optimal solutions well the problem is that a lot of the time computing the optimal solution is intractable meaning if you ran an algorithm that tried out all possible solutions all possible paths out of the forest and compared them to each other you would be waiting a long time for it to run and maybe your grandkids would see the solution in their lifetime and this is the problem we face in causal discovery we want to find the optimal dag that best explains a given data set the problem is if we tried an exhaustive search we have to deal with the fact that the number of possible dags is a super exponential and the number of nodes in a graph in other words if we have just three variables the number of possible dags is 25 if we have six nodes we're already over three million possibilities and if we have a measly ten variables or ten nodes in our graph the possible dags is on the order of 10 to the power of 18. so even though greedy searches do not guarantee the optimal solution at least they give us a solution in a reasonable amount of time and so a causal discovery method that uses a greedy search is the so-called greedy equivalence search algorithm so the basic idea of this algorithm is you start with a complete unconnected dag and you iteratively add edges to this unconnected graph such that you maximize a score value so in other words you start with a set of nodes that correspond to each of your variables but no edges between them then you add edges one by one according to some score so the question is what is the score that i'm talking about basically this quantifies how good your dag is or how well the dag explains the data so there are a few options to defining this score one is the so-called bayesian information criterion and source number one reference at the bottom here has a brief discussion for anyone that is interested then you can repeat this process until you reach some stopping criterion which could be some number of edges have been added or the score stops increasing or whatever that may be okay so the final trick is exploiting asymmetry and so as i discussed in the first video asymmetry is a fundamental property of this causality framework so it's natural to think maybe we can leverage asymmetry to help us find good causal models from data and there are three flavors of asymmetry that i've come across and i'll say as a disclaimer that these aren't any kind of standard classification for these things these are just some labels i'm putting on some themes that i have gleaned from looking at this stuff so the first flavor is what i call time asymmetry which is based on the idea that causes precede effects this is what is used in granger causality which is a method to quantifying a asymmetric relationship between two variables based on prediction and more information about grainger causality can be found in this first reference here and there's a lot of stuff out there on grade your causality you can just do a simple google search and you'll probably find a bunch of stuff the second asymmetry is what i call complexity asymmetry which is basically the occam's razor principle that simpler models are better so going back to our ice cube example from earlier following this principle we would say the ice cube that's actually cube is preferred over the more complicated eyes because it is simpler and finally the third flavor is what i call functional asymmetry where better functional fits are better candidates for a causal model so one method that uses this is the non-linear additive noise model the way this works is suppose we start with two statistically dependent variables x and y we then model y in terms of a non-linear function of x then we compute a noise term by taking the difference between y and this non-linear functional fit and then finally we test whether the noise term n is independent of x if it is we accept the model and say x causes y and if not we reject it and then we can do the same thing in the opposite direction where we model x in terms of a non-linear function of y and repeat the same procedure and more details on this method can be found in reference number three and generally all of these are great resources if you're trying to learn more about causal discovery okay so wrapping up these tricks we have a trick based taxonomy and it's important to note that these tricks are not mutually exclusive in all cases there are indeed algorithms that will mix and match different ones for causal discovery as shown in the bottom row of the table here and as a bit of commentary causal discovery seems to me at least to be a relatively young field so there still has not emerged a single or small set of causal discovery algorithms that beat out all others in all situations and i'll also say that this is by no means an exhaustive list of causal discovery techniques however this is probably a good start for anyone trying to get into causal discovery and the references given at the bottom here can get the ball rolling for you okay so i will conclude with a concrete example like in the previous video so we're going to be using the same census data set as before but instead of having just three variables of age education and wealth we're going to include more variables and instead of using the microsoft do y library for causal inference we're going to be using the causal discovery toolbox so again first step is importing libraries loading data then for a lot of these causal discovery algorithms it helps to start with a so-called graph skeleton so this is like step two that we saw with the pc algorithm where we do the pairwise independence testing and we have bi-directed edges or undirected edges between variables that are statistically dependent and then you can visualize the network pretty easily using network x so the first causal discovery algorithm that i use here in this example is the pc algorithm so again we just do that in two lines and it spits out this causal graph the graph is somewhat reasonable it's not perfect but we can see that we have has graduate degree which is like our education variable causes a variable greater than 50k which is our income variable and then we also have age causing our income variable which is what we expected but what was not expected is our education variable has graduate degree is pointing toward age so this is saying whether or not someone has a graduate degree has a causal impact on their age which is not true if you give someone a graduate degree it's not going to have any effect on their age another interesting thing is we have several variables having a causal effect on the number of hours that someone works in a week so whether or not they have a graduate degree has a causal effect on the number of hours they work their age has an effect and whether or not they're female so this is basically their two options male or female in this data set and then the ethnicity information captured by is white is a bi-directional so the pc algorithm wasn't able to break that symmetry but what's interesting is uh hours per week causes a single variable which is in relationship so what this is saying is the number of hours you work per week has a causal effect on whether you're in a relationship or not so we could look at this all day and kind of craft whatever stories we want in our minds but this should definitely be taken with a grain of salt so the next algorithm that we try out is the greedy equivalent search algorithm which uses trick number two greedy search of the dagspace and this gives us a causal graph that is somewhat similar to what the pc algorithm gave us notably that edge between hours per week and is white the symmetry was broken so it's not a bi-directed edge and then finally we use the lingam algorithm and this one doesn't really give us something sensible it's basically everything is causing past graduate degree so whether you make more than 50 000 impacts your graduate degree how many hours per week you work has an impact on your graduate degree and these edges seem backwards this algorithm doesn't seem to do a great job and that's because it's assuming linear relationships between variables and since most of these variables are boolean that's not something that necessarily makes sense code can be found at the github linked at the bottom here put the link in the description feel free to take this data run with it feel free to leave a comment i'd be interested to hear the results of your analysis so that concludes our series on causality we started in the first video introducing this new science of cause and effect the second video we talked about causal inference and finally in this video we concluded with causal discovery if you enjoyed the series please consider liking subscribing sharing commenting your thoughts if you're interested in learning more check out the blog check out the github to get your hands on the example code discussed in this video as always thanks for watching [Music] you
PFBI-ZfV5rs,2021-10-15T22:02:16.000000,Causal Inference | Answering causal questions,hey folks welcome back this is the second video in a three-part series on causality in this video i'll be talking about causal inference which aims at answering questions involving cause and effect so i'll start by giving um introduction to causal inference and sketching some big ideas and then i'll finish with a concrete example with code using the microsoft do y library in python so with that let's get into the video okay so here we're talking about causal inference which aims at answering questions about cause and effect so given a causal model here we have a directed acyclic graph which i talked about in the previous video and from that how can we estimate causal effects for example how can we estimate the effect of x on y so some examples of questions that fall under the umbrella of causal inference are did the treatment directly help those who took it or was it the marketing campaign that led to increased sales this month or the holiday or how big of an effect would increasing wages have on productivity so these are very practical and significant questions that may not be so readily answered using traditional means and i'll try to highlight what causal inference is good at through what i call the three gifs of causal inference so the first gift is the do operator and the do operator simply simulates a physical intervention and we're all familiar with interventions in the real world this is like when your friend's candy habit gets completely out of control and you just have to sit him or her down and say this has got to stop this is what the do operator does but for a causal model in other words it is a mathematical representation of an intervention so suppose we have this model on the left here we have z causes x which causes y what an intervention in x looks like in this mathematical representation is we delete all the incoming edges into x and manually set x to some predetermined value say x naught so significant contribution from judea pearl and colleagues are the rules of do calculus what these rules provide is a way to translate probabilities that include the do operator into probabilities that do not include the do operator so the power of this is that often we can't perform interventions in the real world this could be because it's physically impossible or unethical or whatever reason for example intervening in someone's height by making them taller to measure the response in basketball ability is not physically feasible or intervening in smoking by forcing someone to smoke a pack of cigarettes every day to measure the response in the risk of lung disease is unethical so in other words often in the real world we have no way to collect data about the interventional probability distribution that is we don't have access to data about probabilities that include the do operator in these situations the rules of do calculus may provide a way to re-express to rewrite probabilities that we are interested in but can't measure directly so the second gift of causal inference is clarifying this notion of confounding and confounding at least for me was something that did not have a clear definition until i read judea pearl's book the book of why so in his book pearl defines confounding as anything that makes the interventional distribution different from the observational distribution in other words anything that makes a probability of y given an intervention in x different from the probability of y given an observation in x so this is easy to see in the three variable case so here we have an example of a causal model which shows the relationship between age education and wealth in this example age is the confounder and this can be understood as age is a common cause of education and wealth which is an idea that's been around for a while as pearl discusses in his book many people took this kind of common cause definition as a definition for confounding but what pearl does in defining confounding in this way in terms of the interventional versus observational distribution is becomes much more easy to generalize this notion to much more than just three variables okay so what does this mean practically if we know age is a confounder this can help inform our analysis of data that we might collect of these three variables so suppose we have this data here of age education and income and we want to assess the impact of education on income if we don't take into consideration age bank a confounder the naive thing to do would be to just partition the data into two subgroups one group has just a high school education and the other group has a college education and just compare their difference in income but since age is confounder this wouldn't give you the best result so knowing what the confounders are of your problem allow you to perform this analysis a different way so in this specific case since age is a confounder we shouldn't compare data between age groups we should compare data within age groups so that's what i'm showing here you can imagine this single data set being split off into four separate data sets uh where we have the blue data set people in their 20s the yellow dataset people in the 30s people in the 40s in red people in the 50s in green and then we repeat the analysis i was talking about before where we kind of compare the incomes of people without just high school education versus college education so you may ask why do we care about this do operator why do we need to talk about interventional probabilities versus observational probabilities and so on ultimately what these tools provide are a way to estimate causal effects so a causal effect is a way to quantify the causal impact that one variable has on another and this is a core part of causal inference so this is what we were naturally doing in the previous slide when we were trying to assess the impact of education on income what we were really doing is quantifying the causal effect that education had on people's incomes but this is obviously applicable to other situations when we ask questions like what productivity be increased if we increase wages or how would sales change if we increase the marketing budget with these questions and several more we're talking about causal effects what is the causal impact of wages on productivity what is the causal effect of marketing spend on sales so looking at the same example as before we have a causal model including age education and wealth we know from the previous slide that age is a confounder because it creates a discrepancy between the interventional and observational probability distributions we can consider education to be a treatment and wealth to be a response to that treatment and then suppose with this causal model in hand we collect some data very similar to what we were talking about in the previous slide but now we're set up to do causal inference we're set up to ask and attempt to answer a question involving cause and effect so a question might be is grad school worth it which might be something someone watching this video is thinking about or something someone is reminiscing upon and wishing they would have known about causal inference before deciding to go to grad school and they're already waist deep into it either way one way to frame this question of is grad school worth it could be what is the treatment effect of education on wealth i'm not saying this is the best way to do it but this is a way we can do it so i'll use this opportunity to run through a concrete example with code in python so the example code is at the github link at the bottom here i also put the link in the description but basically here we're going to estimate the treatment effect of education on income so first we download some libraries load some data this is real census data from the uci university of california irvine the machine learning data repository i don't know the specific name but here is the uh link here to do the causal effect calculation i use the do y library which is a microsoft library for doing causal inference so the next step is we have to define our causal model so again the starting point of all causal inference is a causal model so we need to start with our dag which is the same as we saw in an earlier slide just that education now has a new name called has graduate degree and income has a different name which is greater than 50k so these are both boolean variables which means they're true or false variables so either someone can have a graduate degree or they don't or either they make more than fifty thousand dollars a year or they don't and then age is just an integer next we need a s demand which is basically a recipe for estimating our causal effect you can just do this in one line using the do y library and then finally we can estimate the causal effect so here we're using a t learner which is a type of meta learner i can link a paper talking about meta learners in the description i won't jump into all the details i'll just kind of jump to the result which is the average causal effect is 0.2 so one way to interpret this is having a graduate degree increases your chances of making more than 50 000 a year by 20 however we had a lot of samples in this data set and we've just reduced all those samples to a single number which was the average which may not always be the most representative number so it's always good to plot the distribution and when we plot the distribution so here we have on the x-axis the causal effect the y-axis is the count the number of records or people that had that individual causal effect we see that the distribution is not gaussian so if the distribution is not gaussian that means the average is not a very representative number for that distribution so in other words even though a lot of people had a 0.2 treatment effect there were also a significant number of people that had no treatment effect so it seems we're no closer to answering the question of is grad school worth it however one thing one could do is to dive into these different cohorts kind of look at the samples that had no causal effect from a graduate degree and then look at the people that had a significant causal effect and then you can start to answer the question like what kinds of people benefit from a graduate degree and what kinds of person don't benefit from a graduate degree and then maybe that can kind of help you answer this question so again codes on the github feel free to take it run with it do whatever you want extend the analysis further post your own youtube video about it i'll be really interested to see if anyone actually takes a look and tries to answer this question of is grad school worth it but i guess it's a little too late for me at this point so that was the second video in the three-part series on causality we talked about causal inference which aims at answering questions involving causality however the starting point of all causal inference is a causal model which may not be so easy to have in hand that's where the topic of the next video can be helpful which is causal discovery and that aims at obtaining causal structure from data alone so if you enjoyed this video consider liking subscribing sharing commenting your thoughts i'm always happy and interested in reading the comments check out the blog if you want to get some more details on causal inference and check out the github to get the example code talked about in this video and thanks for watching [Music]
WqASiuM4a-A,2021-10-04T20:02:15.000000,Causality: An Introduction | How (naive) statistics can fail us,hey folks welcome back i'm finally sharing another data science series this video is the first in a three-part series on causality so this idea of causality is mainly based on the work of judea pearl and other researchers working this space pearl actually has a very accessible book out called the book of why geared toward a public audience which i will share in the description in this video i will introduce this idea of causality kind of highlight why traditional statistics isn't the most helpful for understanding it and then finally introducing a new mathematical formulism for understanding causality if you want to dive a bit more into the details check out the blog which i will link in the description and without any further ado let's get into the video well you're probably thinking is why is there a banana on the screen why did i click on this youtube video so we are constantly asking ourselves why why did this happen what is the cause of this or where is this going what's the effect we ask ourselves why to help us craft stories narratives to help us make sense of the world and even though this is a very natural thing for us in understanding and reasoning one of our most powerful tools in statistics is in many ways inadequate for handling cause and effect i'll try to highlight these inadequacies with what i call the three traps of statistics the first trap we have is spurious correlation so this is a statistical correlation with no causal implication so this is like the old saying correlation is not causation and you don't have to look far to find examples of this uh there's a website uh tylervegan.com i have it at the bottom left here and i'll link it in the description as well so here we have a case where we have a spurious correlation so the number of people who drown by falling into a pool correlates with the number of films nicholas cage appeared in so even though this relationship is hilarious it is not causal because we know these two things are not causally uh related to each other correlation is not causation which is something that we all know so sperry's correlation is pretty well known we've all heard correlations not causation uh however trap number two is less well-known and this is simpson's paradox which basically um highlights that how you look at your data matters so let's imagine we do a study for an experimental treatment for heart disease and we collect a bunch of data and we plot it so on the x-axis we have our experimental treatment this could be a drug or behavioral protocol the y-axis we have risk of heart disease and if we look at the plot we would say to ourselves this is a terrible treatment for heart disease it seems the more treatment someone gets the higher the risk of heart disease however if we were to look at two subpopulations say men and women we would get the exact opposite effect so this is summarized nicely by a quote from the man himself judea pearl who said we have a treatment that's good for a man good for a woman but bad for a person here's another example of simpson's paradox but with numbers i took this from the wikipedia page on simpson's paradox so here we have batting averages of derek jeter and david justice over the years 1995 and 1996 so if you look at those two years individually you see that david justice has a better batting average but if you were to combine those two years together derek jeter has a better batting average so again how you look at your data what variables you condition on how you slice your data set has an impact on the conclusions that you can make the final trap of statistics is symmetry which from many perspectives isn't much of a setback but when you're talking about something like causality which is inherently asymmetric it can cause some issues so i'll highlight this by an example let's say we want to model the causal effect between a disease and the severity of symptoms so we model this by a linear expression so y is the severity of our symptom x is the severity of a disease b is all other factors involved and m is just a coefficient that relates x and y but here we have an equal sign so the left left-hand side equals the right-hand side that's what equals means so that means using algebra we can rearrange this expression to get a equation of x in terms of y but here's the problem if we interpret the first equation as diseases cause symptoms then we have to interpret the second equation as symptoms cause disease which is not true we know that's not true so this fundamental symmetry makes algebra perhaps not the best formulism for representing causality so this whole video is supposedly an introduction on causality and i have not defined what it is there are few ways we can define causality the one i like is x causes y if when all confounders are adjusted an intervention in x results in the change in y but an intervention in y does not necessarily change x so i have a little cartoon here let's say we have four variables x w z and y if we intervene in x that means we jiggle it a bit we if x causes y we'll see why jiggle as well however if x causes y but y does not cause x if we intervene in y that is we jiggle y a bit x will not respond so that's causality it is fundamentally asymmetric so if we can't use algebra which relies on symmetry it has this equal sign how can we represent causality so there are the so-called structural causal models which is the kind of way we can represent causality and this consists of two parts one is a directed acyclic graph or a dag so this is a type of graph which comes from the mathematical field of graph theory which consists of vertices these circles here and edges which are these arrows and this is called a directed graph because the lines connecting the different uh circles together have arrowheads on them so that's called a directed graph because the information so to speak flows in one direction and it's acyclic because if you start at a vertex or one of these circles and you follow the arrow heads you'll never return back to the same variable or the same vertex so that's a directed acyclic graph and then there's a second part which are the structural equation models so these are equations that kind of outline the details of the causal connections and so they have these funny looking equal signs here which is basically saying you can't invert these expressions for example you can't invert f sub 1 to get an equation for x in terms of w so these are two key pieces of causality so that was the first video in the three-part series on causality in the next video we will be applying this idea to answering practical real world questions with causal inference if you like this video please consider liking subscribing sharing and commenting your thoughts if you're interested in diving a bit more into the details check out the blog on medium thanks for watching
m19FqRrmvIE,2021-07-18T23:47:21.000000,Why Conflict Is Good & How You Can Use It,are we recording yeah have you ever all right let's just relax have you ever wanted to avoid an uncomfortable conversation or maybe tell someone off or raise your voice or even three hours later if yes then you've been part of a conflict hey folks welcome back this marks the second productivity video i hope you guys enjoyed the first one even though it's not doing so hot but i'm still gonna make this video because i love it in this video i'll be talking about conflict and even though conflict doesn't feel so good it can have a powerful and positive impact on our lives so there's a great book the five dysfunctions of a team this was probably what really sparked something for me um i'd always been interested in like working with people in teams just because you know just look around anything that has made a significant impact on the world it's not just one person doing it alone it's a group of people all moving in the same direction but in the five dysfunctions of a team the author patrick lencioni he describes five key dysfunctions that teams go to i'm not gonna go through all of them but i'm gonna highlight one of them specifically which is the fear of conflict the fear of conflict is widespread no one likes conflict it's very uncomfortable it's very unpleasant and like most things that are uncomfortable and unpleasant we will do almost anything to avoid them so a tail tail sign of the fear of conflict is what lencioni calls artificial harmony so basically what this means is even though there's no obvious conflict in your team no one is yelling no one is trying to imitate karate actions in front of each other there is kind of hidden conflicts there's things that bother people but no one says anything about it and that is bad the metaphor i like to think of is that's like storing barrels of gasoline in your basement all it takes is one little spark and the house goes kaboom so instead of storing gasoline in the basement teams people need to have controlled burns so in other words what that means is you need to have conflict which brings me to the solution or a solution to the fear of conflict which is ironically conflict itself so the solution to the fear of conflict is conflict itself and conflict is a very scary and negative notion in our minds so what makes the fear of conflict so bad is that well it's two things really one it kills honesty when there's hidden conflicts when you have these barrels of gasoline in your basement it kind of constrains the space of allowed speech you can't say certain things some things are taboo some things are off limits some things you know you're holding something back and if you say it it might explode and you know everything's going to be terrible so this fear of conflict kills honesty people can't be honest with each other and maybe not even honest with themselves and the second thing is without these conflicts teams don't go through the things they need to go through in order to build trust it's when we kind of overcome obstacles and challenges where relationships are really strengthened informed by avoiding conflict we are denying ourselves the opportunity for honesty and trust okay so now let's talk about fear the fear of conflict like many other fears doesn't usually stem from any kind of actual danger what it really comes from is the unknown so consider the example of driving to work driving is probably one of the riskiest and most dangerous things we do on a daily basis but no one's afraid to commute to work and that's because we do it every day it's familiar and things that are familiar aren't scary so in a similar way just like how everything every time you drive and you survive it becomes less scary every time a team goes through a conflict and survives conflict becomes less scary okay so conflict solves the fear of conflict but what else does conflict do i think there are two main things two main benefits of conflict number one if there's conflict that's a signal that there is diversity in your team and diversity is not just like a you know corporate term or something that people do for their pr diversity is actually critical if you're trying to solve a diverse set of problems and then the second thing which i touched on earlier is that conflict builds trust it's when we kind of experience hard times we experience something uncomfortable and come out the other side still alive that we start to trust each other and form long-term relationships and here's the reality of conflict conflict isn't just arguments it's not just fisticuffs conflict is a spectrum it could be a reasonable rational disagreement with someone or it could be a full-blown you know rumble royale whatever they call them and even though this isn't directly helpful okay conflicts can be you know reasonable or it can be very unreasonable even though that's not directly helpful what it does is it highlights the flexibility we have in what kind of controlled conflicts we engage in so a conflict could be normalizing criticism it could be making sure everyone has the opportunity to say their opinion even if it's vastly different than what other people might think or it could just be promoting productive discourse or discussions and we can actually use conflict as a tool as opposed to an obstacle so the first thing is conflict is uncomfortable which is obvious but what this means is any sign of discomfort like the idea of talking to a co-worker about something that happened the other day or something they may have said if that notion is uncomfortable to you then that is a sign of underlying insecurity and or underlying conflict so what does this mean that means discomfort as opposed to being an obstacle is an opportunity for us to resolve underlying conflicts or resolve underlying insecurities that our team may have so what this does is it informs a three-part kind of three-step protocol for growth the first step is instead of moving away from discomfort you move toward discomfort this could be in the context of a team or even in the context of an individual or a personal life the second when you move toward discomfort you uncover conflicts and then the last step is once you've uncovered conflict you've discovered underlying insecurities and if you follow that three-step protocol the first step being moving toward discomfort you're gonna find yourself uh growing at the end of it even if it's very uncomfortable and the last thing about conflict is although some conflicts are more productive than others i believe all conflict is good given a single requirement is met and that requirement is people need to be willing to see it through to the end people need to be willing to stick around to see the resolution if that happens i believe any kind of conflict is good conflict what makes bad conflict is things go wrong things get heated  happens quoting the last video and people run away people avoid the resolution because they feel some negative emotions or something that's the worst thing you can do because you're denying yourself all the benefits of conflict by not seeing it through to the end and the last thing this is really the last thing people think that the worst thing for a team is conflict and that's wrong the worst thing for a team is apathy apathy is a sign that people have given up on the team the effort of being involved in the team is not worth the benefits of being in the team and apathy is really the start of a countdown once apathy sets in if people don't if that apathy is not resolved the teams start to fall apart so once again i rambled uh to a tremendous degree about something so i'll just kind of wrap everything up with three key takeaways the first takeaway is the fear of conflict is a fundamental dysfunction of teams with a unexpected solution which is conflict itself the second takeaway is even though moving toward conflict and discomfort is very unnatural it provides tremendous opportunities for teams and individuals and then the third and last takeaway is all conflict is good given everyone is willing to see it through to the end so that ends the second productivity video on conflict i hope you guys enjoyed the video if you have crazy experiences with conflict that you just gotta share please feel free to leave them in the comments section and as always like comment subscribe share and thanks for watching
OmNVB3ff98s,2021-05-10T20:13:32.000000,"Shit Happens, Stay Solution Oriented",i hate to break it to you guys but this camera angle is too close i hate to break it to you guys but happens and ironically when s happens people become less productive and more pro destructive i'm back with another video but this time i won't be talking about technical math and science stuff i know i know you guys are upset calm down this is the first of hopefully many videos on a completely different topic productivity so there are two main motivations for making this video one although i hope people have been getting some value from the technical videos i know they're geared toward a narrow audience and two which is hilariously clear to people that know me i have a bit of an obsession with optimization and i view productivity as the most practical optimization problem namely how can i get the most out of my time obviously this is not a question that can be answered in one video a series of videos or even a lifetime worth of videos however i want to share some hard learned lessons and general tips that have been helpful to me and my goal is not only to kind of share them so it can help you but also by putting this out there i'll have the opportunity to learn from you guys get your tips and your thoughts on productivity so each of these videos will focus around a central idea i'll kind of talk around these ideas and hopefully leave you guys with some practical tips and advice that you can implement in your day-to-day topic of this video is probably the first big piece of advice that i learned in my productivity journey which is stuff happens stay solution oriented which is happens stay solution oriented when i was an undergrad i worked at my university student activities board as an event planner and in event planning there's no way you can prepare for everything but what you can count on is something going wrong and when things go wrong there's one common trap that we fall into people start to play the blame game the blame game is a trap basically the goal of the game is to assign blame as opposed to work toward a solution you know there are too many negatives of the blame game to even list but the first few that come to mind are one it's a waste of time and energy basically every minute you spend playing the blame game the minute you don't spend working toward a solution or the second thing it doesn't solve the problem but it superficially satisfies negative emotions it's like a double-edged sword where both options are bad so not only are you not working toward a solution people will get this kind of like relief from the problem blame is like a proxy for the solution so they no longer kind of think about it or they don't think it's such a big deal anymore and kind of just move on to the next thing and then the last thing is that it can often breed further conflict so like you know you're doing an event or you're working like a service job at like retail or a restaurant you have an audience in front of you and the last thing you need is negative emotions to kind of hijack the situation and take you further from the solution and closer to even more problems so even with all these obvious flaws of the blame game it's natural it will happen you know people just through no fault of their own just kind of get wrapped up in all the emotions the heat at the moment and they just kind of need someone to blame in order to move forward the best piece of advice that i've learned kind of navigating situations like this if you're a leader just take the blame so your team can move forward and even if you're not in charge or you're not the leader or manager or whatever it is the same advice usually applies but uh it's probably a tougher situation to be in so this s stuff happens and we're supposed to say solution oriented during all this stuff how do we do that taking the blame is hard and staying solution oriented with all these negative emotions flying around is also hard our bodies want to keep us safe we're ready to run we're ready to fight we're ready to do all these things you know in a survival context makes a lot of sense in our everyday situations everyday problems not very helpful unfortunately i don't know any hacks on how to stay solution oriented and it's not something that comes naturally usually for me i just try to think of things very simply i ask myself what's the problem then what's the solution everything else is irrelevant so a big part of this is you know exactly managing all these negative emotions like your own that of your team um you know part of a great team that's pretty easy you know you have that trust that safety um and it's pretty easy to talk about um things that are uncomfortable but a lot of teams they have like underlying conflict which is okay you know as long as you're working toward trying to make things better it can be very challenging to manage the negative emotions i really buy into this idea that uh emotions are infectious so like how do we use this the basic idea is if you're able to maintain a positive and productive mindset you know keeping things simple what's the problem with the solution staying solution oriented focusing on the positives this mindset will kind of cascade out to everyone on your team whether you're in charge or not you don't have to explain anything to them all you have to do is be you just gotta do that and naturally we pick up on each other's emotions and we just kind of conform no one wants to feel negative no one wants to be stressed and no one wants to be mad so i feel like there is like a bit of a gravitational pull uh of being positive so find a way to be solution oriented i i try to keep things simple i'm sure there are a bunch of ways if you have ways that you do it that's really helpful to you please share that in the comments i'm very interested and then on this topic of negative emotions you know stress stress is what we think of when we think of negative emotions and it makes me think of a story an idea a thought i learned from an old colleague of mine kj he was a master salesman he kind of had every position you could have in the auto industry from sales person all the way up to general manager which is kind of like as high as you can go eventually he was the general manager for a new car mercedes-benz store but basically i'm paraphrasing uh if you want to hear the full story to my it's in the blog in the description basically he said he doesn't get stressed it's simple there are things he can control there are things he can't control for the things that he can't control there's no need to worry about it the things he can control is just a matter of knowing the solution knowing the process and just doing it stress is when people worry about the things they can't control or they don't follow the process they don't execute the solution for the things they can't control so this kind of goes back to this idea what's the problem what's the solution you can't just at will remove negative emotions they're here you have to find a way to kind of navigate that situation and a lot of times if you say i don't want like don't think about that thing or you try to ignore something you try to ignore negative emotion it does the opposite it actually brings it in in my experience if you say don't think about this don't think about this every time you say that become it gets closer and closer and then that's all you can think about what's usually more helpful more doable is to not ignore the negatives but try to focus on the positives don't ignore don't try to ignore uncertainty just try to focus on certainty you know your attention and your focus is limited so there's only so much that you can you know think about and put in your head at the same time so if you just fill up all that all those slots of readily accessible memory with certainties and positivity and solutions there's no room for negatives there's no room for uncertainty there's no room for unproductive things so to package that all up it's about managing what you're focusing on not what you're trying to avoid all right so i rambled about a lot of stuff today i'm just gonna summarize everything with three key takeaways okay so one beware of the blame game two take the blame so everyone can move forward and three focus on the positives and others will follow and as always like comment subscribe share that's it thanks for watching
zJsl4lFyr6w,2021-04-05T19:19:10.000000,Kmeans-based Blink Detecter DEMO,n/a
GgLaP4Des1Q,2021-03-17T14:53:33.000000,Independent Component Analysis (ICA) | EEG Analysis Example Code,hey guys welcome back this is video number two in the two-part series on principal component analysis pca and independent component analysis or ica ica is the topic of this video so much like the last video i'll start with a brief introduction into the technique dive into the math a little bit i will compare the two approaches talk about their similarities and differences and then i'll finish with a concrete example of how you can use ica with some example code provided in the github repository so let's get right into it okay so the standard problem for independent component analysis is the cocktail party problem so in its simplest form you can think of two people having a conversation at a cocktail party and for whatever reason you happen to have two microphones kind of set up next to these two speakers both microphones are going to pick up audio from both of the speakers uh kind of like our purple microphone and pink microphones here the purple microphones a little closer to the blue speaker so it picks up more of the blue speaker's audio relative to the red speaker and vice versa the pink microphone picks up more audio from the red speaker than the blue speaker so then the problem is how can we take these audio recordings that have both speakers kind of side of the conversation mixed together and separated out um to two audio files uh each of which only contain audio from a single speaker well that's exactly what independent component analysis does it trans uh transforms a set of vectors so you can think of the raw or you can think of the recorded audio by these two microphones into a maximally independent set so so that's what's being represented here um so you have the purple and pink audio signals then they get translated uh to the original the sources of the audio which was uh the speech from the blue speaker and then the speech of the red speaker respectively so again the purple and pink are your measured signals the blue and the red are the independent components or the source of the information or the audio okay so there are a couple key assumptions to independent component analysis so assumption number one your independent components are statistically independent and that's defined in the typical way in statistics the joint distribution of two variables x and y is equal to the probability distribution of x times the probability of y and then the second key assumption is that your independent components are non-gaussian which might be a little strange you know in statistics and science we love to say everything's gaussian it makes things much nicer and it allows us to do a lot of rigorous analysis but this is one of the instances where we actually need the independent components to be non-gaussian for this to work okay so we have our measured signals again that's from your microphone example and then the independent components which is what your speakers are saying in the cocktail party problem so we can use our independent components we can combine them in some way so that's what's being represented by this expression to kind of recreate our measured signals x you can think of the independent components as being sources hence that's why this vector is an s they are sources of information or audio that are being combined in some way to generate what's being measured at your microphone for example so we have x1 and x2 your measured signals and then your independent components or the sources of your signals s1 and s2 but you can also kind of turn this around and you can combine your measured signals to express your independent components if this is the case if you can just have some linear combination of your measured signals to derive your independent components the set of values defined by w is all we need in order to do ica okay so mathematically the goal is as follows so given some uh measured signals given some data x we want to solve for the matrix w such that the set of independent components or the set of source vectors s sub i are maximally independent this concept of maximally independent what does that mean how do we quantify that so there are two ways you can define w in such a way that it minimizes the mutual information between all your independent components or you can maximize the non-gaussianity of the independent components defined by uh this w matrix um i'm not going to go any further than that if you're interested in more information check out the blog post linked in the description i kind of go into a bit more detail on the math pc and ica they're similar techniques in a lot of ways but ultimately they're distinct they are different approaches that kind of aim at different tasks or they make different goals so pca typically compresses information if you saw the pca video the example was hot dogs and hot dog buns those are two quantities that are heavily correlated so instead of representing that information with two variables you can represent it with just one and so that's where pca is a good thing to use because it will compress those variables into those two variables into a single variable on the other hand ica separates information it's going to take two variables for example like your two speakers or the the audio picked up by two microphones placed close to two speakers and it's going to separate out the independent components or the sources or the independent drivers of those measured signals so kind of similar but they are different goals and different final outcomes a commonality between pc and ica is auto scaling so this is a critical part of the preprocessing so auto scaling is for each variable you have to subtract the average of that variable and divide each element by the standard deviation of that variable and then that's one of the reasons why it's typically advantageous to apply pca to your data set before applying ica because it kind of all the pre-processing is already handled for you pca will clump all the information together the correlated variables and then ica will come in and separate out independent drivers if it's applicable okay so as always i'm going to include a concrete example so here this is something that that's relevant to my research and this is where i kind of came across the whole technique of independent component analysis was to solve this specific problem in my research we deal with uh eeg data so what is eeg uh eeg is a technique of measuring brain activity uh by placing a set of electrodes on the head you know eeg is a very powerful technique because it has a very good temporal resolution and it's also non-invasive people can kind of move around with the this cap on but that kind of also leads to one of its more fundamental weaknesses is that since the electrical signals that it's trying to measure from the brain are so weak eeg has to be very sensitive to these kind of fluctuations and voltage which makes it very prone to artifacts or perturbations oscillations in your signal that do not come from brain activity so this could be like blinking which is the what we're trying to resolve in this problem or motion artifacts people talking or other kinds of noise that can kind of get injected into the data so here we have a plot of the voltage versus time sorry i should have had labels on these axes but the y-axis is voltage in millivolts the x-axis is a time index essentially and this is for the fp1 electrode which sits near the front of the head on the left side so on your left forehead and so this electrode is particularly prone to blink artifacts because it's the one of the closest electrodes to your eye and then you can actually see the blinks occurring because you'll have these giant spikes in the signal so we're trying to get rid of that because with eeg we're trying to measure brain activity not blink activity okay so the first step here is applying pca so here we have 64 electrodes on our eeg so that translates to 64 variables so we can use pca to kind of clump that down to just 21 variables and i just did some trial and error to find the right number of principal components to go down to and at the bottom here you can see the explained variation is 99.5 percent and in matlab it's really simple you may notice i don't explicitly auto scale the data that's because the pca function in matlab does this automatically which is pretty nice but it's all done in one line in matlab and it can be done in one line in psyc or a couple lines in psychic learn and if you didn't check out the previous video on pca that'll share some example code on how to do that okay and then again we can apply ica to the set of principal components that we got from pca so that's what's being done here so now we can just plot all the independent components okay so again we had 64 electrodes on our eeg cap that translates to 64 variables we then use pca to reduce the dimensionality from 64 variables to just 21 and then finally we applied ica to those 21 variables to kind of separate out the independent components and then just looking at this visually uh kind of independent component 10 5 and 12 are reminiscent of those blank artifacts we saw in that initial plot and again these aren't the just the independent components themselves or the raw independent components i actually squared them so that all the values would be positive and then the blank artifacts would be a bit more prominent okay so i just used a rough heuristic basically i picked out the independent components which had four prominent peaks and so this isn't a robust way to do it i was doing something fast and wanted it to be repeatable so it picked out independent components 10 and 12 to correspond to the blanks okay so 10 and 12. i'll buy that maybe five should have been included but we'll see how it turns out okay and then we can essentially just drop independent components 10 and 12 because they contain blink information which we're not interested in we only want brain information so we can drop those two components and then just work backwards we'll reconstruct our score matrix basically the output of pca and then we can reconstruct our original 64 variables by going backwards in pca so doing that and plotting everything before the blink removal fp1 had these four prominent peaks corresponding to blinks and then afterwards uh they went away so it's kind of like magic and this is a rough way to do it there are other ways to do it but this is more so just as an example of what ica can be used for so that concludes the two-part series on principle component analysis and independent component analysis if you found this video helpful please like subscribe comment share i would very much appreciate that and i would love to hear your feedback i look forward to seeing you guys in the next video and thanks for watching
WDjzgnqyz4s,2021-02-22T19:39:27.000000,Principal Component Analysis (PCA) | Introduction & Example (Python) Code,okay this is good i got the angle quite on the set hey guys welcome back hey guys welcome back i'm back with another series if you missed the first one it's available on my channel it was on time series signals the fourier transform and the wave of the transform in this new series i'll be talking about two things one principle component analysis and two independent component analysis so principal component analysis or pca is the topic of this video so i'll give you a little intuition share some math and then i'll finish with a concrete example of how you can use pca to analyze the stock market so let's get right into it so the analogy i like to think of for pca is imagine like a massive rock band with like 20 members in the ensemble and you have you know two drummers several guitarists you have several keyboardists or pianists you have a string section a horn section vocalist percussionist the whole works so you have this 20 person band and you know that's not a big deal that's uh you know that's the kind of band made for huge arenas and stadiums but if a band like this is just getting started they're gonna have a hard time fitting in smaller venues like coffee shops and restaurants so a natural solution to this problem is to just kind of reduce the number of players at specific performances so instead of like a keyboardist pianist and whatnot you could just have one person on the keyboard instead of having multiple guitars you could just have one person do an acoustic guitar instead of two drummers and a percussionist you can have someone banging on the bongos and so on in a lot of ways this is basically what pca does so this is the big band on the left is before pca and then you can kind of boil it down to its core elements for the same band to play at the coffee shop but instead of uh a band you can think of a pca applying to a data set instead of musicians or players in the band you can think of the variables in your data set and instead of a song or the music you can think of what your data set is representing a bit more concretely principal component analysis pca reduces input dimensionality and redundancy so we can think of two variables x and y this could be something like hot dogs sold and hot dog bun sold which are directly correlated but in a lot of ways contain redundant information so it may be practical to represent this underlying information instead of through two variables through just one variable and then that's uh application of pca so we can transform our axes from this x and y axis to a new set of axes we'll call them pc1 and pc2 and then if you want to take it a step further you can just remove pc2 and just operate with one variable so essentially we've reduced the dimensionality from two variables x and y to just one pc one if we choose to drop pc2 okay so how does it work the basic idea the goal of pca is to reduce variable redundancy or input variable redundancy by creating a new set of variables where the variance along each subsequent variable is maximized so in the previous example we saw pictorially that we changed from a set of two variables hot dog sold and hot dog bun sold to a new pair of variables we call them pc1 and pc2 and essentially pc1 contained all the relevant information we needed and the way we got pc1 is basically rotated the axes to be kind of along this linear slope of points defined by the hot dog bun and hot dog sales what does that translate to mathematically so we can think of this situation so we have x which is a matrix of data where the rows are data records and the columns are variables we have w which is a vector of weights and then we have t which is a score vector and what i'm going to be calling a principal component so t is what we're interested in we have our data x and we're trying to find uh a w that is going to create this principle component for us okay so here's here's the magic of pca here's the trick to it all so the goal here is to maximize the variance of t subject to the constraint that the norm squared of w so w transpose times w is equal to one okay and then variances uh defined in the usual way so you take every element subtract the mean of the variable you square it and then you divide by uh the number of elements minus one and then you just add this up for every single element in the set of numbers um and so one really important thing when doing pca is you want to auto scale your data so basically what does that mean for each number in each column of your matrix uh you want to subtract the average and divide by the standard deviation so if we do that then the mean of the principal component will turn out to be zero which allows us to kind of drop the mean term in the variance here it turns out that the variance will just be equal to the norm squared of t divided by uh the number of elements minus one okay so what does that mean that means we can rewrite this optimization problem instead of maximizing the variance we can just maximize the norm squared of t because the the vector w that maximizes the norm squared of t is also going to be the same vector w that maximizes the variance of t okay so we can rewrite uh the optimization problem using our above expression for t and it turns out this is actually a pretty straightforward optimization problem to solve and don't be intimidated by the matrices and vectors we can use a very well known and common technique in calculus known as the method of lagrange multipliers which basically allows us to rewrite an optimization problem with constraints a constrained optimization problem as a optimization problem without constraints or an unconstrained optimization problem if none of that makes sense that's fine we just need these relevant expressions here so we can write out the lagrangian which is this l of x uh term here for our pca optimization problem and then we can have the associated equations and this is the exciting part here this first equation if we rearrange it is just an eigenvalue problem which is a standard problem in linear algebra and then the second equation is just a restatement of our original constraint so writing it explicitly here we can solve for the eigenvalue lambda and the vector of weights w using standard eigenvalue approaches if you're doing this in some programming language every programming language like r python matlab they're going to have built-in functions that allow you to solve this problem and then once we have this vector of weights we have everything we need we can just multiply that by x and we can get our principal component and then this naturally extends to multiple components so this we started out just looking for a single component but if you solve the eigenvalue problem your and you have n columns in your matrix x and x is square you're going to end up with n eigenvalues and n corresponding eigenvectors and then if you kind of sort these eigenvalues and eigenvectors from largest to smallest you sort from the largest eigenvalue all the way down to the smallest each corresponding eigenvector w is going to be a set of weights which define a principal component and the principal components associated with the larger eigenvalues contain more information than components associated with smaller eigenvalues so you can define some threshold like in the first slide where we could have just dropped pc2 because it wasn't giving us much additional information you can do the same thing and kind of truncate your variables after a certain amount of information is captured with your principal components okay so just as a recap principle component analysis it reduces input dimensionality and redundancy some key points are new variables are created to be a linear combination of input variables so that's kind of what we saw in the previous slide where you had a matrix multiplied by a vector of weights that's equivalent to a linear combination of your input variables and then each subsequent new variable contains less information we kind of saw that once you sorted your eigenvalues from largest to smallest the principal components associated with the larger eigenvalues contain more information and the principal components corresponding to smaller eigenvalues contain less information and then there are a lot of applications for pca relating variables together so if two variables get kind of clumped together kind of like hot dog bun sold and hot dog sold there's some underlying correlation there you can use it for clustering where you can transform your space from your original input space to like a new pca space and then you can do a clustering algorithm like k-means and then you can also do some outlier identification so you can plot all your points in your principal component space and just kind of visually inspect if there are any outliers all right so here's a fun example i guess at the outset i'm going to say i'm not a financial advisor i've never taken a finance class so in no way is this a recommendation of how you should invest your money this is just a fun example of what pca can do so here we're going to use pca to create an s p 500 index fund so an index fund is basically a set of investments that are meant to follow or track with a specific market the example codes on the github so i'll probably just fly through this i used the yahoo finance module to get real actual stock data so this is all real data this isn't made up and then i use pandas and numpy for all the number crunching so i write some code to input the ticker names from wikipedia and then graham guthrie had a nice medium post of how you can grab all these s p 500 names so i just stole some code from that post and made some edits okay then i pull s p 500 data for 2020 i drop nands get a pandas data frame of just close prices as opposed to all the other information that's available get a list of names ticker names of all the companies in the data frame so we have 253 rows and 499 columns so here i i guess the comments aren't updated so i apologize for that but here we're initializing pca with 10 components and then we'll ex we'll apply pca to our data set and we'll print the explained variants so you can see you know the first three components you're already at more than 90 of the explained variants uh if you just sum up the first three elements of that array there um okay and then we can create an index fund so there's countless ways you can do this i just arbitrarily took the weights defining the first three principal components i sum them together and then i only included the top 61 weights we can represent the uh overall portfolio of this index fund with a bar plot it's a natural way to do it so the y-axis is the relative weight you can also think of this as the number of dollars relative number of dollars you're gonna invest in each specific company and then the x-axis is just the individual ticker names okay and then we can see how our index fund compares to the actual s p 500 over 2020 and just you know visually approximately it doesn't do such a bad job there's some discrepancies uh along the way but everyone cares about percent return so if you would have just bought one share of every single stock in the s p 500 at the beginning of 2020 and then sold those uh same shares at the beginning of 2021 you would have made 20 return if you would have instead followed the investing strategy of this particular index fund derived from pca you would have made 25 so that was the video on principal component analysis i hope that cleared things up if you want to learn more about principal component analysis i have provided a link to my blog post on medium on the topic stay tuned for the next video where i'll be talking about a similar but different technique independent component analysis if you enjoyed this video be sure to like comment subscribe hit the bell share with your friends and family so they too can learn about principal component analysis thanks for watching you
MX7ymkYGiZ0,2020-12-21T00:24:45.000000,The Wavelet Transform | Introduction & Example Code,hey guys welcome back in this video we'll be talking about the wavelet transform hey guys welcome back uh this is the final video in the three-part video series on the 48 in wavelet transforms in this video we'll be talking about the wavelet transform which basically takes the ideas we learned in the fourier transform and extends it to a thing called wavelets so let's get right into it alright so in the last video we were talking about the 40 transform which was basically decomposing any signal into a basis of waves you can do the same basic idea but instead of waves you use something called wavelets so wavelet is simply a wave-like oscillation that's kind of localized in space or time whatever your x-axis is so here's an example we have the first derivative of the gaussian function or a bell curve and the equation is given here to the right of the plot so there are two basic properties of wavelets just like there were two basic properties of waves amplitude and frequency wavelets have two basic properties which are scale which is basically how stretched or squished our wavelength is and location since wavelets are localized and not extending from negative infinity to positive infinity like waves we need to know where the wavelet is located in space so here are some examples of kind of our example wavelet at different scales so if it has a smaller scale we're essentially squishing our wavelet if it has a larger scale we essentially stretch our wavelet and this is controlled by the parameter a in our first derivative of the gaussian function given above and then here's the same example with different locations so we can shift our wavelet a little to the left by changing this by tuning this b parameter or we can do this double whammy situation where we shift our way to the right and we squish it and then here's a reference if you want to learn more about wavelets okay so what's the wavelet transform so just like before with the fourier transform we decomposed our signal into a basis of waves we can decompose our signal using wavelets of various scales and locations so the basic idea is you get a wavelet you pick a scale and then you just slide it across your signal and then at every kind of time step or every location in space you multiply that wavelet by your signal and then you do this for one scale then you pick another scale and then you just keep going like that so here's a visualization of what it looks like and if you're familiar with convolutions that's exactly what we're doing here uh you're just convolving your signal and wavelets of different scales so why are wavelets useful if the fourier transform is so powerful as i was uh going on and on about in the first video uh why do we need the wavelet transform well one thing about the fourier transform is that it gives you global frequency information so uh sines and cosines are defined from negative infinity to positive infinity so it gives you a kind of global average of the frequency information in your signal and if you're interested in uh kind of oscillations that happen over a short time scale the 40 transform may obscure that information and that's exactly what the wavelet transform is good at extracting localized information on the second point the wavelet can not only extract the local spectral information but it can also extract temporal information at the same time which is really nice so you kind of have this trade-off of frequency information and time information but the wave of the transform is kind of like a happy medium of uh between those two and there's also a variety of wavelets to choose from so let's say the signal the sub signal inside of your audio signal that you're interested in has a characteristic shape then you can kind of choose your wavelet in a clever way such that it kind of matches the signal of interest the thing you're trying to extract from your signal and then this is just a lot of math i'm not going to spend too much time here but there are two basic kinds of wavelets there's the continuous wavelet transform and the discrete wavelet transform the major difference is the continuous wavelength transform you pick basically every possible scale and location to do your wavelet transform so you do scale 1 and 1.1 1.2 whatever your resolution is every possible scale uh while the discrete wavelet transform there's only a discrete number of wavelets uh discrete number of wavelet scales and locations that you use to do your transform and then here's just you know just some more general information wavelets create a complete set which means you can represent any function in terms of wavelets which is nice okay and then we're gonna try to bring everything together with a concrete example so this is something relevant to my research which is uh extracting our peak information which is basically the biggest peak you see in uh your favorite medical tv show in ecg which is basically the um electrical activity resulting from your heart beating so we can extract this characteristic our peak or this giant jump in the ecg signal using wavelets so we're using a specific kind of discrete wavelength transform called maximal overlap discrete wavelength transform in this example and you can find the this example code with all the plots and a nice pdf of it at the github link here okay so the first step is we do the wavelet transform here i picked a number of levels or a number of different scales to b6 and then we use the sim 4 wavelet for our wavelength transform so at the top we have our original signal you can kind of see the characteristic the signal of interest which is like this localized peak that's the rpeak we're interested in but the signal is really noisy so it's kind of hard to you know do a simple fine peaks function to extract the rpeaks but if we do this wavelet transform we can see at the first scale the smallest scale which will correspond to the highest frequency because our wavelength is very squished it just looks like a lot of noise so that's not really helpful we go to the next scale we can kind of see the rpeak signal coming out a little bit but still really noisy the next scale is a little bit better um the fourth scale is kind of the goldilocks we we see a really large oscillation near our peaks and then everything else is basically uh zero so that's promising and then we just keep going down and then you can see for the largest scale we have like this low frequency oscillation which makes sense big scale means lower frequency so the rest of the code is just making this plot here so now we can reconstruct our signal using the inverse maximal overlap a discrete wavelet transform uh so that's what that's what's happening here um you know just to run through the code really quickly we are only going to use that fourth scale which was oscillating a lot around our peaks and it was zero everywhere else so that was really promising so we're only going to use that information to reconstruct the signal and then matlab and a lot of programming languages there's probably equivalent functions in uh python or r uh that let you do the inverse maximal overlap discrete wavelet transform in one line so we're going to reconstruct it using only the fourth scale the two to the fourth scale and then we're going to use the sim 4 wavelet again and then the rest of it is just plotting this this image on the right here so the on the top we have the original signal so it's pretty noisy but you can still see where the rpgs are visually but then our reconstructed signal is a lot better behaved and you can see it's almost trivial um picking out the r peaks in the reconstructive system and then this last bit of code is just leveraging that so our reconstructed signal it's really easy to pick out the art peaks so we can just do a simple fine peaks function and then we can plot those peaks on our original signal and it does a reasonable job and so again you can find this code on the github if you want to take a closer look at it if you want to just completely steal it that's completely fine with me i kind of took a lot of this code from a matlab example that did a similar thing with a different data set so i encourage you to take a look at that so that was wavelets i hope you guys enjoyed these three videos found this video helpful like subscribe comment i'm always trying to get better improve my understanding learn grow all that fun stuff yeah thanks for watching you
rPUytg38b6Q,2020-12-04T01:10:36.000000,The Fast Fourier Transform | How does it (actually) work?,hey guys welcome back in this video we'll be talking about the fast fourier transform if you missed the last video it was a bit of a primer introduction on time series signals and the fourier transform so if you missed that definitely check that out and stay tuned for the next video which will kind of extend this idea to a thing called wavelets in the wavelet transform so let's get right into it alright so in the last video we were looking at the fourier transform so if you give it some function in terms of time or space it'll spit out a function in terms of frequency or wave number but most of the time we don't know a functional form of anything we're dealing with in the real world so that's where this idea of the discrete for you transform is very useful like we were looking on the previous video you can have an audio signal so you're playing your favorite record you record it with a microphone and then you can kind of digitize it so you have it in a form that your computer can understand and just how you can digitize your audio signal you can discretize your fourier transform so basically you convert your infinite integral to a sum over n elements so let's take a closer look at this expression so we have f f sub k is equal to the sum over zero to big n minus one uh with x sub n times e to the minus 2 pi i kn over big n so we can define this term let's call it big r sub k n and then we'll set that equal to e to the minus 2 pi i k n divided by big n so we can rewrite our sum in terms of these two elements and then just so we're all on the same page f sub k is an element of what i'll call a column vector x sub n is an element of a column vector and then r sub k n is an element of a matrix so because of the nice properties of matrix multiplication it turns out we can write our sum equivalently as matrix multiplication so that's nice discrete fourier transform is just matrix multiplication but so what how does this help us if we're coming to code this whole thing up we're still going to naively need two for loops to do this double sum this is what they call n squared time so that means the time complexity of this operation will take n square so if you have n elements you're going to have to do n squared computations so we converted our discrete fourier transform to matrix multiplication didn't seem entirely useful but let's just consider a concrete example so let's assume the case where big n equals big k so big n is the size of our column vector where x sub n was an element of and big k is the length of the column vector where f k is an element of that uh so we recall that big r sub k n is equal to e to the 2 pi i k n over big n and then we can just plug it in so we can plug in k and n which will correspond to the indices of our matrix and then big n is equal to four so we plug in those values and we get this matrix on the right here lots of ones lots of minus ones lots of eyes and minus sighs and just looking at it and this matrix is symmetric which means that if you were to swap the rows and columns it would be the same matrix as you started with so that's pretty nice and then this is a special element in the matrix so this element kind of defines the resolution of the discrete fourier transform so we can call it something special like k9 and then we can write our any fourier matrix in terms of this k naught and so this is for the four by four case that we have here and then we can write a few more examples so we have the eight by eight case which uh looks like that big scary matrix on the top there um where k naught is defined on the right hand side here uh we have the four by four case which is the same thing as before and then we have the two by two case and then again k naught is defined as e to the minus two pi i over big n and this extends to any fourier matrix of any size now where n where your fourier matrix has to be square and n gives you the dimensionality of that square matrix so this is the trick so we had the n squared there was a lot of symmetry in our four-year matrices we saw the symmetric matrix there were a lot of redundant terms a lot of ones a lot of minus ones a lot of eyes minus size uh in the 2x2 case the 4x4 case the 8x8 case so it would be really nice if we could leverage this redundancy and symmetry to make our algorithm a bit more efficient and that's exactly the idea behind the fast fourier transform so it's just a fast way of computing the discrete fourier transform and the basic idea is given by this expression so r sub n is our n by n for your matrix any fourier matrix you want that's n by n with the added caveat then n is a power of two that's an important point um and then it turns out you can write any for your matrix uh that's m by n by this expression here i can define these terms individually but i think it's better to just look at a concrete example so again let's return to our four by four case um so here we have i sub big n over two so big n is four so we're looking at i sub two and that's just our two by two identity matrix then we have d sub two which will be the first two diagonal elements of the four by four fourier matrix so we have uh one and minus i respectively and it's a d is defined to be a diagonal matrix so we only keep the diagonal terms from the fourier the four by four fourier matrix and we set everything else to zero and then finally we have the 2x240 matrix which i flashed on the previous slide but it turns out to be this and then the last thing is we have this permutation matrix which is defined by this expression on the right here so basically you if you have a permutation matrix and you multiply it some vector by it it'll just reshuffle the elements in your vector such that the even indexed elements are in the top half and the odd indexed elements on the bottom half okay so it's just plug and chug so we're just plugging in the two by two identity matrix the two by two diagonal matrix and these slots here we plug in the 2x240 matrix here and here and then we have our 4x4 permutation matrix so it's just we're just plugging into this expression um and it may not be immediately obvious but one thing that is promising is that these matrix have a lot of zeros in them which is good we like zeros because that means there's no multiplication to do so on the left hand side again the time complexity is n squared uh but it turns out on the right hand side we don't have n squared time complexity uh so let's take a closer look this permutation matrix is basically free so if you represent your data in a smart way in a nice data structure and use a good algorithm you can basically do this in one step for this first matrix half of it is just an identity matrix basically and then the other half is two diagonal matrices stacked on top of each other so that means half the elements so in this case 40 elements are zero so that means there are only four non-trivial terms we have to worry about so that's where this time complexity of order n comes from and then for this middle matrix half the elements are zero so there's only um two n or eight elements that we have to worry about uh so it turns out as n gets larger and larger this gives you uh you can devise an algorithm that has n log n time complexity uh so the fast fourier transform is just efficient matrix multiplication and so you know it might not be immediately obvious why this is order n log n but i want to give you some uh intuition of what's going on here so let's look at another example we have the 16 by 16 for your matrix which is given we're just plugging into the expression that i showed on the previous slide but there's nothing stopping us from playing the same game for the eight by eight fourier matrix in this middle matrix right here so we can plug that in and then we get a whole bunch more zeros again we go from uh n squared uh time complexity to order n time complexity plus order two n time complexity plus constant time complexity and then we have a 4x440 matrix here so we plug that in and we pick up even more zeros so this is kind of the intuition of what's going on we just recursively apply this matrix multiplication we rewrite our kind of n by n for your matrix in terms of the n over 2 by n over 2 for your matrix and some other terms and we get the n log n time complexity let's try to bring everything together with a concrete example uh so i wrote this example in matlab the code is available at the github page linked here and in the description so we're going to look at the spectrum of an audio signal so in this case i'm just playing the low e string on electric guitar so we can see what that looks like the first step is we read in our audio file then in one line of code in matlab we can apply the fast fourier transform uh here we're just defining some terms we're getting the length of the audio signal we're converting our frequency indices to actual frequency values uh we're computing the two-sided and then from the two-sided the one-sided power spectrum in these lines here and then we just plot everything so at the top we have our audio signal and then the bottom we have our fourier transform so you see we have these kind of discrete peaks at different frequencies so the first one is e2 the low e string 82 hertz so that's what we expected but the cool thing is we're getting exactly or approximately the harmonic series so you're just getting integer multiples of the fundamental so the fundamental frequency in this case is a 282 hertz and then we're just getting integer multiples of it so we start with 82 164 247 330 415 and so on and um you know as someone that is into both physics and music it's really nice when those two fields come together and if you keep going down the harmonic series it turns out you approximately get a major scale which is pretty cool for the physics and slash music personalities that exist inside my brain so that was the fast for you transform if you found this video helpful like subscribe comment i'd love to hear your feedback stay tuned for the next video which will extend this idea to a thing called wavelets in the wavelet transform thanks for watching
mj86XmfOniY,2020-11-15T20:41:53.000000,"Time Series, Signals, & the Fourier Transform | Introduction",yeah hey guys welcome my name is sean i'm a physics phd student at the university of texas dallas and in this video i'll be talking about the fourier transform i know there's tons maybe not even tons thousands millions of videos about fourier transforms on youtube but i just wanted to post a quick digestible video on the topic mainly to serve as a primer for my next two videos which will be on the fast fourier transform and something called the wavelet transform by no means am i an expert in any of these topics i just use them sometimes in my research in the future i'll be posting more videos relating to physics research data science machine learning so stay tuned for those again i'm not an expert in these topics so if you hear me say something stupid make a mistake say something wrong i invite you to please leave a comment let me know i'm gonna do my best to read all the comments and reply to them if you find this video helpful and you want to see more like in the future uh subscribe like share comment all that good stuff all right without any further ado let's get into the video so everyone's undoubtedly familiar with time series even even if you haven't heard the term so time series is simply a set of values indexed by time so you flip on your favorite news network you they're typically talking about how stocks are amazing or stocks are terrible or whatever and typically they're showing uh like a stock price or here we have an index price uh plotted over time so we have time on the x-axis and we have the price on the y-axis um or even more uh relevant to our daily lives seven day forecast so here we have uh the temperature plotted over time so these are time series it's just a set of values indexed over time so a signal is a specific kind of time series the only difference here is a signal typically represents a physical event uh so for example you could be listening to your favorite record that could be represented as an audio signal so you have like this time oscillating amplitude uh or so a lot of times it's going to be a voltage or you could have a biometric signal so this is something i deal with in my research uh here we have the surface body temperature plotted over time so time series signals very common everyone's probably really familiar with them this is just terminology okay so another very common thing and is very fundamental to understanding the fourier transform is waves so a wave is simply an any oscillating quantity around an equilibrium so there are two basic properties of waves amplitude which is the magnitude of the quantity and frequency which characterizes the oscillation so here we have a sine wave so the amplitude is characterized by the y-axis and the frequency is characterized by the number of peaks in a given time interval so now getting to the bread and butter the main topic main event of the video the fourier transform so before we transform intuitively is a decomposition of a signal into sines and cosines so here we have this purple signal f x plotted over time if we do the fourier transform it turns out we can decompose this into two simple sine waves of different frequencies and the way we do this is via the 40 transform which is given by this expression so we have f of k is equal to the infinite integral of f x which is our signal our time series or function whatever you want to plug in there times e to the minus 2 pi i k x dx um and if you're confused then i've been talking about sines and cosines and you don't see a sine or cosine in this expression i just want to put here as a reminder we have euler's formula which relates e to the x to cosine and sine so another way to think of the fourier transform is in simple terms is if we have a signal plotted over time uh a 40 transform simply changes our x-axis from time to frequency so here we have the same signal uh it's made up of those two simple sine waves as we saw in the previous slide so if you were to compute the fourier transform and plot the resulting what they call power spectrum uh resulting from the fourier transform you get something that looks like this bottom plot so we have two spikes at one hert and two hertz so this is just another way of thinking of the fourier transform so in the previous slide it was expressed in terms of x and k so x is usually like a spatial coordinate and k is typically like a what they call a wave number which as has units of inverse space and here we have things in terms of time t and angular frequency omega which is defined in this way two pi times the frequency the code to generate these plots is available on the github so check that out so here we're kind of getting into the practical applications of the 4u transform spectral analysis is just one of many applications of before you transform uh so spectral analysis is uh simply the examination of a signal so again it could be your audio signal your body temperature over time based on its constituent frequency energies so this is applications in uh you can look at the spectrum of light so we're all familiar with perhaps the pink floyd album dark side of the moon white light comes in rainbow comes out because white light is made up of basic constituent frequencies of different colors you can extend this idea to different light sources like the sun or led bulbs in your house or fluorescent bulbs in your house or if you are if you're really struggling incandescent bulbs also spectral analysis is very relevant to audio production i'm a musician this is something i do a lot you can kind of turn down or turn up different frequency values or ranges of your audio signal and this is relevant to make your music or any kind of audio speech given a lecture or something you can make the audio sound a lot better doing this kind of audio production and then finally something i do and deal with in my research is eeg which is measuring the surface brain activity so you can understand eeg signals you can look at them through the lens of their frequency bands some very widely used uh frequency ranges of eeg are the delta theta alpha and beta band and you may have heard of like alpha waves or beta waves in popular culture so that's what they're referencing so that was before you transform stay tuned for the next video on the fast 40 transform which makes this idea more practical again like and subscribe if you found this video helpful please leave a comment i'd love to hear your feedback i'm a phd student like i said earlier so i'm learning posting these videos as part of the learning process see you next time see you next time until next time catch you later you
Gwz4zXPeP_Q,2020-11-12T22:58:00.000000,biometricDahboard3 DEMO,n/a
lciC1s4FO0g,2020-09-23T13:02:57.000000,biometricDashboard2 DEMO,n/a
