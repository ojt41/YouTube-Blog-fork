{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "814fe156-7c2b-4ff5-a997-eaa76a695e49",
   "metadata": {},
   "source": [
    "# Fine-tune CLIP on Title-Thumbnail Pairs\n",
    "\n",
    "Code authored by: Shaw Talebi\n",
    "\n",
    "[Video link](https://youtu.be/W4s6b2ZM6kI) | [Blog link](https://medium.com/towards-data-science/fine-tuning-multimodal-embedding-models-bf007b1c5da5) <br>\n",
    "[Dataset](https://huggingface.co/datasets/shawhin/yt-title-thumbnail-pairs) | [Fine-tuned Model](https://huggingface.co/shawhin/clip-title-thumbnail-embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c76683-915f-47ad-9da6-c3dadcc1fda3",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "174053fb-7724-47b1-b0b7-01c12e356b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "from sentence_transformers import (\n",
    "    SentenceTransformer,\n",
    "    SentenceTransformerTrainer,\n",
    "    SentenceTransformerTrainingArguments,\n",
    ")\n",
    "from sentence_transformers.losses import MultipleNegativesRankingLoss\n",
    "from sentence_transformers.evaluation import TripletEvaluator, SentenceEvaluator\n",
    "\n",
    "from typing import List, Dict\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe437b80-8367-43a1-b2b0-73b30069b201",
   "metadata": {},
   "source": [
    "### import model and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cdba8c1-0e18-4844-9349-e208dde5581b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"sentence-transformers/clip-ViT-L-14\"\n",
    "model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c583a083-d5e3-4d24-99cf-85dc5e6ba496",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"shawhin/yt-title-thumbnail-pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b461de0-48ce-467b-aa3f-1660806349e4",
   "metadata": {},
   "source": [
    "### freeze model params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3b8a3ab-fef2-4790-8df8-d554807a2ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick specific layers to train (note: you can add more layers to this list)\n",
    "trainable_layers_list = ['projection']\n",
    "\n",
    "# Apply freezing configuration\n",
    "for name, param in model.named_parameters():\n",
    "    # freeze all params\n",
    "    param.requires_grad = False\n",
    "\n",
    "    # unfreeze layers in trainable_layers_list\n",
    "    if any(layer in name for layer in trainable_layers_list):\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8df4b51-3a26-408e-b0ec-2724462f7eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable: 0.model.visual_projection.weight\n",
      "Trainable: 0.model.text_projection.weight\n"
     ]
    }
   ],
   "source": [
    "# Verify trainable parameters\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Trainable: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "090b77c5-b965-472e-82b9-4f6c9fa9ee4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 427,616,513\n",
      "Trainable parameters: 1,376,256\n",
      "Percentage of trainable parameters: 0.32%\n"
     ]
    }
   ],
   "source": [
    "# Count total and trainable parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Percentage of trainable parameters: {100 * trainable_params / total_params:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66aacde-b393-4a4d-8995-c864d3b56452",
   "metadata": {},
   "source": [
    "### preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "999f8992-dada-4030-ba2d-5820616c656b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process positive pairs\n",
    "def preprocess(batch):\n",
    "    \"\"\"\n",
    "        Preprocessing data without augmentations for test set\n",
    "    \"\"\"\n",
    "    # get images from urls\n",
    "    image_list = [Image.open(requests.get(url, stream=True).raw) for url in batch[\"thumbnail_url\"]]\n",
    "\n",
    "    # return columns with standard names\n",
    "    return {\n",
    "        \"anchor\": image_list,       \n",
    "        \"positive\": batch[\"title\"],  \n",
    "        \"negative\": batch[\"title_neg\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5057366-9792-48f4-967b-a04840cd4fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove columns not relevant to training\n",
    "columns_to_remove = [col for col in dataset['train'].column_names if col not in ['anchor', 'positive', 'negative']]\n",
    "# applu transformations\n",
    "dataset = dataset.map(preprocess, batched=True, remove_columns=columns_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd98291c-cd08-499e-b7ef-7f401048495d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['anchor', 'positive', 'negative'],\n",
       "        num_rows: 53\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['anchor', 'positive', 'negative'],\n",
       "        num_rows: 11\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['anchor', 'positive', 'negative'],\n",
       "        num_rows: 12\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1919ac-db6c-417e-8912-db8deff0f758",
   "metadata": {},
   "source": [
    "### eval pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "621a134e-f623-426c-9edf-3305af59c70e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def create_triplet_evaluator(set_name):\n",
    "    \"\"\"\n",
    "        Create triplet evaluator for \"train\", \"valid\", or \"test\" split\n",
    "    \"\"\"\n",
    "\n",
    "    return TripletEvaluator(\n",
    "        anchors=dataset[f\"{set_name}\"][\"anchor\"],\n",
    "        positives=dataset[f\"{set_name}\"][\"positive\"],\n",
    "        negatives=dataset[f\"{set_name}\"][\"negative\"],\n",
    "        name=f\"yt-title-thumbnail-{set_name}\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7eb7ec04-890e-43ae-902f-d83c0c16cd16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: {'yt-title-thumbnail-train_cosine_accuracy': np.float64(0.9622641509433962)}\n",
      "Valid: {'yt-title-thumbnail-valid_cosine_accuracy': np.float64(1.0)}\n"
     ]
    }
   ],
   "source": [
    "evaluator_train = create_triplet_evaluator(\"train\")\n",
    "evaluator_valid = create_triplet_evaluator(\"valid\")\n",
    "\n",
    "print(\"Train:\", evaluator_train(model))\n",
    "print(\"Valid:\", evaluator_valid(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb73c7b7-9b82-46d2-930c-29a7058074e1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class ImageTextRetrievalEvaluator(SentenceEvaluator):\n",
    "    def __init__(\n",
    "        self,\n",
    "        images: List,\n",
    "        texts: List[str],\n",
    "        name: str = '',\n",
    "        k: int = 1,\n",
    "        batch_size: int = 32,\n",
    "        show_progress_bar: bool = False\n",
    "    ):\n",
    "        self.images = images\n",
    "        self.texts = texts\n",
    "        self.name = name\n",
    "        self.k = k\n",
    "        self.batch_size = batch_size\n",
    "        self.show_progress_bar = show_progress_bar\n",
    "\n",
    "    def __call__(self,\n",
    "        model: SentenceTransformer,\n",
    "        output_path: str = None,\n",
    "        epoch: int = -1,\n",
    "        steps: int = -1) -> Dict[str, float]:\n",
    "        \n",
    "        # Get embeddings for all images\n",
    "        img_embeddings = model.encode(\n",
    "            self.images,\n",
    "            batch_size=self.batch_size,\n",
    "            show_progress_bar=self.show_progress_bar,\n",
    "            convert_to_tensor=True\n",
    "        )\n",
    "        \n",
    "        # Get embeddings for all texts\n",
    "        text_embeddings = model.encode(\n",
    "            self.texts,\n",
    "            batch_size=self.batch_size,\n",
    "            show_progress_bar=self.show_progress_bar,\n",
    "            convert_to_tensor=True\n",
    "        )\n",
    "        \n",
    "        # Compute similarity matrix\n",
    "        cos_scores = torch.nn.functional.cosine_similarity(\n",
    "            img_embeddings.unsqueeze(1),\n",
    "            text_embeddings.unsqueeze(0),\n",
    "            dim=2\n",
    "        )\n",
    "        \n",
    "        # Get indices of top k predictions for each image\n",
    "        _, top_indices = torch.topk(cos_scores, k=self.k, dim=1)\n",
    "        \n",
    "        # Calculate Recall@k (correct if ground truth index is in top k predictions)\n",
    "        correct = sum(i in top_indices[i].tolist() for i in range(len(self.images)))\n",
    "        recall_at_k = correct / len(self.images)\n",
    "\n",
    "        return {f'{self.name}_Recall@{self.k}': recall_at_k}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae771da5-0b88-433c-9694-5423dca7a058",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def create_recall_evaluator(set_name, k=1):\n",
    "    \"\"\"\n",
    "        Create triplet evaluator for \"train\", \"valid\", or \"test\" split\n",
    "    \"\"\"\n",
    "\n",
    "    return ImageTextRetrievalEvaluator(\n",
    "        images=dataset[f\"{set_name}\"][\"anchor\"],\n",
    "        texts=dataset[f\"{set_name}\"][\"positive\"],\n",
    "        name=f\"yt-title-thumbnail-{set_name}\",\n",
    "        k=k\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f67f59f9-3a23-4e4a-8def-1279431d93c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: {'yt-title-thumbnail-train_Recall@1': 0.660377358490566}\n",
      "Valid: {'yt-title-thumbnail-valid_Recall@1': 0.6363636363636364}\n"
     ]
    }
   ],
   "source": [
    "# Create new evaluator with Recall@k\n",
    "evaluator_recall_train = create_recall_evaluator(\"train\", k=1)\n",
    "evaluator_recall_valid = create_recall_evaluator(\"valid\", k=1)\n",
    "\n",
    "print(\"Train:\", evaluator_recall_train(model))\n",
    "print(\"Valid:\", evaluator_recall_valid(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4534a4f7-524a-466a-9995-0df5eb72ffc4",
   "metadata": {},
   "source": [
    "### define training args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c59ae6af-2b2e-421b-aeea-79c8bdc3cb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss (note: loss expects columns to be ordered as anchor-positive-negative)\n",
    "loss = MultipleNegativesRankingLoss(model)\n",
    "\n",
    "# hyperparameters\n",
    "num_epochs = 2\n",
    "batch_size = 16\n",
    "lr = 1e-4\n",
    "finetuned_model_name = \"clip-title-thumbnail-embeddings\"\n",
    "\n",
    "train_args = SentenceTransformerTrainingArguments(\n",
    "    output_dir=f\"models/{finetuned_model_name}\",\n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    learning_rate=lr,\n",
    "    # Evaluation settings\n",
    "    eval_strategy=\"epoch\",\n",
    "    eval_steps=1,\n",
    "    logging_steps=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e54aeea-f468-47c7-baa9-abf61ef0ebc9",
   "metadata": {},
   "source": [
    "### fine-tune model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2861e593-beb3-45cd-861f-53ff0e3eae4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8/8 00:05, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Yt-title-thumbnail-train Recall@1</th>\n",
       "      <th>Yt-title-thumbnail-valid Recall@1</th>\n",
       "      <th>Sequential Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.750500</td>\n",
       "      <td>1.491625</td>\n",
       "      <td>0.830189</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.909091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.331500</td>\n",
       "      <td>1.499041</td>\n",
       "      <td>0.849057</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.909091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "636b4c3615ee4f1a9ef5af3a8bf588bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.75 s, sys: 822 ms, total: 2.57 s\n",
      "Wall time: 6.66 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8, training_loss=1.3635553307831287, metrics={'train_runtime': 6.3182, 'train_samples_per_second': 16.777, 'train_steps_per_second': 1.266, 'total_flos': 0.0, 'train_loss': 1.3635553307831287, 'epoch': 2.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"valid\"],\n",
    "    loss=loss,\n",
    "    evaluator=[evaluator_recall_train, evaluator_recall_valid],\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cac741a-69f9-4de4-a816-dba229fbb5b0",
   "metadata": {},
   "source": [
    "### evaluate fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a957fc9e-0da5-4c71-8ac4-c5f330c3d817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: {'yt-title-thumbnail-train_cosine_accuracy': np.float64(1.0)}\n",
      "Valid: {'yt-title-thumbnail-valid_cosine_accuracy': np.float64(1.0)}\n",
      "Test: {'yt-title-thumbnail-valid_cosine_accuracy': np.float64(1.0)}\n"
     ]
    }
   ],
   "source": [
    "evaluator_test = create_triplet_evaluator(\"test\")\n",
    "\n",
    "print(\"Train:\", evaluator_train(model))\n",
    "print(\"Valid:\", evaluator_valid(model))\n",
    "print(\"Test:\", evaluator_valid(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dae887ae-3f18-4456-b0ed-4b5587cbc461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: {'yt-title-thumbnail-train_Recall@1': 0.8490566037735849}\n",
      "Valid: {'yt-title-thumbnail-valid_Recall@1': 0.9090909090909091}\n",
      "Test: {'yt-title-thumbnail-test_Recall@1': 0.75}\n"
     ]
    }
   ],
   "source": [
    "evaluator_recall_test = create_recall_evaluator(\"test\")\n",
    "\n",
    "print(\"Train:\", evaluator_recall_train(model))\n",
    "print(\"Valid:\", evaluator_recall_valid(model))\n",
    "print(\"Test:\", evaluator_recall_test(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85baa873-c0da-43bb-ab95-0858e1850e72",
   "metadata": {},
   "source": [
    "### push model to hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b20ffa6b-d608-4774-9a92-4144cac7a167",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "170da5bf2e1b4fab93af9acb8835b211",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.71G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/shawhin/clip-title-thumbnail-embeddings/commit/05dcc90819309f6823025915ff0a58d4e2bdd95d'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(f\"shawhin/{finetuned_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb1de2e-5fee-431f-92f6-64eb96ed381e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
