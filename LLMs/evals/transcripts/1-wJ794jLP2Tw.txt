this is the sixth video in the larger series on full stack data science in this video I'm going to be talking about automating data pipelines I'll start with a highle overview of how we can do that and then dive into a concrete example using GitHub actions which gives us a free way to automate workflows in our GitHub repos and if you're new here welcome I'm sha I make videos about data science and Entrepreneurship and if you enjoy this content please consider subscribing that's great no cost way you can support me in all the videos that I make so I'll start with a story about a friend of mine back in grad school so it seemed about every Friday to cope with our existence of being physics grad students me and many other grad students would find themselves at a bar close to campus and there was one grad student in particular who would show up after doing research for several hours and he would be drinking a beer and he'd say something like technically I'm working right now because my code is running and of course we would always laugh about it but this is a sentiment that I find a lot of data scientists and other developers share which is this sense of satisfaction when some piece of software that they wrote is off and running all on its own while they are off doing something else that they enjoy doing like having a beer with fellow grad students so this is at least one motivation toward automating data pipelines and other aspects of the machine learning pipeline here I'm going to talk at a high level about two ways we can automate data pipelines the first way is via an orchestration tool which is something I mentioned in the previous video on data Engineering in this series this includes things like airflow dagster Mage and many more the second way is what I call python plus triggers this is Python scripts that run given particular Criterion such as it's a particular time of the day or a file appears in a directory let's dive into each of these approaches the first way an orchestration tool so I'll start with airflow because from the dozens of interviews I've had with data engineers and ml Engineers it seems that this has emerged as a standard tool for many building data and machine learning pipelines one of the biggest benefits of airfow is that it can handle very complicated workflows that include hundreds and even thousands of tasks this is a major reason why this has become an industry standard among data Engineers managing and Enterprise scale data pipelines one downside of airflow as someone who was trying to learn it specifically for this series and this project that I've been working on is that it can be pretty complicated to set up and maintain these workflows and of course that comes with a steep learning curve these challenges of getting set up with airflow have created a market for airflow rappers such as prefect dagster Mage and Astro the upside of these wrappers is that you can tap into the power of airflow with potentially simpler setup in maintenance however one downside of these airflow wrappers is that this ease of use is often coupled with managed or paid services for these rappers however to my knowledge most of these have open-source versions if you want to go the self-managed route regardless of what orchestration tool you want to use for many machine learning applications where the there's a relatively simple data pipeline these tools may be Overkill and may over complicate your machine learning system which motivates the second approach to automating data pipelines using python plus triggers you can say that this is the old fashion way because before we had these orchestration tools if you wanted to build these data and machine learning workflows you'd have to build them from scratch so to make this a bit more concrete let's look at a specific example say we wanted to implement a very simple ETL pipeline consisting of one data source and one target database what this might look like is we pull data from a web Source we'll run an extraction process via a python file call it extract. py we'll transform the data in some way in a python script called transform. piy then we'll load it into our database using another script called load. piy so this is a simple ETL pipeline which is something I discussed in the previous video video in this series on data engineering given these three Python scripts there's nothing stopping us from creating yet another script that consolidates all these steps we can call that script ET L.P so now instead of running three scripts consecutively we can just run one script although this has streamlined the process this is still not automated because we still have to manually go to the command line and type python space L.P this is where the idea of a trigger comes in so let's say we wanted to run this ETL pipeline every single day one thing we could do is run a KRON job KRON is a command line tool which allows you to schedule the execution of some process so let's say we wanted to run our ET L.P file everyday at midnight we would type something like this into the command line and then that would get saved in our system as a Cron job and every day at midnight This would run but of course if we do this this would be running the Cron job on our local machine which may not be something that we want to do some Alternatives could be doing this process on a server that we manage or spinning up a cloud resource that runs this process but both of these options come with a bit of overhead in maintenance for setting up these compute resources so it would be great if we could set up this trigger to execute our ETL pipeline without the extra effort of setting up these compute resources this is where GitHub actions are super helpful gith GitHub actions is github's builtin cicd platform cicd stands for continuous integration and continuous delivery the typical schematic of cicd looks something like this where I suppose this is supposed to be some kind of infinite Loop of integrating new updates into your software system and delivering those updates in real time hearing this you might be thinking sha what does continually deploying code have to do with data pipelines well although data may not play a role in traditional software development when it comes to building machine Learning Systems data play a central role in the software development put another way when it comes to machine learning we use data to write programs to write algorithms which is manifested as a machine learning model so if we're talking about continuously integrating and delivering a machine learning application that will require us to continually integrate and up update the data that we feed into that system two key benefits of GitHub actions is that firstly the compute to run these workflows is provided for free for public repositories this is great for poor developers like me who just want to put out example code like for this video or for those who are building proof of Concepts or building projects for their portfolios and of course there are paid versions for Enterprises and businesses and the second thing is that we don't have to worry about setting up compute environments whether that's an on premise server or a cloud resource all the setup happens via writing a. yaml file with that highle overview let's walk through a concrete example of automating an ETL pipeline to turn YouTube video transcripts into text embeddings if you're unfamiliar with text embeddings I have a whole video where I talk about that but understanding what they are is not necessary for this example here the steps we're going to walk through are one we're going to create our ETL python script two we're going to create a new GitHub repo three we're going to write our workflow via the yaml file four we're going to add repo secrets to our GitHub repo this will be necessary to allow the GitHub action to automatically push code to the repo and also to make API calls to the YouTube API without exposing my secret API key and then finally we'll just push and commit all our code to the repo let's start with the first one creating our ETL python script and so here I have a fresh Visual Studio code I created a new folder and now I'm going to create a new file I'm going to call it data pipeline. pi and I've already pre-written the code here so I'll just paste it over and explain it one by one the first thing I'm going to do is some imports the first line here is we're going to import a set of functions from another python script called functions. piy which we'll write in a second and then we'll import the time module and the date time module the reason we want time and date time is that it'll allow us to print when this python script is run and how long each of the following steps took to run toward that end first I'm going to print the time that the pipeline is starting to run we can do that using the datetime module next I'll start importing the different steps of our data pipeline so the first step is the extraction the first thing it's going to do is extract the video IDs of every single video on my YouTube channel that whole process is baked into this get video IDs function which is going to be in this functions. piy script that will create in a second this is something I walked through in the previous video of the series on data engineering some other fanciness that's happening here is that I'm just capturing the time just before and just after this step is is run so I can print how long it took to run this function and this is helpful for debugging purposes and observability once we have the video IDs extracted we can use a python library to extract the transcript of each video given its video ID using similar process here where this whole get video transcripts process is abstracted as this function and we're capturing how long it took to run that step next I have this transform data function which is just doing some data cleaning so ensuring that the data have the proper types and handling any special character strings and then finally step four we're going to take the titles and the transcripts from all the YouTube videos and make the text embeddings with these files in place we'll go ahead and create another file and this will be the functions do Pi file and here we will put all the different steps that we just put into our data pipeline there's a lot here actually more functions than are used in the data pipelines script I'm not going to walk through this because I've talked about it in previous videos but of course this code is freely available on the GitHub so you can check that out if you're interested one thing I'll point out is that at each of these key steps in the data pipeline we're not outputting any data structures or data frames as intermediate steps the data are actually being saved to this data directory this is actually something we'll have to create so we'll create a folder called Data so that the files have somewhere to go one last thing you'll notice is that we're importing a ton of libraries here so I'll go ahead and create another file called requirements. text where we can put all the different requirements for this data pipe line polers is what I'm using to handle all the data frames YouTube transcript API is the python library that allows us to pull the YouTube transcripts given a video's video ID I'm using the sentence Transformers library to create the embeddings and then I'm using the requests library to make API calls to the YouTube API all right now that we've written all the code for our ETL pipeline let's create a fresh GitHub repo we can push the code to we can create a GitHub repo from the command line or from the web interface I'll do it from the web to do that you just go to your repositories Tab and you click new it will give it a name I'll call it data pipeline demo and I'll just say demo data pipeline via GitHub actions we'll keep it as a public repo one benefit of doing it as a public repo is that GitHub won't charge you for the compute costs of the GitHub actions I'll add a readme file I'll also add a giit ignore using the python template and then I'll choose a license I'll just do Apache 2.0 all right so we now have our repo and then what we'll want to do is we'll want to clone our repo open up our terminal to whatever folder we want to store this repo in and then we'll do get clone and then we'll download this newly created repo we can see we have the license and the read me now that we have our repo we can go ahead and write our yaml file which will Define the work flow that will automate the execution of our data pipeline okay so the next thing we want to do is add all the code we just wrote to this new repo we created so the code we wrote earlier I stored it in this data pipeline demo temp folder so what I'm going to do is just copy and paste that over we go back here hit LS we see that all those files are here and then what we want to do is create a new folder we'll call it GitHub and then in that GitHub folders we'll create a new folder called workflows the reason we do this is that GitHub will look in this GitHub workflows subdirectory for all the workflows that we want to run as GitHub actions with that directory created we can open up our GitHub repo locally so it's called Data pipeline demo we'll open that up and we can see all the code is here and our GitHub workflows folders here so what we wanted do is create a new file in this workflows folder we'll call it data pipeline. yml and here is where we'll Define our GitHub action there are a few key elements of a GitHub action the first is its name which we Define like this and if you're not familiar with yaml files they're essentially python dictionaries so they consist of these key value pairs that can additionally be nested so the simplest of these when it comes to GitHub actions is this one we wrote here which is the name of the workflow which I'm just calling data pipeline workflow the next element of the workflow is the trigger so this will Define when this workflow is run that's specified by this on syntax and from here we have a few different options we can make it so that this workflow runs anytime new code is pushed to the repository we just do that by writing this another thing we can do is have the option to manually trigger this workflow using this option which is workflow dispatch so we'll see later that setting this option a button will appear under our GitHub actions tab which will allow us to manually run this workflow but the one that we probably care most about is this option which allows us to schedule our workflow as a Cron job which is what we saw in an earlier slide to make this Aon job we simply type cron here and then we specify the arguments for the Cron job what I want is for this workflow to run every night at 12:35 a.m. so the Syntax for that looks like this and if you're not familiar with the syntax of running a Cron job there's a great website called cron tab. Guru it has a guide for scheduling Cron job so this is what we wrote in our emo file it's saying at 0035 it's going to run and it actually tells you the next time that this job would run alternatively we could do all all asteris which means this is going to run every minute however when it comes to GitHub actions 5 minutes is the fastest that it can run so you would write every fifth minute like this but I tried this and even if you specify to run every 5 minutes it still won't run that fast it'll run closer to every 15 minutes so there are some limitations on how quickly you can run a workflow using GitHub actions but here we don't want to do anything crazy and just running it once a day is fine because I'm posting videos every week so running this workflow every day is more than sufficient now that we have the name of our workflow and we' specified when it's going to run the next thing we want to do is Define the workflow itself workflows consist of jobs which have names so the names of the jobs are specified like this and then what we can do is specify what system this specific job will run on so we'll say auntu latest so this will just run on the latest version of Ubuntu which is a Linux operating system and then jobs will consist of steps which we can specify like this here we're going to have a handful of steps to run this whole workflow the first step we'll call it checkout repo content and we can actually make use of pre-built GitHub actions provided by GitHub this one is called checkout version 4 what this step of the workflow is doing is pulling all the code from our GitHub repo and if you're curious about this specific action we can Google it and then there's a repo on GitHub that has a lot more information about this action you can check this out if you like it's at github.com actions checkout then I'll do one more thing here and I'll add this with thing and I'm going to specify a token what I'm doing here is giving this workflow access to one of the repository Secrets which will give it read and write access since it's a public repo it doesn't need any special permissions to pull the code onto this auntu instance at spun up but later in the workflow we're going to push code to this repo which will require special access and so that's the reason I'm adding this token to give the action the proper permissions and we'll create this token in a sec and I'll show you how to make it a repository Secret okay so that's step one of our job all we did was create a fresh auntu instance and then pull the code from our repo next thing we're going to do is set up python I'll call this step of the workflow set up python I'll use another another pre-built action called setup python version 5 and similarly if you're curious about this you can just Ty it into Google and then you can pull up the repository for setup Python and it'll show you the basic use usage and have more information on that but it does exactly what it sounds like it does then we can add this with thing and specify the python version here I'll use 3.9 I'll also add this option which will cach the libraries that we install with Pip so it doesn't have to install these libraries from scratch every single time it runs the workflow it can just install it once and then it can reuse those libraries from Cache that second step we set up python now we'll do another step and we'll install the dependencies here we're going to use a different command so we'll use this run thing this is essentially like running a command from the command line so we'll just do pip install requirements. text this is just as if we opened up our terminal and typed this in the command line and the reason we can do that is because we've just installed python on the machine so now we can run pip on it with the libraries and installed we can now run the data pipeline so I'll Define another step call it run data pipeline I'll need to do one thing here which is import a environment variable called YouTube API key and this will be another repository secret that will Define later and what this is is my YouTube API key which is needed to run this function here the get video IDs so once we've done that we can now run our data pipeline so we do that like this so we'll use the Run command again and we just type python data pipeline. piy just like we were typing this into the command line now we've run this python script on this machine that just got spun up and then what we can do next is see if any changes were made to the git repository after running the data pipeline give it an ID and then we'll run another command I'll use this vertical bar syntax which will allow me to import mult multiple lines of commands but we can go through this one by one What's Happening Here is that first we're configuring the GitHub account then we're adding all the local changes what this line is doing is that it's checking the difference between the staged changes and the last commit and this quiet option will set the exit status of the command to zero if there are no changes and we'll set it to one if there are changes so if the exit status is one we'll create a new variable called changes and that'll be equal to true and then we'll store this in the GitHub environment what that allows us to do is do one last step we'll call it commit and push if changes what we can do is basically have an if statement so if changes so this environment variable that we just created equals true we'll run this command so again we'll use this vertical line to do a multi-line command and we'll commit the staged changes and then we'll push it to the repository this is where we need the special permissions from the personal access token that we defined earlier and then one last thing we need to do is add a file to the data folder because since it's empty GitHub won't actually push any empty folders to the repo so we should just have a file in here and then we can just say something like data go here and with that we've created our workflow in this data pipeline. yaml file we have our data pipeline here we have all the functions that make it up here we have our requirements file with all the libraries that are needed to run our data Pipeline and we have a data folder in which we can store all the data produced from our data pipeline in this example we can get away with storing the data on GitHub because it's super small it's not going to be more than 100 megabytes or so but if you're working with data sets much bigger than that talking about gigabytes or even more in that case you probably want to store that in a remote data store and those are changes you would just make in your data pipeline itself for example instead of reading and writing to a local directory you would read and write to an S3 bucket or to Google Drive or to a database so that'll just depend on your specific use case before we can push all these local changes to our GitHub repository we need to create two secret variables that are available to to the GitHub actions in order for this pipeline to run successfully and so to create a repository secret we'll go over to settings we'll scroll down to secrets and variables and we'll click actions we'll see a screen like this and we'll see this repository Secrets section this will allow us to create repository secrets that will be accessible to our GitHub actions the first one we want to create was this personal access token but we need to actually create it personal access token for this and so in order to do that I'll click on my profile here scroll down to settings open that in new tab and then I'll scroll down to developer settings click on personal access tokens and then I'll click on tokens classic and you can see that I've already created some personal access tokens but what I'll do is create a new one it asks for a note so I'll call this data pipeline demo P expiration we'll just leave at 30 days and then we can select the scope so we only need this repo scope here and the reason is we just want our GitHub action to be able to push code to our public repo it has read and write access and then we can actually leave everything else unchecked to do that we'll just hit generate token and then this token will appear you shouldn't share this token with anyone I'm sharing it with you because I'm going to delete it right after this demo but we can copy that and then we'll come back over here and we'll paste that into our secret now this personal access ACC token will be accessible as an environment variable to our data workflow we just hit add secret and then I'll do a similar thing for my YouTube API key this I will not share but if you're importing your own YouTube API key or any kind of API key it's important to just paste the RW string and not put quotations around the API key I'll just add that and hit add secret now we have two repository Secrets here the personal access token and the YouTube API key so now with the secrets in place we can commit and push all our changes to the GitHub repo and watch the workflow run at our local changes we'll commit our local changes first push and then we can push all the local changes to our before we do that let's add one thing to our git ignore file if you're on Mac you'll notice that your GitHub repos will always add this DS store thing I don't want that so I'm going to include that in the git ignore file and we'll do get push now we pushed it to the repo and I guess I didn't properly remove this so I'll just go ahead and manually delete this so we see all our code has been pushed to the repo we can see that this little pending dot is appearing so if we click that we can see that our workflow is running and so to watch that we can click on the actions tab if we click on this and go to run data pipeline we can see that run data pipeline was the name of our job in the data pipeline. yo file and then these were all the steps that we defined so we'd setup job check out repo content set up python install dependencies and then we have run data pipeline check for changes commit and push if there are changes and then these steps are automatically generated from the pre-built actions of setup Python and the checkout action so we'll wait for these dependencies to install all right so the dependencies install took about 2 minutes now it's going to run the data pipeline so this will actually take longer than the dependenc because what's happening is we're making the API calls to the YouTube API to grab all the video IDs and there's another step of grabbing all the transcripts for each YouTube video and then finally we have to generate the text embeddings for those transcripts and the title of those YouTube videos so this might take a few minutes we can see that everything we printed in our data pipeline is showing up here and then we're checking for changes and they word changes so it's pushing the code but it failed and it's probably because I deleted that DS store after this workflow got kicked off this is a great opportunity to actually go to our workflow and use the manual trigger so this was that dispatch workflow option that we created so we can actually run this manually like this and so once we click that we can see that now it's running again now we go through that whole process all over again all right so this time we see the data pipeline ran successfully and it committed and pushed the changes so we can see that it added these three files and then it's going to going to just do some post setup of python and then post checkout of repo content while that's finishing up we go to our data folder and we can see that the data from our workflow is here now these files can be used in some Downstream task Okay so we've successfully automated the data pipeline the next natural step is to integrate this automated data pipeline into the final machine learning application I actually do that in this repo here which I'll also Link in the description below so we can just click that and we can see that we have our workflows folder here and we have our data pipeline. yamof file so this is the same thing that we just wrote and then we have our data pipeline here and so these were the files that we defined earlier and the rest of this is similar to what we saw in the previous video of this series where we created a search API endpoint using fast API and Docker however here instead of deploying this API endpoint to AWS I deployed it on Google Cloud platform specifically using the cloud run service and the reason I did that is because they have a free tier also they support continuous deployment so anytime A change is made to this GitHub repo it'll redeploy the API running on Google cloud and then the front end is publicly available and I'm hosting it on hugging face I'll also put this in the description below but this is now live so you can go and see the fruits of all the labor of this video series the first run is going to be slow because it has to wake up the container on Google Cloud but eventually this spins up search results so if I type in llms it'll bring up all the videos on llms I can do something else like data freelancing and then it shouldn't take as long for the second run cuz the container's already awake and we get the results for data freelancing and then we can of course do this series on full stack data science and see the search results came out much faster the container's now awake you can see all the different videos relevant to full stack data science well that brings us to the end of the series we've come a long way talking about the four hats of a full stack data scientist being that of a project manager a data engineer a data scientist and an ml engineer if you're curious to learn more about fullstack data science or the details behind this semantic search app check out the other videos in the series and as always thank you so much for your time and thanks for watching