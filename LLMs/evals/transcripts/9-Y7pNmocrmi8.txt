although language models like GPT Claude and llama gain an advanced understanding of the world they don't know everything that's because information is constantly being generated or may not be widely available a popular way to overcome this ignorance is retrieval augmented generation or rag for short in this video I'll discuss how to build multimodal rag systems in other words rag systems that can process text and non-text data I'll start by reviewing three highlevel strategies for doing this then walk through an example implementation with python code and if you're new here welcome I'm sha I make videos about the things I'm learning about and building in Ai and if you enjoy this content please consider clicking the Subscribe button that's a great NOC cost way you can support me in all the videos that I make so here we're going to talk about multimodal rag which will combine the ideas from the previous two videos of this series namely multimodal large language models and multimodal embeddings before we talking about multimodal rag it's worth answering the question what is rag since I talked about this in a previous video I won't get into all the nitty-gritty details but just at a high level rag stands for retrieval augmented generation and it consists of improving in llms responses by automatically providing it the relevant context just to give a simple example of when this might be necessary suppose that I'm trying to remember the name of a python library that a coworker had mentioned in a meeting from yesterday if I were to take that query and pass it to say chat GPT CLA or whatever your favorite large language model is it would probably give a response like this that says I'm sorry but I don't have specific access to the details of your me meeting there's no reason we might expect chaty PT or the like to have access to our meeting notes or transcript so this is a very reasonable way to respond to this query this is a broader limitation of large language models although they have tremendous general knowledge about the world they have no knowledge of things that occurred after their pre-training and their understanding of specialized or domain specific information may be limited rag allows us to overcome these limitations by automatically providing the relevant context to a large language model so what this might look like is we'll take that same query of what was that python library that Rachel mentioned in yesterday's meeting and before passing that query to Chad gbt I'll just take the entire transcript of the meeting and add it into my prompt and then pass it to chat GPT from this the large language model will be able to extract the desired python python Library which may have been unstructured and unstructured is indeed the name of a python library that helps parse files of various different formats which is very relevant for multimodal applications so what I just showed was a specific example of what rag might look like but this is a much broader idea and a basic overview of a rag workflow may look something like this where we start with a user query we take that query to retrieve the relevant context from a knowledge base we then will take the query and the relevant context and format it into a prompt then we'll take that prompt pass it to a large language model so it can generate a response there are many technical details here and knobs and dials that we can adjust to refine and adapt such a system for a particular use case but this view here shows all the Essential Elements if you want to explore rag more deeply you can check out my previous video on the topic so now that we've defined what rag is let's see how we can make such a system multimodal so in other words how can we build a rag system that doesn't just process Text data but that can also process non-text data a simple strategy would be to take a query like we did before then take that query to perform search over a multimodal knowledge base so these blue rectangles here are representing text information while these green rectangles are representing image data then we can in the same exact way as before construct a prompt but now instead of only using context that is text based we can use context that is also image based or Audi based or includes time series data or whatever type of data modality that you might want to work with we can then pass this into a multimodal large language model so this is a large language model that can not only process text but it can also process non-textual data and then it will generate a response for us the two key elements here that make this rag system multimodal are one it's knowledge base containing not just text based data and two is the multimodal large language model which can process multiple types of data so here I'll talk about three different strategies for building multimodal rag systems and we can view each of these different levels of multimodal rag as becoming more and more sophisticated as we go to the next level and then I'll just mention that the discussion here will assume that you already have a basic understanding of multimodal large language models as well as multimodal embedding models which were discussed in the previous two videos of this series starting with level one is to just translate everything into text hearing this your first thought might be well sha that doesn't really sound like a multimodal system it sounds like you're just translating all the different data types into text and just using a regular rag system and it sounds like that because that's exactly what's happening here and I would say anytime it's possible to implement the strategy for your use case I would go with this one and that's because you don't want to over complicate your AI system for the sake of sophistication a general law of engineering is to keep your systems as simple as possible so what this might look like is you can take text documents extract the text and split them into chunks you could take figures and tables and extract their captions and maybe some of their key insights and store them as text chunks and then you can take images and grab their captions or descriptions and store them as text chunks or you can even take images pass them through a multimodal large language model and prompt that model to just generate a descriptive caption for that image and that way you can extract the essential features from an image another example might be if you have videos or audio recordings that consist of people talking you can transcribe that speech and just store that speech into text chunks but no matter what the original modality is this process will translate everything to text and then you can store all these different text chunks and text items in a knowledge base and then you can just dynamically retrieve the most relevant chunks for a given query like a traditional rag system although this is a very simple way to do multimodo rag basically you put all the effort in the pre-processing of the data this has obvious limitations so sometimes there's not an obvious way to translate the original modality to text for example let's say you have time series data of like market indicators or you have time series data of brain activity from EEG there may not be an obvious way to fully capture the information in that signal into text and even if there is a way to do that you don't have a good way of passing that full richness of information to a large language model because the prompt here is text only so one way we can overcome that is level two so here we're going to keep the retrieval process as text only however we are going to use a multimodal large language model so what that looks like is that we will do the knowledgebase step exactly as we did in the previous slide so we'll take all our data modalities and we'll extract text features from them and store them in our knowledge base here it doesn't necessarily have to be a full-blown texal description of the underlying data it could simply be meta tags including like a title or a date or user that generated that content or whatever and then these meta tags could be used in these search process however the key difference here is that once we identify which items are most relevant to our query we will directly input the original data modality into our query and then we can process this multimodal prompt using a multimodal language model and a key detail here is that you have to ensure that the model you're using to process this query can indeed take the modalities that you're passing to it these days processing text and images is quite common however if you wanted to also process audio and video you would have to make sure that the model that you're using can do that the last General approach I'll describe for multimodal rag is to do both multimodal retrieval and use a multimodal language model and so what that will look like here is that let's say you extract text from the text documents let's say you have these PDF reports which consist of text and figures so you're going to extract both those data types and then you extract images directly then what you can do is you can use multimodal embeddings which we talked about in the previous video to generate Vector representations of all these items and store them into a multimodal knowledge base the key benefit of using multimodal embeddings here is that you'll have a shared Vector space which can represent the raw data that you want to include in your knowledge base and then we can perform search over these items directly by Computing the similarity between a vector representation of the query and all the items in the knowledge base so that was probably a bit high level and Abstract so let's see a concrete example of how we can Implement such a multimodal rag system using python so here we're going to walk through the development of a multimodal Blog question answering assistant so in other words an AI assistant that will have access to all the text and images from these two blog posts and we be able to answer any questions based on this content we'll start by importing some helpful libraries here we'll use the Transformers Library again to import clip which is a multimodal embedding model that can represent text and images in a shared Vector space and then we'll import a few different functions from pytorch okay the next step is we're going to load in the text and image data although I won't walk through the data extraction process the code for doing this is freely available on the GitHub repository link down here so if you go here and then you go to the multimodal rag example there'll be a notebook that lays out exactly how I took the blog articles and extracted all the text and extracted all the images from them I'll just load in the text content and image content which are saved as Json files also available on the GitHub and then I'll load in the multimodal embeddings which I generated using clip and then saved as a pie torch tensor and so we can just load those in like this if we print the shape of these tensors we see that the first one is 86x 512 indicating that we have 86 text chunks that are represented by 512 dimensional vectors so it's a vector in a 512 dimensional space and then similarly we have 177 images which are also represented in this 512 dimensional Vector space so now that we've imported the text and image content and their embeddings the next step is to define a query and embed that query in the same space as the text and image content so I'll Define a query that is what is Clips contrast of loss function and to embed this query we'll need to follow four steps first we'll load in clip this is a special type of clip that only processes is text so we'll load that in using the Transformers Library we'll also load in this data processor which will just handle the tokenization of this query before passing it into the model so once we have those imported then we can pre-process the text with this processor we just imported and then we can pass the resulting inputs into clip which is our model here and this will give us the outputs once we have outputs we can just use this text embeds attribute to extract the text embeddings and then if we print it shape we see it's a 1x 512 High torch tensor which is compatible with the representations we saw on the previous slide for our text and image chunks at this point we have Vector representations of our query as well as every item in our knowledge base so basically all the contents of these two blog posts now we can use these Vector representations to perform Vector search so this will consist of computing the similarity between the query embedding and all the embeddings in the knowledge base so we do this in the following way first I'll Define some parameters to constrain the search so here I'll restrict it to the top five most similar items and then have this additional similarity threshold that will exclude any items that have a similarity score below 0.05 starting with just the text items the way we can compute the similarities is by doing a matrix multiplication so what we do is we'll multiply the query embeddings with the text embeddings Matrix we just do this transpose here so the Matrix is the right shape and it's important to know here that if we do this matrix multiplication the elements of this text similarities tensor will be between negative infinity and positive Infinity which makes it hard to Define any kind of Threshold at the outset so a common strategy to rescale these similarities is applying the soft Max function to these values we can do that by first defining a temperature value we'll just take these text similarities divide it by the temperature and pass it through this softmax function from the P torch library and this will give us these text scores which are essentially similarities but instead of ranging from negative Infinity to positive Infinity these range from 0 to 1 and then once we have these scaled text scores we'll sort them here I'm doing ARG sort so this will sort the values but rather than returning the sorted values themselves it'll return the indices of the sorted values you can just imagine this is a one-dimensional tensor and the zeroth element of the tensor will be the index of the largest similarity score the next element will be the index for the second largest similarity score and so on and so forth now we can use these indices to generate a tensor of the actual ual sorted scores and then what this will allow us to do is to do this somewhat sophisticated filtering process to ensure that we're only returning the top K results that are above this predefined threshold we're using Python's list comprehension capability here just to break this down we're creating a tuple which consists of the indices of the sorted scores and the sorted scores themselves then we're going to Loop through the indices and the scores in these tupal and then then we'll extract the index if its score is greater than or equal to our predefined threshold so the result of this will be a list of indices sorted from largest to smallest score and then at the end of it we can just take the top K elements from this list so I know there's a lot of steps here but this is how we can simultaneously filter for this threshold and these topk results and then what we can do is go through our text content list this is what we imported from the Json file and then just return all the content from this I topk filtered list so after all that we are returned just a single search result and the reason is even though K was set equal to five the other four search results had similarity scores below the threshold so they were not included I want to point two things out from looking at this first is that Beyond just the text that was used to do this search there's also the article title and the section title where this text was taken from in the article and so having metadata like this is helpful for both the search functionality because it gives you more flexibility in how you do search but also it allows you to give more context to the downstream model in generating responses so this is just a good practice and then you can check out the data preparation code of how I extracted this metadata on the GitHub repo but the second thing I want to point out is that this text doesn't seem super helpful in answering our original query of what is clip's contrastive loss function indeed the section is about contrastive learning which is definitely similar to our query but just because this text here is similar to the query doesn't necessarily mean that it's helpful in answering it this is one of the main limitations of vector search which is that using these embedding models out of the box even though it allows you to identify similar Concepts it doesn't necessarily help you identify items in your knowledge base that are helpful to answering your query although there are many ways we could mitigate this issue a simple solution is to just have less stringent search parameters to do that I'll first can this whole similarity search process into a userdefined function so we don't have to just keep writing it over and over again and then I will do similarity search over the text chunks and the images separately and this way I can have different search parameters for the text and images here I have more inclusive search parameters than we saw in the previous slide so instead of k equal to 5 I have k equal to 15 the threshold went from 0.05 down to 0.01 and then I kept temperature the same and then for images I just keep k at five I have the threshold as 0.25 and I have the temperature as 0.5 so the way I determined these parameters was just through trial and error on three simple queries and just eyeball in it so that's a fast and easy way to build out an initial version of a retrieval system but if you are serious about optimizing it and improving the model's results it's going to be helpful to generate a data set of example queries and their Associated relevant items then you can do some kind of grid search to find the best choice of search parameters here the results from these two searches are shown here so we have 15 different text items returned but just one image item was returned looking at these texts this one mentions contrast of learning this one mentions clip this one mentions clip this one mentions clip so we see that yes indeed that these items are similar to our input query but it doesn't necessarily mention anything about the loss function that's not a promising result but if we look over to the image this is exactly what we're looking for so this is Clips contrastive loss function laid out mathematically now that we've retrieved our context let's see how we can put this into a prompt to pass it to a multimodal language model so the text results are stored in this list of dictionaries with all the different metadata so we're going to need to translate that data structure into a string representation so we can pass it to a language model the way I'll do that is I'll iterate through all the items in this text results and then just format it according to this code here so what this looks like in text is this this we have the article title with these Asters on either side we have the section that the text was taken from and then we have the text snippet right here and the reason I'm using these asteris is because this translates to bolded text in markdown which is a text formatting language so a trick for adding more structure to your prompts is to just write them in markdown then we can do a similar thing for the image results and so what that looks like is we have the article title the section it was taken from the path of the image and then the image caption then what we can do is we'll generate this promp template and so here we're going to combine the query and the text and image context says given the query and then we'll just insert the query here and the following relevant Snippets and then we'll just include all the search results here please provide a concise and accurate answer to the query incorporating the relevant information from the provided Snippets where possible so once we've constructed this prompt we can pass it to llama 3.2 Vision here I'll run everything locally using ol llama like I did in the first video of this series and since I'm doing it in the same exact way I won't spend too much time on the code here but basically what we're doing is we're loading in llama 3.2 Vision which can understand both text and images and then we'll pass in the prompt as well as all the images from our image search results and then with that we can print out the response the monos response looks like this we can see that it has like this interesting syntax to it which is actually markdown so what we can do is take the raw output of the model and then process it with some kind of markdown reader and it's going to look something like this and then this is another upside of writing your prompts in markdown is that models will tend to write their responses in markdown when prompts use it there are some good things about this response so one it correctly understands what is in the image so it says the image depicts a contrast of loss function for aligning text and image representations in multimodo models however it seems to misunderstand the meaning of positive Pairs and negative pairs in this context it seems to think that a positive pair is any text image pair while negative pairs are text text or image image pairs which don't make any sense in the context of clip all pairs are text image however positive pairs are a text image pair that correspond to one another while negative pairs are text image pairs that don't correspond to one another so the model doesn't really quite understand that and maybe if the text results were more helpful it would have understood that it goes on to talk about text text or image image pairs which just don't come up in clip but it does kind of get this right that it calculates the difference between positive and negative pair similarities how it works text image embeddings generates embeddings for both text and images using multimode encoder so that's true and then it goes on to talk about the benefits which is interesting so lines text and images for better visual content understanding of visual content from text description enhances performance and downstream tasks like image classification retrieval and generation okay so it gets the benefits it doesn't really answer the question of what is clips contras of loss function it does describe some like General features and some important points but even though it had the answer in the image it didn't convey that information very well and I suspect one of the reasons for that is so many of those text search results were not helpful here we just kind of walk through the process and logic of implementing this multimodal rag system step by step but this isn't how you would actually implement this for some kind of real world project so for that also on the GitHub repository I have a radio user interface that kind of packages this all up if you run that Notebook 3 in the example code you should see something like this so let's try a different one so what are three paths described for making llms multimodal although this is something that llama doesn't necessarily need to read my articles to understand and this is something more article specific so let's see how it handles this one so far so good so it's using the Snippets it got the first path llm plus tools path to llm plus adapters so here it seems to have done a much better job and that's likely because the search results are higher quality for this query as opposed to previous queries and then you notice that we're streaming the text here which is better from a user interface standpoint and then you can see how that's implemented in the example code so here we implemented this multimodal rag system and one thing that came up is that the quality of a multimodal rag system is very much dependent on the quality of the search process of this retrieval process and we saw that the search returning irrelevant items can be a major problem there actually a few other ways we can improve this the first is we could use a ranker the way this works is that rather than only relying on Vector search or keyword-based search what you can do is take those top 15 text search results we saw in the example code and we can pass them through yet another language model that's trained via contrast of learning but specifically to identify query and helpful answer pairs so before we used clip embeddings to generally identify items in the knowledge base that were similar to our query the value of a ranker is that we could use a model that was specifically trained to identify positive pairs of qu iies and relevant search items so the output of this will be another set of similarity scores we can use in our rag system another strategy is that rather than using Clips multimodal embeddings out of the box we could fine-tune clip so that rather than simply evaluating the similarity between the query and items in our knowledge base we can f- tune an existing multimodal model to align queries with relevant items in the knowledge base so not just semantically similar ones but items that are helpful in answering the query but of course the downside of fine tuning is that this is going to require us to generate a lot of query item positive pairs in order to do this training process so if either of these techniques sound interesting to you and you want me to cover them in future videos let me know in the comment section below if you enjoyed this video but you want to learn more check out the blog published in towards data science although this will be a member Only Store you can as always access it completely for free using the friend Link in the description below and with that thank you so much for your time and thanks for watching