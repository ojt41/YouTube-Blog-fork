with the rise of open- source models and efficient fine-tuning methods it's never been easier to build custom ml solutions for example anyone with a single GPU can now fine-tune a large language model on their local machine which is exactly what I did in a previous video of this series however since my machine is an M series Mac which doesn't have an Nvidia GPU I had to use the free GPU on Google collab to run that example this is somewhat disappoint pointing because using collabs free gpus is somewhat restrictive and not as convenient as running something on my local machine that's why in this video I'm going to share an easy way to fine-tune an llm locally on Mac and if you're new here welcome I'm sha I make videos about data science and Entrepreneurship and if you enjoy this content please consider subscribing that's a great no cost way you can support me in all the videos that I make if you've been keeping up with machine learning over the past decade then you're probably familiar with Nvidia and all their different gpus it turns out that gpus are super helpful for machine learning because they can much more efficiently train and run machine learning models than traditional CPUs they've also demonstrated an ability to take Nvidia from a hundred billion valuation all the way up to 3 trillion in the past 5 years nvidia's dominance of the GPU Market has greatly influenced the available open-source tools for running and training neural networks the result of this is that a lot of open- source tools work seamlessly with Nvidia Hardware while this is great for Windows and Linux users it often leaves Mac users like me left sitting on these Sidelines after a failed attempt to locally fine-tune llama 2 on my local machine my impression was pulling this off is something that would take several hours of effort that was until I discovered the mlx python library mlx is a python Library developed by Apple's machine learning research team for efficiently running Matrix operations on Apple silicon so we see the documentation here it's inspired by Frameworks like pie torch Jacks and array file one of the notable differences with mlx is that it allows you to use the unified memory model of these M1 chips no longer do you have to worry about RAM and vram being separate things M1 chips just have a single memory so that means me with my Mac Mini M1 2020 with only 16 gigs of memory am capable of fine-tuning a large language model locally on my machine while mlx is a pretty low-level framework it's not going to have highlevel abstractions for loading and training models like hugging face for example there is this example implementation of Laura which is very readily hackable and adaptable to another use case which is exactly what I'm going to do here similar to the previous Cur video here I'm going to fine-tune a quantized version of mistol 7B instruct to respond to YouTube comments in my likeness however instead of using hugging face in Google collab here I'm going to use the mlx library and my local machine and again here are the specs of my machine it's a Mac Mini M1 from 2020 with only 16 GB of memory so by today's standards my machine is hilarious but despite that it's still good enough to implement ment this fine-tuning example so I've put together some example code and that's available on GitHub if you go to my YouTube blog repo go to the llm tab here we see all the different code and videos and blogs of this series so now there's a new one called kulur mlx so we click on that what I've done here is I have this notebook that walks through the example code and then I've taken all the scripts from that mlx example implementation so I've put them in the scripts folder and then here I've prepared data so this is data of my YouTube comments and I prepared it in this Json L format but we won't talk about this right now talk about it a little later okay so the first step in running this example is to go to the repo and clone it so I'll copy this Ur here I'll go over to my terminal let me zoom in a bit okay so we're going to clone the repo get clone might take a while unfortunately there's not a way to clone a specific subfolder of a GitHub repo if you you want to download some code from a repo you have to clone the entire thing go to the repo we just cloned and then the code is in the llms subdirectory and then it's Q mlx so here we see everything we've got our example code requirements file some scripts some data and a readme okay so I cloned the repo and I navigated to this folder so now let's create a python environment that allows us to run the code so here I'll create a new virtual environment called mlx DV and then I'll activate this environment so this how you do it in bash and zsh and I guess every Mac User uses this so this is how you'll activate the environment all right so now you can see we're in this mlx DV environment but the only thing in here is PIP so you see you do pip list all there is is PIP so let's install all the required libraries so we can just do the PIP install D requirements. text and this will install everything while that's happen happening we can look at what is in that so here we have mlx we have mlx LM so this is a library built on top of mlx specifically for large language models also we have the Transformers library and numpy and then finally since I wrote the code in a Jupiter notebook we'll install Jupiter lab in IPI widgets okay so we installed all the requirements and if you run into trouble in the installation steps so here are some important notes from mlx is documentation first and foremost you need the M series chip like that's the whole point of this video and this Library the second is that you are using a native python that's greater than or equal to 3.8 so I think here I'm using 3.12 if I just do this yeah so I'm using 3.2.2 then you have to have at least Mac OS 13.5 but they recommend that you have Mac OS 14 which is what I'm running on my machine right now so now that we've cloned the repo and we set up our environment let's run through the example code so to do that we'll do Jupiter lab okay so here we've got the example code video link and blog link coming soon cuz I'm making it right now and then we're going to zoom in because it's probably tiny on your screen all right so mlx fine tuning we're going to do some imports so we're going to import subprocess because all the example code that we're hacking runs from the command line and so subprocess allows us to run terminal commands through Python and then here I'm importing the mlx LM library to run inference on a model a little later so I defined some helper functions here they're not super important right now so I'll just come back to them as we encounter them in the code and then this is an optional step that example code from mlx comes with this convert. py script which is capable of taking any model from the hugging face Hub and converting it into the proper mlx format and additionally quantizing it so I actually did this for mistol 7B instruct version 0.2 and then there there's this argument that you can pass that will push the model to the mlx community so I guess that's worth calling out right now mlx has this page on hugging face here are several mlx compatible models many of which are quantized so there's Google Gemma mistol quen code Gemma llama 3 53 whisper llama 3.1 even so this page seems pretty active and basically any model that you would want to find tune is probably already available here and if it's not you can simply just go find the model that you want to use in mlx let's go to mistal AI I'll go to Mistral 7B instruct so this is the one I used given this hugging face model path we can convert it to the mlx format and quantize it using this convert. py script so there was this typo so I had to add this quantize flag so what this will do is grab the hugging face model convert to the mlx format and this quantize flag converts into 4bit quanti ization so I actually didn't run the command yet so you could run this command using the subprocess module but I found it's better to just print the command like this and copy paste it into the terminal because when you run these shell commands in a jupyter notebook you don't see the same like progress metrics that you would normally and this will actually save a mlx version of that model on your local machine that you can run for inference and fine-tuning since here we're going to use a model that's already available on the hugging face hub here's the model card for it we can just skip this quantized model step and so here what's happening is I'm going to build the prompt that is defined here so again here we're creating a YouTube comment responder what this model will do is you'll pass in a comment and it'll respond to that comment hopefully in my likeness so for this example I'm not just going to pass the raw comment into the model itself for inference I have actually constructed this prompt which is the same prompt that we saw in the previous Cur example to help the model generate better responses I created a Lambda function for this to find this instruction string and then did some string manipulation to incorporate the comment into it dynamically the result of that is this prompt Builder Lambda function which takes in the comment and so here we're just doing a very boring comment great content thank you and then we'll Define the max number of tokens and then here we're going to use the mlx LM library to do inference so the syntax looks very similar to hugging face which is pretty convenient if you're comfortable with hugging face already so here what we're going to do is we're going to load in the model so this is that same quantize model and then we're going to have the model generate a response so we pass in the model the tokenizer which was loaded automatically The Prompt that we defined using our prompt Builder the max number of tokens which I set to 140 and then I put verbos equal to True okay so we can take a look at the prompt here which is all this stuff I'm not going to read all this you can read it if you like but this is just to help nudge the model in the right direction then we have this please respond to the following comment and this is the comment that we spliced in using that prompt Builder Lambda function and then down here is the model's response so this is the raw quantized model no fine-tuning whatsoever and this is the response well first and foremost it put sha GPT in the wrong place this is supposed to be at the end of the response not at the beginning but it says thank you for your kind words I'm glad you found the content helpful and enjoyable if you any specific questions or topics you'd like me to cover in more detail please feel free to ask I would never respond to a comment like this this is something we've seen in the previous fine-tuning videos where the unfin tuned responses tend to be pretty verbose and when I respond to comments or really any kind of communication I try to keep it as concise as possible so this is very different than something I'd actually write in response to a comment so let's see how we can fine-tune this model to generate responses that sound a lot more like me here again we're going to use one of the scripts from that mlx examples repo and I'm going to construct the command in the same way so I put everything in a list and the reason I have it in a list is because that's how you can run these terminal commands in Python you'll pass it in as a list to the subprocess module but I realized that even with this fancy helper function that chat GPT wrote me to continuously print outputs from my terminal command it still wasn't printing the loss during training so I actually found an alternative strategy to be more helpful which is to just take this command variable here and to translate it into a string that I can copy and paste into my terminal and so this construct shell command is just a pretty simple helper function up here that's just doing some basic string manipulation to convert this list into a string what I'll do is I'll copy this string and then we can go over to terminal and then I'll paste the command here here okay so walking through this a little bit I'll try to zoom in even more hopefully that's legible we're running a python script so that's what this is the python script we're running is called lowra and it's in this scripts subdirectory and then we're just going to define the parameters for training so here we're specifying the model which is that 4-bit version of mistal 7B instruct we have this train flag because we're going to run training we're going to set the number of iterations so we're going to do 100 iterations that means it's going to run through 100 batches and the default batch is four next is steps per eval so this is the number of batches that the training will run through before Computing the validation loss so I set it to 10 which is the same as the number of steps per training loss evaluation the Val batches is the number of examples to include in the validation loss calculation setting it to ne1 goes through everything in the validation data set and here that's only 10 examples next we set the learning rate at 1 to the minus 5 which is actually the same as the default but I have it explicitly written here cuz I was playing around with this I probably ran this a dozen different times trying to find the best hyper parameters and then low R layers 16 which is also the default parameter but I went through a lot of iterations to just end up coming back to the default and then finally we use this test flag which computes the test loss at the end of training I guess before we run this it's worth talking about the data that we're using here we have three data sets so we have have this train. Json L test. Json L and valid Json L the way I make these data sets are here in this jupyter notebook also available on the GitHub what I'm doing here is I'm taking a CSV file of YouTube comments and responses which looks like this so I've got 70 comments these are real comments and 70 real responses from me and so this is the way I'm going to train the model to respond to comments in my likeness and way I do the train test validation split is I have 50 examples for training 10 examples for testing and 10 examples for validation I won't walk through this code cuz I did it in the Cur video and I did a similar thing for the open AI fine-tuning API video but if you're curious about how I'm doing the data preparation feel free to check this out we're using the same prompt in the training data as I used at inference in the example code we just saw very similar strategy the Json l format if you're familiar with python is essentially a list of dictionaries so a dictionary is a set of key value Pairs and a list is just a sequence a collection of elements Json L is just going to be a collection of these key value pairs where each key value pair each dictionary consists of one key and one value so super simple the key is text and the value is the example that you want to use to train or evaluate the language model notice that this contains the instructions essentially of the model this bit is the comment the real comment from the user and then it ends with this instruction token and then here's the real response from me so all of these are packed together to form the example and we have 70 of these and 50 are going toward training 10 are going for testing and 10 are going for validation so here I randomly select examples for testing and validation and then I just write everything to these Json L files so hopefully this example code is easy for you to follow and hack and adapt for your own use case and if you get stuck you know feel free to drop a comment or reach out I'm happy to help try to get you unstuck okay so that brief aside was the data preparation now let's go into training so I actually haven't done this while running OBS so my computer might blow up if I do this so I've got activity monitor and it's not looking great because OBS is already using a gig of memory and when I was running this the fine-tuning script was taking like 14 G gabt of memory and so I'm going to execute this script but if my computer blows up and I'm not able to post this video I'm so sorry all right here goes nothing nothing has blown up yet we'll just keep the activity monitor here so we can monitor the memory pressure so we see that the fine-tuning script is taking about 10 gbes of memory there's this other python script which I guess is the Jupiter notebook taking up 4 GB of memory M and then we got OBS here taking up 1 gab of memory so when I was doing this for real I was basically doing nothing else on my machine I was just allowing as much memory as possible to be dedicated to the fine-tuning script but here it seems like the mlx library is handling it pretty well you know it seems to just dynamically adapt to however much memory is available okay so it computed the V loss I don't think anything's going to blow up so that's great and kudos to Apple and their ml research team for writing a good software library but but I do think since less memory is being allocated to this compared to just not running anything else on my machine this is going to take a lot longer to run before when I was just allowing the training to run all by itself I didn't have the jupyter notebook running either this was hitting like 14 GB of memory 13 to 14 or something got kind of close to 13 there and it took about like 15 or 20 minutes to fine-tune the model with the hyperparameters shown here batch size of four and has 50 training examples so I'm not going to sit here for 20 minutes to wait for this to run cuz I've already done this and I can show you the finished product once again like those cooking shows where they show you how to prepare the food and put in the oven and then magically they had the lasagna that they made last night and they're going to eat it in front of us okay so here's me eating the lasagna we're going to quit out of this process killed that and then what we can do is to run inference with the fine tuned model so this doesn't have the adapter file so what I'll do is open up a new one of these we'll do this in private once training is done so we'll say 20 minutes goes by you go get a sandwich or something while this is running this adapters. npz file will appear in the repository these are the low weights learned during training once that is here we can continue with the Jupiter notebook and we'll use these adapters to run inference again so this will be our fine-tuned model so we're going to run it in a similar way but instead of printing a command and copy pasting it into the terminal here we can just run it in the notebook because there's really not much to see you can see that it failed cuz the adapters file wasn't there but now that it's here we can run this again so now it loaded the pre-trained model it passed in the prompt and the prompt is including the comment which is just that simple great content thank you and then we have the response from Sha GPT glad you enjoyed it smiley face sha GPT so this is much more aligned with something I would actually say to a short comment like this going back this long and poorly formatted comment is not what we want but just after 50 training examples we see that the model is noticeably responding in a different way and then we can even run this a few times to see what else it comes up with glad you liked it so I guess it'll probably keep generating responses that are similar to that yeah glad you're here happy to help okay yeah these are great these are things that I have said in comments so it's doing a good job but that's a really easy comment to respond to let's try a different comment so here's a harder one that at I think this is more recent I don't know this is not in the training data or anything so let's see how sha GPT mlx handles this one so the comment is I discovered your channel yesterday and I'm hooked great job it would be nice to see a video of fine-tuning shot GPT using hugging face I saw a video you running it on Google collap 7B any chance of doing a video on your laptop Mac or using hugging face spaces so that's exactly what this video is let's see what shot GPT thinks hi thanks for your kind words sure thing I'll do a video about fine-tuning HF version on Sha GPT on my Mac then has this YouTube link hope it helps sha GPT okay so let's see what this takes us to ah the video doesn't exist probably because I'm making the video it'd be really crazy if when I post this video this becomes the URL you know it's responding appropriately you know it's like thanking them it's pretty short it puts sha GPT at the end this sentence here doesn't make a whole lot of sense so I'll do a video about fine-tuning HF versions of Shaw GPD so hugging face versions of sha GPD on my Mac see what else it says so I guess this is a kind of hard question to respond to how would it know what I want to do glad to hear it glad you found the channel useful okay well this is a nice response but it doesn't answer the person's question so let's try another one hey glad you're enjoying the channel okay refuses to respond I guess another thing is we can check out the memory spikes during inference so let's do this one more time take a look at the memory pressure so we see that it kind of goes up a bit I guess it's opening up another python instance to run these subprocesses and it takes about 4 GB of memory yeah GL here so it's refusing to respond to this guy's comment I think I got lucky when I uploaded this because it had a really good response let's see glad you enjoyed it I'm looking forward to doing a fine-tuning video on my laptop I've got a Mac M1 mini that runs the latest version of the HF API so the great thing about this one is that it's got right that I have a M1 Mac Mini does run the latest version of the hugging face API but we didn't use the hugging face API but yeah I guess we did so we imported Transformers so Transformers is working under the hood so that was the example super simple but I will say there was one thing I forgot to mention which is that in this L.P file I went through like a dozen different sets of hyperparameters to try to get this thing working which is just the reality of machine learning machine learning is much more art than science or at least for now but I did want to point out one thing that I had to do so I had to go in here and kind of hack one set of hyper parameters okay here so adjusting the rank of these Lowa adapters is not something that this L.P file exposes as a command line argument so you can't just say oh I want to try rank four or rank eight or rank 16 or whatever from the command line I had to go in to the file and just manually change it I think it was eight originally and I changed it to four and this improved the training performance before I did this it was just kept overfitting and I tried a lot of different sets of hyper parameters but reducing the rank worked a lot better and this aligns with results from the low R paper if you've taken a look at and if you haven't check it out it's a really good read rank four rank eight seem to be that sweet spot for the results I guess I can pull it up real quick so in table six of the lowette paper you can see they were comparing what weights they were applying the adapters to and the different ranks of the adapters in the qar example on Google collab I just applied fine tuning to the query layer using rank eight but in this example I applied it to both the query and the value layers and I used rank four and you can see that at least in these examples the rank four rank eight is kind of like this inflection point in the performance like it kind of flattens out and actually starts to get a little worse as the rank gets too big okay so that brings us to the end of this walkthr the example code is again freely available on the GitHub repository shown here and I'll link it in the description below if you enjoyed this video or you have suggest sus for future content please let me know in the comment section below and as always thank you so much for your time and thanks for watching