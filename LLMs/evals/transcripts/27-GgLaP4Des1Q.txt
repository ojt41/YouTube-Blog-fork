hey guys welcome back this is video number two in the two-part series on principal component analysis pca and independent component analysis or ica ica is the topic of this video so much like the last video i'll start with a brief introduction into the technique dive into the math a little bit i will compare the two approaches talk about their similarities and differences and then i'll finish with a concrete example of how you can use ica with some example code provided in the github repository so let's get right into it okay so the standard problem for independent component analysis is the cocktail party problem so in its simplest form you can think of two people having a conversation at a cocktail party and for whatever reason you happen to have two microphones kind of set up next to these two speakers both microphones are going to pick up audio from both of the speakers uh kind of like our purple microphone and pink microphones here the purple microphones a little closer to the blue speaker so it picks up more of the blue speaker's audio relative to the red speaker and vice versa the pink microphone picks up more audio from the red speaker than the blue speaker so then the problem is how can we take these audio recordings that have both speakers kind of side of the conversation mixed together and separated out um to two audio files uh each of which only contain audio from a single speaker well that's exactly what independent component analysis does it trans uh transforms a set of vectors so you can think of the raw or you can think of the recorded audio by these two microphones into a maximally independent set so so that's what's being represented here um so you have the purple and pink audio signals then they get translated uh to the original the sources of the audio which was uh the speech from the blue speaker and then the speech of the red speaker respectively so again the purple and pink are your measured signals the blue and the red are the independent components or the source of the information or the audio okay so there are a couple key assumptions to independent component analysis so assumption number one your independent components are statistically independent and that's defined in the typical way in statistics the joint distribution of two variables x and y is equal to the probability distribution of x times the probability of y and then the second key assumption is that your independent components are non-gaussian which might be a little strange you know in statistics and science we love to say everything's gaussian it makes things much nicer and it allows us to do a lot of rigorous analysis but this is one of the instances where we actually need the independent components to be non-gaussian for this to work okay so we have our measured signals again that's from your microphone example and then the independent components which is what your speakers are saying in the cocktail party problem so we can use our independent components we can combine them in some way so that's what's being represented by this expression to kind of recreate our measured signals x you can think of the independent components as being sources hence that's why this vector is an s they are sources of information or audio that are being combined in some way to generate what's being measured at your microphone for example so we have x1 and x2 your measured signals and then your independent components or the sources of your signals s1 and s2 but you can also kind of turn this around and you can combine your measured signals to express your independent components if this is the case if you can just have some linear combination of your measured signals to derive your independent components the set of values defined by w is all we need in order to do ica okay so mathematically the goal is as follows so given some uh measured signals given some data x we want to solve for the matrix w such that the set of independent components or the set of source vectors s sub i are maximally independent this concept of maximally independent what does that mean how do we quantify that so there are two ways you can define w in such a way that it minimizes the mutual information between all your independent components or you can maximize the non-gaussianity of the independent components defined by uh this w matrix um i'm not going to go any further than that if you're interested in more information check out the blog post linked in the description i kind of go into a bit more detail on the math pc and ica they're similar techniques in a lot of ways but ultimately they're distinct they are different approaches that kind of aim at different tasks or they make different goals so pca typically compresses information if you saw the pca video the example was hot dogs and hot dog buns those are two quantities that are heavily correlated so instead of representing that information with two variables you can represent it with just one and so that's where pca is a good thing to use because it will compress those variables into those two variables into a single variable on the other hand ica separates information it's going to take two variables for example like your two speakers or the the audio picked up by two microphones placed close to two speakers and it's going to separate out the independent components or the sources or the independent drivers of those measured signals so kind of similar but they are different goals and different final outcomes a commonality between pc and ica is auto scaling so this is a critical part of the preprocessing so auto scaling is for each variable you have to subtract the average of that variable and divide each element by the standard deviation of that variable and then that's one of the reasons why it's typically advantageous to apply pca to your data set before applying ica because it kind of all the pre-processing is already handled for you pca will clump all the information together the correlated variables and then ica will come in and separate out independent drivers if it's applicable okay so as always i'm going to include a concrete example so here this is something that that's relevant to my research and this is where i kind of came across the whole technique of independent component analysis was to solve this specific problem in my research we deal with uh eeg data so what is eeg uh eeg is a technique of measuring brain activity uh by placing a set of electrodes on the head you know eeg is a very powerful technique because it has a very good temporal resolution and it's also non-invasive people can kind of move around with the this cap on but that kind of also leads to one of its more fundamental weaknesses is that since the electrical signals that it's trying to measure from the brain are so weak eeg has to be very sensitive to these kind of fluctuations and voltage which makes it very prone to artifacts or perturbations oscillations in your signal that do not come from brain activity so this could be like blinking which is the what we're trying to resolve in this problem or motion artifacts people talking or other kinds of noise that can kind of get injected into the data so here we have a plot of the voltage versus time sorry i should have had labels on these axes but the y-axis is voltage in millivolts the x-axis is a time index essentially and this is for the fp1 electrode which sits near the front of the head on the left side so on your left forehead and so this electrode is particularly prone to blink artifacts because it's the one of the closest electrodes to your eye and then you can actually see the blinks occurring because you'll have these giant spikes in the signal so we're trying to get rid of that because with eeg we're trying to measure brain activity not blink activity okay so the first step here is applying pca so here we have 64 electrodes on our eeg so that translates to 64 variables so we can use pca to kind of clump that down to just 21 variables and i just did some trial and error to find the right number of principal components to go down to and at the bottom here you can see the explained variation is 99.5 percent and in matlab it's really simple you may notice i don't explicitly auto scale the data that's because the pca function in matlab does this automatically which is pretty nice but it's all done in one line in matlab and it can be done in one line in psyc or a couple lines in psychic learn and if you didn't check out the previous video on pca that'll share some example code on how to do that okay and then again we can apply ica to the set of principal components that we got from pca so that's what's being done here so now we can just plot all the independent components okay so again we had 64 electrodes on our eeg cap that translates to 64 variables we then use pca to reduce the dimensionality from 64 variables to just 21 and then finally we applied ica to those 21 variables to kind of separate out the independent components and then just looking at this visually uh kind of independent component 10 5 and 12 are reminiscent of those blank artifacts we saw in that initial plot and again these aren't the just the independent components themselves or the raw independent components i actually squared them so that all the values would be positive and then the blank artifacts would be a bit more prominent okay so i just used a rough heuristic basically i picked out the independent components which had four prominent peaks and so this isn't a robust way to do it i was doing something fast and wanted it to be repeatable so it picked out independent components 10 and 12 to correspond to the blanks okay so 10 and 12. i'll buy that maybe five should have been included but we'll see how it turns out okay and then we can essentially just drop independent components 10 and 12 because they contain blink information which we're not interested in we only want brain information so we can drop those two components and then just work backwards we'll reconstruct our score matrix basically the output of pca and then we can reconstruct our original 64 variables by going backwards in pca so doing that and plotting everything before the blink removal fp1 had these four prominent peaks corresponding to blinks and then afterwards uh they went away so it's kind of like magic and this is a rough way to do it there are other ways to do it but this is more so just as an example of what ica can be used for so that concludes the two-part series on principle component analysis and independent component analysis if you found this video helpful please like subscribe comment share i would very much appreciate that and i would love to hear your feedback i look forward to seeing you guys in the next video and thanks for watching