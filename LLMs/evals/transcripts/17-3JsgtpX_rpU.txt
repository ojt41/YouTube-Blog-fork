the roles that have the highest score are it professionals it seemed like it professionals was a promising customer avatar for these workshops we have Consultants they seem like a promising customer here and then finally data analyst seems like a promising Avatar these actually were the three avatars that seem to be most interested in this type of Workshop large language models have taken over the business world and all the big companies are trying to use generative AI although tools like chat GPT are clear cly powerful what's not so clear is how businesses can use this technology to drive value in this video I'm going to talk about three ways to use AI for something that every business cares about sales I'll talk about each use case at a high level and then dive into example python code using real world data and if you're new here welcome I'm sha I'm a data scientist turned entrepreneur and if you enjoy this content please consider subscribing that's a great no cost way you can support me and all the content that I make for most businesses that I've interacted with using AI typically means building a chatbot a co-pilot an AI assistant or agent while these are great solutions to some problems they are far from a cure all a key challenge is that large language models are inherently unpredictable getting them to solve a particular problem in a predictable way often comes at a major cost for example popular tools like chat GPT and GitHub co-pilot were notoriously losing money in 2023 however businesses can use AI for more than just building chatbots and the like here I'm going to talk about three AI use cases that are not a chatbot these are all going to be in the context of sales which is something that every business has to do the first use case is data augmentation the second is structuring unstructured data and the third one is lead scoring I'm going to talk through each of these use cases one by one and then dive into some example python code implementing these use cases on real world data from my business data augmentation consists of adding data to your data set this can look two different ways you can either add more variables to your data set so here more columns or you can add new rows or examples and the reason this is powerful is because businesses can use data to solve problems and make decisions for example a popular business are data vendors who sell data to companies to help them make decisions one of the most popular data vendors that we all know is FICO they generate these credit scores for basically every potential lender and then sell them to financial services companies that are writing loans for people another reason data augmentation is valuable to businesses is because it can help augment their machine learning and data analytics efforts an example of that in a sales context is let's say we have have a list of names and a resume for each person on that list if we wanted to do some kind of analysis on these resumés we might want to extract key information such as years of experience or industry from each of these resumés there are a few ways we could go about this so one way is we can just read every resume and manually write out the years of experience or the industry of that person but of course this would be very tedious and if you have thousands or tens of thousands of resumés this is not something that's practical another idea is to try to automate this process with some kind of computer program but if you have resumés coming from many different sources this also may not be feasible because people format their resumés in many different ways so this is one area that large language models can help because they can better comprehend a body of text like a resume to extract things like years of experience or industry more readily from text another key AI use case that a lot of people are talking about is structuring unstructured data so what does that mean structured data is simply data that can be represented by rows and columns typically consisting of numbers on the other hand unstructured data cannot fit into a table examples of this are text documents PDFs audio files images these type of data are not organized in a table the reason that this distinction is important is because when it comes to using data it's much easier to extract insights from it and do analysis is on it when it's in a structured format while if it's unstructured you're going to have to do some extra steps before you can do any kind of analysis on that data one of the exciting things we're seeing these days with large language models is the Improvement of the so-called text embeddings and if you're not familiar text embeddings I got a whole video on it so if you want to dive into the details you can check out that video but at a high level all a text embedding does is it takes a chunk of text and translates it into a meaningful set of numbers so basically what this means is we can take a stack of résumés or any type of text and organize it into a structured format into a table that we can readily analyze and then the final use case I'm going to talk about is lead scoring essentially what this consists of is taking customer data and from that information generating a prediction of How likely it is that that customer will buy your product or service what this typically looks like is you'll take data such as job title the revenue of the company that they work at customers's Behavior how much they interacted with your website or social media platforms or something like that and then where the lead came from as well as many other potential indicators and all this information is synthesized into a single metric that represents How likely it is that that customer will buy your product while lead scoring has been around for a very long time it's not something new that came around with large language models however the opportunity that large language models have presented is not so much in generating the score itself but rather improving the inputs that we can pass into lead scoring models now I want to bring together all three of these use cases and apply them to a case study from my business so I've recently been trying to validate doing AI workshops as a new Revenue source for my business my first step in validating this idea was reaching out to a 100 people on LinkedIn the Outreach script look something like this I'd say hey I'd add some personalization and then just ask for their feedback on doing a AI workshop and gave them some details so from these 100 DMS 58 people responded which is pretty good 18 people said yes they would attend and 19 people said no they wouldn't attend then everyone else who responded is basically a maybe the goal of this initial Outreach is to identify promising customer avatars so what I did here is I sent out DMS to a wide range of people of all different job titles and backgrounds and then did an analysis trying to find the common factors between the people that responded said yes and said no here I'm going to walk through the code of this analysis which involves the three use cases we saw earlier and as always the code is freely available at the GitHub repository however I can't share the data because I was analyzing people's resumés and don't want to share that publicly due to privacy concerns okay so here we have the example code for the first use case we start as always by importing some helpful python libraries I'm using polers to organize the data in data frames I'm using the open AI python API to do the data augmentation and then finally I import numpy the first step here is loading the data there was actually a Step Zero which consisted of extracting text from people's resumés where I got the resumés was actually from LinkedIn so if you go to anyone's LinkedIn profile and you go to more and then you do save to PDF what will happen is LinkedIn will aut automatically generate a resume of that person which will look something like this for everyone that I reached out to I went and manually downloaded this resume and then I have this python script here which will actually crop the resume to only keep this white area and extract the text from it that's what this code does it extracts the text and then it saves it in a polar data frame consisting of two columns one corresponding to the name and then the second Corr responding to the text from the resume so that's what we're importing here I saved it to a CSV file and so now I'm reading it in as a data frame at this point the only two variables in our data frame are the names and the resumés so let's see what it would look like to try to extract the years of experience from the text in people's resumés here I'm going to use the open AI python API to do this that'll involve us defining two different prompts so I have this system prompt which will be used in open ai's chat completions API this is the system prompt you are a resumé analysis assistant your task is to classify RS into one of the following experience level buckets based on the number of years of professional experience listed in the resume and then a defines these five buckets and then it gives some more instructions so I actually generated this system prompt using chat GPT I just asked it to create me a system prompt for chat GPT and I think that tends to be a pretty good starting point for prompts because Lar language models tend to think differently than we think so it's better to have data generated from a large language model if you're going to pass it in to another large language model typically just a rule of thumb but we also need a user prompt that we're going to pass into the model the way of implemented that is using this prompt template so this is essentially a function that takes in the text of a resume and then dynamically inserts that text into this prompt then we can just pass this whole thing to the open aai API to generate a response what this prompt is asking is to take the text from this resume and then output either 1 2 3 4 or five based on the levels of experience so once we have that we can pass each resume over to the open AI API to generate a response to basically classify each person's level of professional experience and so the way that looks is very similar to what we saw in the AI assistance video so if you haven't checked that out I'll link it on the screen here basically what's happening is we're going through the data frame one row at a time we are extracting the text from the resume passing it into the prompt template and defining that as our prompt and then what we do is we pass the system prompt and the user prompt both to GPT 40 there are also a few arguments that we can specify which will improve the responses from the model so here I just want the model to respond with a single number however large language models like I said earlier are inherently unpredictable you'll ask it to do a very straightforward task but then it'll have some weird formatting maybe instead of just returning the number it'll first try to put experience level colon and then the number in its completion so that's a very reasonable thing to do is just trying to format the text in a nice way but if we're trying to parse the responses from the model in a predictable way that's going to cause some problems the way I control for that is I just set the max number of token that the model can respond with to one which will hopefully correspond to a number between 1 and five I set the number of completions as one so this will only generate one completion but you could also have it generate multiple completions and then you can pick the completion that was returned most frequently and then finally I set the temperature at 0.1 and temperature essentially increases the randomness of the responses so a very high temperature will generate completions that don't make any sense and then a very very low temperature will generate completions that are very predictable so what I do is for each resume I generate the completion and then store it in this experience level list it's also worth calling out that there are multiple models we can use here I use GPT 40 That's apparently the most intelligent large language model open AI has I also tried GPT 40 mini which was significantly faster and it's about 20 times cheaper we can see this here I think I ran it twice with GPT 40 mini which cost a total of 4 cents but running it twice with GPT 40 cost 1.20 so it's about 30 times more expensive to use GPT 40 than GPT 40 mini so which makes the most sense will just depend on your use case in an earlier version of this I used GPT 40 mini but instead of just generating one response I generated three responses although I didn't do a proper comparison of the two I feel that the result of running GPD 40 mini multiple times was similar to running gp40 just once for a small use case like this it's not super important which is better GPT 40 mini done three times versus GPT 40 just run once but you can imagine if you're working with a lot more data and it's a difference of $6,000 versus $60,000 then that's something that'll make a huge difference okay so this experience level list all the elements are strings so I just convert them to integers and store them in a numpy array and then they look like this this and then I add this numpy array as a new column in the original data frame this is what it looks like there people's names here so I have to blur it out but now we went from just having name and resume to having name resume and experience level but of course we could have done other things like job title industry and other pieces of information that are obvious by looking at someone's resume but if you have a th000 or 10,000 resume is not viable for a human to go through and read everything you can just pass it off to a capable large language model to extract that information dynamically use case number two structuring unstructured data although extracting data from text in the way that we just did is sort of structuring unstructured data using text embeddings is a lot cheaper and captures much more information from the underlying text here I'm going to do some imports again we're going to use polers for the data frames and then I'm using this sentence Transformers Library so open AI has text embeddings via their API but you know their API money on the other hand sentence Transformers is completely free and open source so I'm going to use that here here I'm going to load in the data that we just generated from the previous notebook and then doing a couple things here so here I'm doing some string manipulation to the rume text I'm removing any trailing white space and then I'm removing the first line which includes the person's name the person's first and last name probably doesn't have a whole lot of useful information for us in doing these text embeddings so it's better to just remove it and then also added this thing here to replace the people's names with numbers so I can show it in this example one of the great things about the sentence Transformers library is that they have a wide range of compatible embedding models that you can choose from all mini LM L6 V2 is by far the most popular but then there's this other one that I'm using here which is paraphrase multilingual mpet base V2 and the reason I'm using this is that it works on both sentences and paragraphs but of course there are many other text embedding models that you can choose from I like to go to hugging face and look at their model repository if you select this filter sentence similarity all the embedding models will come up so we see that all mini LM L6 V2 is here this seems to be a very popular one I haven't used it before it's from by it's called BG M3 Alibaba has it nomic AI so there are tons of embedding models so many from sentence Transformers here so we're going to load in the model using the sentence Transformers module we can generate the embeddings like this what is happening here is we take the resumé column from the data frame we convert it to a list and then we pass it into the embedding model to generate the embeddings and then here I'm doing some fanciness so I'm defining a schema for a new polar data frame to organize all of these embeddings and then I append this DF embeddings data frame to the original data frame then it looks something like this we have the name here the resume and the experience level so this is everything we generated in the first notebook but now we we have all these embeddings there's 768 of them so what we've done now is essentially converted this resumé column into a set of numbers that we can use for further analysis which leads us right into use case number three which is lead scoring and customer segmentation here I've got a whole lot of imports we've got polar and numpy again mat plot lib will allow us to make some plots for training the lead scoring model and some machine learning stuff I'm going to use sklearn I've got random Forest classifier imported logistic regression the Au score which is a way we can evaluate binary classification models and then finally a function to generate a train and testing data set so first step is we're going to import the data that we just created in that previous notebook but now we're going to import Outreach data so we haven't seen this yet what this consists of is a list of everyone's names their rle kind of defined by me whether they responded to the Outreach and whether they said yes no or maybe that's what's in this DF Outreach then we're going to join these two data frames together so we join DF and DF Outreach and we'll drop any duplicates then again I have this line of code here to replace people's names with numbers so I can show it in this example code so the basic idea of lead scoring is that you're going to take some inputs like job title revenue of company customer Behavior so on and so forth and you want to predict the probability that that person buys your product I'm actually going to do something a little different since I don't have sales data I only have whether they said yes or no to the offer I'm going to train two different models one model is going to predict the probability that a person will say yes to my offer based on their resume and then another model to predict the probability that a person will say no to my offer based on their resume and the resumés will be represented by all the different text embeddings which is what's happening in this line of code here as well as experience level which we generated in that first notebook but as a first step since we have 768 of these embedding Dimensions I'm going to do a dimensionality reduction and so the way I'm going to do that is I'm going to train this auxiliary model so this model isn't going to predict the probability of someone saying yes or no but rather it's just going to predict the probability that they resp respond to my Outreach and here I'm going to use a random force classifier and I set a very large number of estimators with a pretty narrow depth just because I'm trying to get a sense of the most important predictors in predicting whether someone will respond or not do that in two lines of code so we initialize the classifier and then we fit it to our data we can then evaluate the classifier using the Au score Au score of 100 means it's perfect but of course it's overfitting because because we used all the data to train it but we don't really care about the performance of this model cuz we're not actually going to use it directly the only reason I trained this model is so I can get a sorted ranking of the most important features now we have a list of the features which are most helpful in predicting whether someone will respond and then these features are least helpful and it's funny that the experience level variable that we went through so much trouble of generating is one of the least predictive variables here which kind make sense so the people that would attend this Workshop they're from all different experience levels they'll be complete beginners or people who have been working in some field for decades now that we have a sense of which predictors are most helpful now we're going to train the yes and no models what I do first is I'm going to create a new data frame that only involves people who responded to the Outreach because if someone didn't respond then they had no way of saying yes or no so it's not really fair to put them into the training data from this new data frame I'll Define two columns one corresponding to whether they said yes and then another corresponding to whether they said no once I have that I can train the yes and the no model here I do it in a little sophisticated way I do it in a for Loop just because I didn't want to copy paste the same code for the yes model and no model cuz it's very similar so what I do here is I initialize a list that will save the classification models here I have a list of the target names the target names are what we just created here yes and no and then this is sending how many of the variables to use in the yes and no models and so again we have 769 variables that we could use but here I'm only going to use the 75 most predictive based on this initial analysis I did up here and then I initialize two more lists to store the evaluation metrics of the models based on the training and testing data and then here I Define the feature names to use in these models this is a data frame involving the most important features and then I'm just going to extract the variable names up until the 75th row so I'm going to essentially grab the 75 most important predictors now I just do this little for loops and so the first model corresponds to the yes model so this will get passed in I'll create two data sets have the predictors here and the target here then I'll create a train test split where I use 20% of the data for testing 80% for training then I'll train this logistic regression model and then once that model's trained I'll just append it to this list I initialized earlier and then I'll compute the Au score for the train and testing data set this will happen for the yes model and the no model and then at the end of that this is the result this is the performance of each model so the first is the yes model the second is the no model this is the performance on the training data and then this is the performance on the testing data so the big biggest problem with this analysis here is that there are only 100 examples really for something like this and we have so many embeddings you probably want at least a thousand examples or on that order but I still think there's some clues that can be extracted from this small data set at this point we've trained both a model to predict the probability that someone will say yes and the probability that someone will say no and so what I do here is I apply both of these models to the entire data set that's generating this array yes score and this array no score which just has the probabilities of yes and no respectively for each person in that original data frame then I'm adding The Columns to the original data frame the first is corresponding to this yes score so essentially the probability that someone will say yes to the offer the no score is essentially the probability that a person says no and then I generate this score which I'm defining as the yes score minus the no score so it's kind of like you want to maximize the probability that someone says yes while minimizing the probability that someone says no so this is just like a hacky way of doing that and then finally I'm adding yes and no columns to the original data set okay so now we've generated these scores for every lead in the data set now let's do some analysis good thing to do in any kind of analysis is to plot a histogram here's a histogram of the scores higher score is better we can also break this down by role this is pretty insightful so we can see that the roles that have the highest score so basically they have the highest yes score and lowest no score are it professionals and this kind of agrees with my intuition through doing the Outreach it seemed like it professionals was a promising customer avatar for these Workshops the next role here is data architect but this isn't something we can take too seriously because there was only one person with this role in the data set next we have Consultants so this also aligns with my intuitions they seem like a promising customer here data manager seem promising but again there's only two of them and then finally data analyst seems like a promising Avatar these actually were the three avatars that I had written down in my notes that seem to be most open or most interested in this type of Workshop but it's not just about like who is the best customer but who's like the anti- customer who do you want to avoid and make sure you don't reach out to it's interesting like data scientists and dat students are among the bottom which again aligns with my experience data SS seem to be split like half people like are so anti- generative Ai and llms others are excited about it but neither is super interested in doing these workshops because they either already know the stuff if they're into it and if they're not into it they don't care to learn this stuff of course there are data scientists out there who would attend workshops like this but bu and large as a population they're not super into it and then students were another one typically students just don't have money money to pay for a workshop like this so that also aligns with my experience you know another thing we can do which I just thought of just so that experience level doesn't go to waste we can include that here as well so even though it wasn't very predictive we can still use it to segment the data a bit okay so this is interesting people with the least amount of experience so just beginners seem to score most highly and then these midc careers so if we go back to this data augmentation script one was the entry level folks so people with 0 to one years of experience and then three were the midlevel folks so people in their careers for a bit so they seem to be the most promising avatars based on this analysis and then everyone else has negative score so more likely to say no than yes that's an interesting pattern of course like there are only four people that have 0 to two years of experience so take that with the grain of salt and overall there are only 100 people in this data set so this whole analysis needs to be taken with the grain of Sal and then the last thing we can do is Define customer segments and so this is helpful because now what this allows you to do instead of like getting so granular for someone's score you know sales is a lot of times very unpredictable so the more precise your metrics are when it comes to this kind of analysis probably the worse off your strategy is going to be in practice it just makes your strategy very fragile if it's dependent on the third decimal place of this score that you generated using machine learning that's why it's really common to Define grades based on these scores so basically what you do is you take all the people in some sample you generate scores for them and then based on that sample you'll Define segments so you'll basically say anyone with a score higher than 0.05 will put in grade A anyone with a score below minus 0.05 will put in grade C and then everyone in the middle will put in Grade B the way I did it is that I put the bottom 20% in grade C the top 10% in grade A and then everyone else I put in Grade B so that's what this is showing here and then what we can do is we can use these grades and then apply it to New resumés or New Leads so as you identify a list of a thousand potential people to reach out to or 10,000 or 100,000 or a million just like more leads than you could ever reach out to or maybe it costs a lot of money and effort to reach out to a lead so you want to like distill it down to the most important you can take resumés pass it into this pipeline of the yes score and the no score and then compute the grade now you have different segments of customers of a grade leads b-grade leads and c-grade leads this might be a better way of segmenting it because let's say we just did the roles I identified the Consultants data analysts and it professionals as good people to reach out to and then data scientists and data students as bad people to reach out to but then we see a data scientist appearing right here with a grade of a and they had a pretty solid yes score and a low no score it's just that they didn't respond and maybe if they had responded they would have said yes to the offer that's one of the reasons why doing the lead scoring and then segmenting customers based on the lead score can be beneficial Beyond some generalization of a particular role or something else okay so that brings us to the end again the code is freely available on the GitHub and a Blog will be coming out soon if you enjoyed this topic of real world AI use cases that are not a chatbot I have about a dozen other use cases that I didn't include here but I could include in a future video so if that's something that you'd be interested in please let me know in the comment section below and as always thank you so much for your time and thanks for watching